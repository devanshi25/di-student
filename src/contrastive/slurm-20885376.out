07/10/2022 01:13:20 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
07/10/2022 01:13:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/runs/Jul10_01-13-19_uc2n508.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
[INFO|tokenization_auto.py:344] 2022-07-10 01:13:20,442 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:20,870 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:20,870 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:23,388 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:23,388 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:23,388 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:23,388 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:23,388 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:13:23,803 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:23,804 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 01:13:23,828 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 01:13:23,828 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 01:13:23,828 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:24,248 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:24,248 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 01:13:24,260 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-10 01:13:24,799 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:25,224 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:13:25,225 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,199 >> loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,200 >> loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,200 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,200 >> loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,200 >> loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:28,200 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:13:28,616 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:13:28,617 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:648] 2022-07-10 01:13:29,106 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:13:29,106 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:344] 2022-07-10 01:13:31,402 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:31,824 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:31,824 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:34,377 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:34,378 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:34,378 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:34,378 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:34,378 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:13:34,798 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:34,798 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 01:13:34,892 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 01:13:34,892 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 01:13:34,892 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:35,321 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:35,321 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 01:13:35,333 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-10 01:13:35,921 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:36,358 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:36,359 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:38,908 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:38,909 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:38,909 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:38,909 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:13:38,909 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:13:39,334 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:39,334 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 01:13:39,355 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 01:13:39,355 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 01:13:39,355 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:39,777 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:39,778 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 01:13:39,790 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 01:13:40,331 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:13:40,332 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 01:13:40,775 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 01:13:40,912 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.4.bias', 'fit_denses.0.bias', 'fit_denses.3.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.4.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.2.weight', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-10 01:13:40,912 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-10 01:13:43,691 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 01:13:43,696 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 01:13:43,696 >>   Num examples = 6874
[INFO|trainer.py:1281] 2022-07-10 01:13:43,696 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 01:13:43,696 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-10 01:13:43,696 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-10 01:13:43,696 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 01:13:43,696 >>   Total optimization steps = 10750
[INFO|trainer.py:2389] 2022-07-10 01:13:49,771 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:13:49,771 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:13:49,771 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:13:51,818 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-215
[INFO|configuration_utils.py:439] 2022-07-10 01:13:51,820 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-215/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:13:51,943 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-215/pytorch_model.bin
{'eval_loss': 0.24591359496116638, 'eval_accuracy': 0.9005669428696031, 'eval_f1': 0.5230125523012552, 'eval_precision': 0.5122950819672131, 'eval_recall': 0.5341880341880342, 'eval_runtime': 2.0426, 'eval_samples_per_second': 1122.614, 'eval_steps_per_second': 140.51, 'epoch': 1.0}
[INFO|trainer.py:2389] 2022-07-10 01:13:56,834 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:13:56,835 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:13:56,835 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:13:58,843 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-430
[INFO|configuration_utils.py:439] 2022-07-10 01:13:58,845 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-430/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:13:58,966 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-430/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:13:59,218 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-215] due to args.save_total_limit
{'eval_loss': 0.20910269021987915, 'eval_accuracy': 0.9127780200610554, 'eval_f1': 0.5475113122171946, 'eval_precision': 0.5817307692307693, 'eval_recall': 0.5170940170940171, 'eval_runtime': 2.0041, 'eval_samples_per_second': 1144.127, 'eval_steps_per_second': 143.203, 'epoch': 2.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:04,019 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:04,019 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:04,019 >>   Batch size = 8
{'loss': 0.3276, 'learning_rate': 9.275092936802975e-05, 'epoch': 2.33}
[INFO|trainer.py:2139] 2022-07-10 01:14:05,869 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-645
[INFO|configuration_utils.py:439] 2022-07-10 01:14:05,871 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-645/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:05,994 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-645/pytorch_model.bin
{'eval_loss': 0.21772605180740356, 'eval_accuracy': 0.8983863933711296, 'eval_f1': 0.6070826306913997, 'eval_precision': 0.5013927576601671, 'eval_recall': 0.7692307692307693, 'eval_runtime': 1.8454, 'eval_samples_per_second': 1242.561, 'eval_steps_per_second': 155.523, 'epoch': 3.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:10,859 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:10,859 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:10,859 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:14:12,853 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-860
[INFO|configuration_utils.py:439] 2022-07-10 01:14:12,855 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-860/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:12,976 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-860/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:13,220 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-645] due to args.save_total_limit
{'eval_loss': 0.21283411979675293, 'eval_accuracy': 0.919755778456171, 'eval_f1': 0.5740740740740742, 'eval_precision': 0.6262626262626263, 'eval_recall': 0.5299145299145299, 'eval_runtime': 1.99, 'eval_samples_per_second': 1152.283, 'eval_steps_per_second': 144.224, 'epoch': 4.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:18,098 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:18,098 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:18,098 >>   Batch size = 8
{'loss': 0.2111, 'learning_rate': 9.548570309439875e-05, 'epoch': 4.65}
[INFO|trainer.py:2139] 2022-07-10 01:14:20,177 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1075
[INFO|configuration_utils.py:439] 2022-07-10 01:14:20,179 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1075/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:20,309 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1075/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:20,576 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-860] due to args.save_total_limit
{'eval_loss': 0.2212812602519989, 'eval_accuracy': 0.8979502834714348, 'eval_f1': 0.6073825503355705, 'eval_precision': 0.5, 'eval_recall': 0.7735042735042735, 'eval_runtime': 2.0748, 'eval_samples_per_second': 1105.141, 'eval_steps_per_second': 138.323, 'epoch': 5.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:25,441 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:25,441 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:25,441 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:14:27,378 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1290
[INFO|configuration_utils.py:439] 2022-07-10 01:14:27,380 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1290/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:27,510 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1290/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:27,767 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-430] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 01:14:27,803 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1075] due to args.save_total_limit
{'eval_loss': 0.1919943243265152, 'eval_accuracy': 0.9241168774531182, 'eval_f1': 0.5628140703517588, 'eval_precision': 0.6829268292682927, 'eval_recall': 0.47863247863247865, 'eval_runtime': 1.9327, 'eval_samples_per_second': 1186.398, 'eval_steps_per_second': 148.494, 'epoch': 6.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:32,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:32,790 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:32,790 >>   Batch size = 8
{'loss': 0.1813, 'learning_rate': 9.058950254602428e-05, 'epoch': 6.98}
[INFO|trainer.py:2139] 2022-07-10 01:14:34,847 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1505
[INFO|configuration_utils.py:439] 2022-07-10 01:14:34,849 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1505/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:34,975 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1505/pytorch_model.bin
{'eval_loss': 0.21554312109947205, 'eval_accuracy': 0.9258613170518971, 'eval_f1': 0.6009389671361502, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.5470085470085471, 'eval_runtime': 2.0531, 'eval_samples_per_second': 1116.825, 'eval_steps_per_second': 139.786, 'epoch': 7.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:39,786 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:39,786 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:39,786 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:14:41,828 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1720
[INFO|configuration_utils.py:439] 2022-07-10 01:14:41,830 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1720/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:41,952 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1720/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:42,197 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1505] due to args.save_total_limit
{'eval_loss': 0.22846387326717377, 'eval_accuracy': 0.9114696903619712, 'eval_f1': 0.6088631984585742, 'eval_precision': 0.5543859649122806, 'eval_recall': 0.6752136752136753, 'eval_runtime': 2.0381, 'eval_samples_per_second': 1125.077, 'eval_steps_per_second': 140.819, 'epoch': 8.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:46,918 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:46,918 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:46,918 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:14:49,073 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1935
[INFO|configuration_utils.py:439] 2022-07-10 01:14:49,077 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1935/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:49,202 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1935/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:49,449 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1720] due to args.save_total_limit
{'eval_loss': 0.3158396780490875, 'eval_accuracy': 0.8730920191888356, 'eval_f1': 0.5739385065885799, 'eval_precision': 0.4365256124721604, 'eval_recall': 0.8376068376068376, 'eval_runtime': 2.1515, 'eval_samples_per_second': 1065.774, 'eval_steps_per_second': 133.396, 'epoch': 9.0}
[INFO|trainer.py:2389] 2022-07-10 01:14:54,317 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:14:54,317 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:14:54,317 >>   Batch size = 8
{'loss': 0.1574, 'learning_rate': 8.569330199764984e-05, 'epoch': 9.3}
[INFO|trainer.py:2139] 2022-07-10 01:14:56,232 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2150
[INFO|configuration_utils.py:439] 2022-07-10 01:14:56,234 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2150/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:14:56,356 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2150/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:14:56,604 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1935] due to args.save_total_limit
{'eval_loss': 0.21344268321990967, 'eval_accuracy': 0.9262974269515918, 'eval_f1': 0.623608017817372, 'eval_precision': 0.6511627906976745, 'eval_recall': 0.5982905982905983, 'eval_runtime': 1.9112, 'eval_samples_per_second': 1199.74, 'eval_steps_per_second': 150.164, 'epoch': 10.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:01,285 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:01,285 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:01,285 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:15:03,206 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2365
[INFO|configuration_utils.py:439] 2022-07-10 01:15:03,209 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2365/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:03,331 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2365/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:03,576 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2150] due to args.save_total_limit
{'eval_loss': 0.23317664861679077, 'eval_accuracy': 0.9289140863497601, 'eval_f1': 0.5955334987593051, 'eval_precision': 0.7100591715976331, 'eval_recall': 0.5128205128205128, 'eval_runtime': 1.9169, 'eval_samples_per_second': 1196.228, 'eval_steps_per_second': 149.724, 'epoch': 11.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:08,371 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:08,372 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:08,372 >>   Batch size = 8
{'loss': 0.1367, 'learning_rate': 8.080689385037211e-05, 'epoch': 11.63}
[INFO|trainer.py:2139] 2022-07-10 01:15:10,357 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2580
[INFO|configuration_utils.py:439] 2022-07-10 01:15:10,360 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2580/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:10,483 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2580/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:10,728 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2365] due to args.save_total_limit
{'eval_loss': 0.28768390417099, 'eval_accuracy': 0.9215002180549499, 'eval_f1': 0.625, 'eval_precision': 0.6097560975609756, 'eval_recall': 0.6410256410256411, 'eval_runtime': 1.9818, 'eval_samples_per_second': 1157.013, 'eval_steps_per_second': 144.816, 'epoch': 12.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:15,362 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:15,362 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:15,362 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:15:17,304 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2795
[INFO|configuration_utils.py:439] 2022-07-10 01:15:17,313 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2795/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:17,444 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2795/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:17,707 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2580] due to args.save_total_limit
{'eval_loss': 0.2616181969642639, 'eval_accuracy': 0.9049280418665504, 'eval_f1': 0.6461038961038962, 'eval_precision': 0.5209424083769634, 'eval_recall': 0.8504273504273504, 'eval_runtime': 1.9384, 'eval_samples_per_second': 1182.928, 'eval_steps_per_second': 148.06, 'epoch': 13.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:22,825 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:22,825 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:22,825 >>   Batch size = 8
{'loss': 0.1283, 'learning_rate': 7.591069330199765e-05, 'epoch': 13.95}
[INFO|trainer.py:2139] 2022-07-10 01:15:24,796 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3010
[INFO|configuration_utils.py:439] 2022-07-10 01:15:24,798 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3010/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:24,925 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3010/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:25,181 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2795] due to args.save_total_limit
{'eval_loss': 0.24608339369297028, 'eval_accuracy': 0.9092891408634975, 'eval_f1': 0.6258992805755397, 'eval_precision': 0.5403726708074534, 'eval_recall': 0.7435897435897436, 'eval_runtime': 1.9672, 'eval_samples_per_second': 1165.604, 'eval_steps_per_second': 145.891, 'epoch': 14.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:30,019 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:30,019 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:30,019 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:15:31,963 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3225
[INFO|configuration_utils.py:439] 2022-07-10 01:15:31,965 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3225/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:32,092 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3225/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:32,348 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3010] due to args.save_total_limit
{'eval_loss': 0.2182890921831131, 'eval_accuracy': 0.9158307893589185, 'eval_f1': 0.5987525987525987, 'eval_precision': 0.582995951417004, 'eval_recall': 0.6153846153846154, 'eval_runtime': 1.9395, 'eval_samples_per_second': 1182.251, 'eval_steps_per_second': 147.975, 'epoch': 15.0}
[INFO|trainer.py:2389] 2022-07-10 01:15:37,025 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:37,025 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:37,025 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 01:15:39,081 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3440
[INFO|configuration_utils.py:439] 2022-07-10 01:15:39,083 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3440/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:39,210 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3440/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:39,462 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3225] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 01:15:39,498 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 01:15:39,498 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1290 (score: 0.1919943243265152).
[INFO|trainer.py:2139] 2022-07-10 01:15:39,567 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0
[INFO|configuration_utils.py:439] 2022-07-10 01:15:39,569 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:39,693 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/0/pytorch_model.bin
{'eval_loss': 0.2570905387401581, 'eval_accuracy': 0.9175752289576974, 'eval_f1': 0.6212424849699398, 'eval_precision': 0.5849056603773585, 'eval_recall': 0.6623931623931624, 'eval_runtime': 2.0522, 'eval_samples_per_second': 1117.357, 'eval_steps_per_second': 139.852, 'epoch': 16.0}
{'train_runtime': 115.8653, 'train_samples_per_second': 2966.375, 'train_steps_per_second': 92.78, 'train_loss': 0.1817441274953443, 'epoch': 16.0}
***** train metrics *****
  epoch                    =       16.0
  train_loss               =     0.1817
  train_runtime            = 0:01:55.86
  train_samples            =       6874
  train_samples_per_second =   2966.375
  train_steps_per_second   =      92.78
07/10/2022 01:15:39 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 01:15:39,700 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:39,700 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:39,700 >>   Batch size = 8
{'eval_loss': 0.1919943243265152, 'eval_accuracy': 0.9241168774531182, 'eval_f1': 0.5628140703517588, 'eval_precision': 0.6829268292682927, 'eval_recall': 0.47863247863247865, 'eval_runtime': 1.9747, 'eval_samples_per_second': 1161.167, 'eval_steps_per_second': 145.336, 'epoch': 16.0}
***** eval metrics *****
  epoch                   =       16.0
  eval_accuracy           =     0.9241
  eval_f1                 =     0.5628
  eval_loss               =      0.192
  eval_precision          =     0.6829
  eval_recall             =     0.4786
  eval_runtime            = 0:00:01.97
  eval_samples            =       2293
  eval_samples_per_second =   1161.167
  eval_steps_per_second   =    145.336
07/10/2022 01:15:41 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 01:15:41,700 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 01:15:41,700 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:41,700 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-10 01:15:44,415 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:15:44,416 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 01:15:44,844 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 01:15:44,994 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.4.bias', 'fit_denses.0.bias', 'fit_denses.3.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.4.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.2.weight', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-10 01:15:44,994 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-10 01:15:45,133 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 01:15:45,139 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 01:15:45,139 >>   Num examples = 6874
[INFO|trainer.py:1281] 2022-07-10 01:15:45,139 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 01:15:45,139 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-10 01:15:45,139 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-10 01:15:45,139 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 01:15:45,139 >>   Total optimization steps = 10750
***** predict metrics *****
  predict_accuracy           =     0.9215
  predict_f1                 =     0.5385
  predict_loss               =     0.2115
  predict_precision          =     0.6731
  predict_recall             =     0.4487
  predict_runtime            = 0:00:02.28
  predict_samples            =       2293
  predict_samples_per_second =   1002.029
  predict_steps_per_second   =    125.417
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:15:49,794 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:49,795 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:49,795 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:15:51,744 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-215
[INFO|configuration_utils.py:439] 2022-07-10 01:15:51,746 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-215/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:51,869 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-215/pytorch_model.bin
{'eval_loss': 0.30609312653541565, 'eval_accuracy': 0.8940252943741823, 'eval_f1': 0.509090909090909, 'eval_precision': 0.4827586206896552, 'eval_recall': 0.5384615384615384, 'eval_runtime': 1.9459, 'eval_samples_per_second': 1178.38, 'eval_steps_per_second': 147.49, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:15:56,743 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:15:56,743 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:15:56,743 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:15:58,773 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-430
[INFO|configuration_utils.py:439] 2022-07-10 01:15:58,776 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-430/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:15:58,897 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-430/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:15:59,140 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-215] due to args.save_total_limit
{'eval_loss': 0.19284828007221222, 'eval_accuracy': 0.9245529873528129, 'eval_f1': 0.6295503211991433, 'eval_precision': 0.630901287553648, 'eval_recall': 0.6282051282051282, 'eval_runtime': 2.0268, 'eval_samples_per_second': 1131.336, 'eval_steps_per_second': 141.602, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:04,236 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:04,237 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:04,237 >>   Batch size = 8
{'loss': 0.3272, 'learning_rate': 9.29368029739777e-05, 'epoch': 2.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:06,197 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-645
[INFO|configuration_utils.py:439] 2022-07-10 01:16:06,199 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-645/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:06,326 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-645/pytorch_model.bin
{'eval_loss': 0.2433272898197174, 'eval_accuracy': 0.8822503270824248, 'eval_f1': 0.5807453416149069, 'eval_precision': 0.4560975609756098, 'eval_recall': 0.7991452991452992, 'eval_runtime': 1.9563, 'eval_samples_per_second': 1172.086, 'eval_steps_per_second': 146.702, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:11,698 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:11,698 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:11,698 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:13,878 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-860
[INFO|configuration_utils.py:439] 2022-07-10 01:16:13,883 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-860/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:14,013 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-860/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:14,268 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-645] due to args.save_total_limit
{'eval_loss': 0.19583523273468018, 'eval_accuracy': 0.9162668992586132, 'eval_f1': 0.6496350364963502, 'eval_precision': 0.5668789808917197, 'eval_recall': 0.7606837606837606, 'eval_runtime': 2.1761, 'eval_samples_per_second': 1053.735, 'eval_steps_per_second': 131.889, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:19,247 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:19,247 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:19,247 >>   Batch size = 8
{'loss': 0.2046, 'learning_rate': 9.548570309439875e-05, 'epoch': 4.65}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:21,286 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1075
[INFO|configuration_utils.py:439] 2022-07-10 01:16:21,288 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1075/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:21,412 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1075/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:21,665 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-860] due to args.save_total_limit
{'eval_loss': 0.19394291937351227, 'eval_accuracy': 0.9297863061491496, 'eval_f1': 0.5683646112600537, 'eval_precision': 0.762589928057554, 'eval_recall': 0.452991452991453, 'eval_runtime': 2.0343, 'eval_samples_per_second': 1127.174, 'eval_steps_per_second': 141.081, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:26,719 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:26,720 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:26,720 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:28,770 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1290
[INFO|configuration_utils.py:439] 2022-07-10 01:16:28,772 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1290/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:28,901 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1290/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:29,152 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1075] due to args.save_total_limit
{'eval_loss': 0.21178112924098969, 'eval_accuracy': 0.9188835586567815, 'eval_f1': 0.6294820717131474, 'eval_precision': 0.5895522388059702, 'eval_recall': 0.6752136752136753, 'eval_runtime': 2.0468, 'eval_samples_per_second': 1120.283, 'eval_steps_per_second': 140.219, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:33,722 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:33,722 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:33,722 >>   Batch size = 8
{'loss': 0.1799, 'learning_rate': 9.058950254602428e-05, 'epoch': 6.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:35,591 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1505
[INFO|configuration_utils.py:439] 2022-07-10 01:16:35,593 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1505/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:35,715 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1505/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:35,957 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1290] due to args.save_total_limit
{'eval_loss': 0.1950240135192871, 'eval_accuracy': 0.9201918883558656, 'eval_f1': 0.5590361445783132, 'eval_precision': 0.6408839779005525, 'eval_recall': 0.49572649572649574, 'eval_runtime': 1.8647, 'eval_samples_per_second': 1229.662, 'eval_steps_per_second': 153.909, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:40,601 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:40,602 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:40,602 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:42,536 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1720
[INFO|configuration_utils.py:439] 2022-07-10 01:16:42,538 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1720/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:42,663 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1720/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:42,914 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1505] due to args.save_total_limit
{'eval_loss': 0.20039042830467224, 'eval_accuracy': 0.927605756650676, 'eval_f1': 0.6612244897959183, 'eval_precision': 0.6328125, 'eval_recall': 0.6923076923076923, 'eval_runtime': 1.9304, 'eval_samples_per_second': 1187.81, 'eval_steps_per_second': 148.67, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:47,469 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:47,469 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:47,469 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:49,474 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1935
[INFO|configuration_utils.py:439] 2022-07-10 01:16:49,484 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1935/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:49,611 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1935/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:16:49,861 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-430] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 01:16:49,897 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1720] due to args.save_total_limit
{'eval_loss': 0.18734042346477509, 'eval_accuracy': 0.9215002180549499, 'eval_f1': 0.5871559633027523, 'eval_precision': 0.6336633663366337, 'eval_recall': 0.5470085470085471, 'eval_runtime': 2.0012, 'eval_samples_per_second': 1145.81, 'eval_steps_per_second': 143.414, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:16:54,654 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:16:54,654 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:16:54,654 >>   Batch size = 8
{'loss': 0.1568, 'learning_rate': 8.569330199764984e-05, 'epoch': 9.3}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:16:56,625 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2150
[INFO|configuration_utils.py:439] 2022-07-10 01:16:56,627 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2150/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:16:56,751 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2150/pytorch_model.bin
{'eval_loss': 0.2603300213813782, 'eval_accuracy': 0.9027474923680767, 'eval_f1': 0.6080843585237258, 'eval_precision': 0.5164179104477612, 'eval_recall': 0.7393162393162394, 'eval_runtime': 1.9673, 'eval_samples_per_second': 1165.531, 'eval_steps_per_second': 145.882, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:01,499 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:01,499 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:01,499 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:03,488 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2365
[INFO|configuration_utils.py:439] 2022-07-10 01:17:03,490 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2365/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:03,618 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2365/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:03,876 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2150] due to args.save_total_limit
{'eval_loss': 0.26415547728538513, 'eval_accuracy': 0.9175752289576974, 'eval_f1': 0.544578313253012, 'eval_precision': 0.6243093922651933, 'eval_recall': 0.4829059829059829, 'eval_runtime': 1.9848, 'eval_samples_per_second': 1155.265, 'eval_steps_per_second': 144.597, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:08,950 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:08,951 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:08,951 >>   Batch size = 8
{'loss': 0.142, 'learning_rate': 8.079710144927536e-05, 'epoch': 11.63}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:10,864 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2580
[INFO|configuration_utils.py:439] 2022-07-10 01:17:10,868 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2580/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:10,993 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2580/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:11,247 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2365] due to args.save_total_limit
{'eval_loss': 0.24920985102653503, 'eval_accuracy': 0.9241168774531182, 'eval_f1': 0.6200873362445415, 'eval_precision': 0.6339285714285714, 'eval_recall': 0.6068376068376068, 'eval_runtime': 1.9101, 'eval_samples_per_second': 1200.442, 'eval_steps_per_second': 150.252, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:15,893 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:15,893 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:15,894 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:17,991 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2795
[INFO|configuration_utils.py:439] 2022-07-10 01:17:17,993 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2795/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:18,120 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2795/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:18,381 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2580] due to args.save_total_limit
{'eval_loss': 0.20227953791618347, 'eval_accuracy': 0.9267335368512866, 'eval_f1': 0.5841584158415841, 'eval_precision': 0.6941176470588235, 'eval_recall': 0.5042735042735043, 'eval_runtime': 2.0931, 'eval_samples_per_second': 1095.478, 'eval_steps_per_second': 137.114, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:23,395 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:23,395 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:23,395 >>   Batch size = 8
{'loss': 0.1264, 'learning_rate': 7.59009009009009e-05, 'epoch': 13.95}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:25,567 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3010
[INFO|configuration_utils.py:439] 2022-07-10 01:17:25,569 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3010/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:25,698 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3010/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:25,952 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2795] due to args.save_total_limit
{'eval_loss': 0.22791025042533875, 'eval_accuracy': 0.9140863497601396, 'eval_f1': 0.6358595194085028, 'eval_precision': 0.5602605863192183, 'eval_recall': 0.7350427350427351, 'eval_runtime': 2.1683, 'eval_samples_per_second': 1057.494, 'eval_steps_per_second': 132.36, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:31,271 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:31,271 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:31,272 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:33,301 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3225
[INFO|configuration_utils.py:439] 2022-07-10 01:17:33,304 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3225/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:33,430 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3225/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:33,690 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3010] due to args.save_total_limit
{'eval_loss': 0.20922638475894928, 'eval_accuracy': 0.9241168774531182, 'eval_f1': 0.6233766233766234, 'eval_precision': 0.631578947368421, 'eval_recall': 0.6153846153846154, 'eval_runtime': 2.0252, 'eval_samples_per_second': 1132.231, 'eval_steps_per_second': 141.714, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:38,810 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:38,810 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:38,811 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:40,937 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3440
[INFO|configuration_utils.py:439] 2022-07-10 01:17:40,939 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3440/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:41,075 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3440/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:41,340 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3225] due to args.save_total_limit
{'eval_loss': 0.25740697979927063, 'eval_accuracy': 0.9105974705625818, 'eval_f1': 0.5610278372591007, 'eval_precision': 0.5622317596566524, 'eval_recall': 0.5598290598290598, 'eval_runtime': 2.1228, 'eval_samples_per_second': 1080.17, 'eval_steps_per_second': 135.198, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:46,351 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:46,351 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:46,351 >>   Batch size = 8
{'loss': 0.1146, 'learning_rate': 7.101449275362319e-05, 'epoch': 16.28}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:48,335 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3655
[INFO|configuration_utils.py:439] 2022-07-10 01:17:48,338 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3655/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:48,467 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3655/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:48,721 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3440] due to args.save_total_limit
{'eval_loss': 0.2513132691383362, 'eval_accuracy': 0.9153946794592237, 'eval_f1': 0.6104417670682731, 'eval_precision': 0.5757575757575758, 'eval_recall': 0.6495726495726496, 'eval_runtime': 1.9801, 'eval_samples_per_second': 1158.048, 'eval_steps_per_second': 144.945, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:17:53,304 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:17:53,304 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:17:53,304 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:17:55,216 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3870
[INFO|configuration_utils.py:439] 2022-07-10 01:17:55,218 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3870/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:17:55,348 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3870/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:17:55,609 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3655] due to args.save_total_limit
{'eval_loss': 0.2608143091201782, 'eval_accuracy': 0.9180113388573921, 'eval_f1': 0.6131687242798353, 'eval_precision': 0.5912698412698413, 'eval_recall': 0.6367521367521367, 'eval_runtime': 1.908, 'eval_samples_per_second': 1201.796, 'eval_steps_per_second': 150.421, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:00,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:00,832 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:00,832 >>   Batch size = 8
{'loss': 0.1084, 'learning_rate': 6.611829220524873e-05, 'epoch': 18.6}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:02,786 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4085
[INFO|configuration_utils.py:439] 2022-07-10 01:18:02,788 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4085/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:02,913 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4085/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:18:03,165 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3870] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 01:18:03,202 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 01:18:03,202 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1935 (score: 0.18734042346477509).
[INFO|trainer.py:2139] 2022-07-10 01:18:03,265 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1
[INFO|configuration_utils.py:439] 2022-07-10 01:18:03,267 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:03,390 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/1/pytorch_model.bin
{'eval_loss': 0.2984956204891205, 'eval_accuracy': 0.9249890972525077, 'eval_f1': 0.6055045871559633, 'eval_precision': 0.6534653465346535, 'eval_recall': 0.5641025641025641, 'eval_runtime': 1.9496, 'eval_samples_per_second': 1176.153, 'eval_steps_per_second': 147.211, 'epoch': 19.0}
{'train_runtime': 138.121, 'train_samples_per_second': 2488.398, 'train_steps_per_second': 77.83, 'train_loss': 0.16859909603907983, 'epoch': 19.0}
***** train metrics *****
  epoch                    =       19.0
  train_loss               =     0.1686
  train_runtime            = 0:02:18.12
  train_samples            =       6874
  train_samples_per_second =   2488.398
  train_steps_per_second   =      77.83
07/10/2022 01:18:03 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 01:18:03,396 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:03,397 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:03,397 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.18734042346477509, 'eval_accuracy': 0.9215002180549499, 'eval_f1': 0.5871559633027523, 'eval_precision': 0.6336633663366337, 'eval_recall': 0.5470085470085471, 'eval_runtime': 1.9597, 'eval_samples_per_second': 1170.104, 'eval_steps_per_second': 146.454, 'epoch': 19.0}
***** eval metrics *****
  epoch                   =       19.0
  eval_accuracy           =     0.9215
  eval_f1                 =     0.5872
  eval_loss               =     0.1873
  eval_precision          =     0.6337
  eval_recall             =      0.547
  eval_runtime            = 0:00:01.95
  eval_samples            =       2293
  eval_samples_per_second =   1170.104
  eval_steps_per_second   =    146.454
07/10/2022 01:18:05 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 01:18:05,370 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 01:18:05,370 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:05,370 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-10 01:18:08,093 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:18:08,093 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 01:18:08,524 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 01:18:08,678 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.4.bias', 'fit_denses.0.bias', 'fit_denses.3.bias', 'fit_denses.1.weight', 'fit_denses.1.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.4.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.2.weight', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-10 01:18:08,678 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-10 01:18:08,817 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 01:18:08,822 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 01:18:08,823 >>   Num examples = 6874
[INFO|trainer.py:1281] 2022-07-10 01:18:08,823 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 01:18:08,823 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-10 01:18:08,823 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-10 01:18:08,823 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 01:18:08,823 >>   Total optimization steps = 10750
***** predict metrics *****
  predict_accuracy           =     0.9163
  predict_f1                 =     0.5429
  predict_loss               =     0.1905
  predict_precision          =     0.6129
  predict_recall             =     0.4872
  predict_runtime            = 0:00:02.29
  predict_samples            =       2293
  predict_samples_per_second =   1000.683
  predict_steps_per_second   =    125.249
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:13,561 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:13,561 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:13,561 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:15,693 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-215
[INFO|configuration_utils.py:439] 2022-07-10 01:18:15,696 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-215/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:15,821 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-215/pytorch_model.bin
{'eval_loss': 0.2518731355667114, 'eval_accuracy': 0.8983863933711296, 'eval_f1': 0.01687763713080169, 'eval_precision': 0.6666666666666666, 'eval_recall': 0.008547008547008548, 'eval_runtime': 2.1282, 'eval_samples_per_second': 1077.455, 'eval_steps_per_second': 134.858, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:20,877 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:20,877 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:20,877 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:22,821 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-430
[INFO|configuration_utils.py:439] 2022-07-10 01:18:22,823 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-430/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:22,946 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-430/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:18:23,199 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-215] due to args.save_total_limit
{'eval_loss': 0.18319696187973022, 'eval_accuracy': 0.9271696467509812, 'eval_f1': 0.631346578366446, 'eval_precision': 0.6529680365296804, 'eval_recall': 0.6111111111111112, 'eval_runtime': 1.9397, 'eval_samples_per_second': 1182.124, 'eval_steps_per_second': 147.959, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:27,815 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:27,816 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:27,816 >>   Batch size = 8
{'loss': 0.3272, 'learning_rate': 9.275092936802975e-05, 'epoch': 2.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:29,716 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-645
[INFO|configuration_utils.py:439] 2022-07-10 01:18:29,718 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-645/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:29,843 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-645/pytorch_model.bin
{'eval_loss': 0.18671609461307526, 'eval_accuracy': 0.9267335368512866, 'eval_f1': 0.6146788990825689, 'eval_precision': 0.6633663366336634, 'eval_recall': 0.5726495726495726, 'eval_runtime': 1.8969, 'eval_samples_per_second': 1208.793, 'eval_steps_per_second': 151.297, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:34,676 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:34,676 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:34,677 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:36,679 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-860
[INFO|configuration_utils.py:439] 2022-07-10 01:18:36,681 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-860/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:36,806 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-860/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:18:37,055 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-645] due to args.save_total_limit
{'eval_loss': 0.2529661953449249, 'eval_accuracy': 0.8918447448757086, 'eval_f1': 0.6012861736334405, 'eval_precision': 0.48195876288659795, 'eval_recall': 0.7991452991452992, 'eval_runtime': 1.999, 'eval_samples_per_second': 1147.088, 'eval_steps_per_second': 143.574, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:41,758 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:41,759 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:41,759 >>   Batch size = 8
{'loss': 0.2165, 'learning_rate': 9.54954954954955e-05, 'epoch': 4.65}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:43,691 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1075
[INFO|configuration_utils.py:439] 2022-07-10 01:18:43,702 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1075/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:43,830 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1075/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:18:44,084 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-430] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 01:18:44,118 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-860] due to args.save_total_limit
{'eval_loss': 0.17165257036685944, 'eval_accuracy': 0.9254252071522023, 'eval_f1': 0.6013986013986014, 'eval_precision': 0.6615384615384615, 'eval_recall': 0.5512820512820513, 'eval_runtime': 1.9278, 'eval_samples_per_second': 1189.427, 'eval_steps_per_second': 148.873, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:49,130 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:49,130 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:49,130 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:51,264 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1290
[INFO|configuration_utils.py:439] 2022-07-10 01:18:51,269 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1290/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:51,398 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1290/pytorch_model.bin
{'eval_loss': 0.18390023708343506, 'eval_accuracy': 0.9241168774531182, 'eval_f1': 0.609865470852018, 'eval_precision': 0.6415094339622641, 'eval_recall': 0.5811965811965812, 'eval_runtime': 2.1299, 'eval_samples_per_second': 1076.589, 'eval_steps_per_second': 134.75, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:18:56,456 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:18:56,456 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:18:56,456 >>   Batch size = 8
{'loss': 0.1756, 'learning_rate': 9.059929494712104e-05, 'epoch': 6.98}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:18:58,360 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1505
[INFO|configuration_utils.py:439] 2022-07-10 01:18:58,361 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1505/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:18:58,484 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1505/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:18:58,727 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1290] due to args.save_total_limit
{'eval_loss': 0.21179746091365814, 'eval_accuracy': 0.922808547754034, 'eval_f1': 0.639511201629328, 'eval_precision': 0.6108949416342413, 'eval_recall': 0.6709401709401709, 'eval_runtime': 1.8998, 'eval_samples_per_second': 1206.971, 'eval_steps_per_second': 151.069, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:03,228 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:03,228 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:03,228 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:05,253 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1720
[INFO|configuration_utils.py:439] 2022-07-10 01:19:05,272 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1720/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:05,398 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1720/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:05,657 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1505] due to args.save_total_limit
{'eval_loss': 0.24262893199920654, 'eval_accuracy': 0.927605756650676, 'eval_f1': 0.5337078651685393, 'eval_precision': 0.7786885245901639, 'eval_recall': 0.405982905982906, 'eval_runtime': 2.0215, 'eval_samples_per_second': 1134.295, 'eval_steps_per_second': 141.972, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:10,182 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:10,182 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:10,183 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:12,235 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1935
[INFO|configuration_utils.py:439] 2022-07-10 01:19:12,237 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1935/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:12,377 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1935/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:12,630 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1720] due to args.save_total_limit
{'eval_loss': 0.22550438344478607, 'eval_accuracy': 0.9206279982555604, 'eval_f1': 0.6111111111111112, 'eval_precision': 0.6111111111111112, 'eval_recall': 0.6111111111111112, 'eval_runtime': 2.0484, 'eval_samples_per_second': 1119.389, 'eval_steps_per_second': 140.107, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:17,277 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:17,277 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:17,277 >>   Batch size = 8
{'loss': 0.1497, 'learning_rate': 8.570309439874658e-05, 'epoch': 9.3}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:19,203 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2150
[INFO|configuration_utils.py:439] 2022-07-10 01:19:19,205 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2150/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:19,334 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2150/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:19,597 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1935] due to args.save_total_limit
{'eval_loss': 0.2158612310886383, 'eval_accuracy': 0.919755778456171, 'eval_f1': 0.6166666666666667, 'eval_precision': 0.6016260162601627, 'eval_recall': 0.6324786324786325, 'eval_runtime': 1.9229, 'eval_samples_per_second': 1192.491, 'eval_steps_per_second': 149.256, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:24,254 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:24,254 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:24,254 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:26,356 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2365
[INFO|configuration_utils.py:439] 2022-07-10 01:19:26,358 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2365/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:26,489 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2365/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:26,758 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2150] due to args.save_total_limit
{'eval_loss': 0.21689778566360474, 'eval_accuracy': 0.9293501962494548, 'eval_f1': 0.6124401913875598, 'eval_precision': 0.6956521739130435, 'eval_recall': 0.5470085470085471, 'eval_runtime': 2.0981, 'eval_samples_per_second': 1092.911, 'eval_steps_per_second': 136.793, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:31,395 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:31,395 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:31,395 >>   Batch size = 8
{'loss': 0.138, 'learning_rate': 8.080689385037211e-05, 'epoch': 11.63}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:33,319 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2580
[INFO|configuration_utils.py:439] 2022-07-10 01:19:33,321 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2580/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:33,446 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2580/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:33,695 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2365] due to args.save_total_limit
{'eval_loss': 0.23712536692619324, 'eval_accuracy': 0.9302224160488443, 'eval_f1': 0.6059113300492611, 'eval_precision': 0.7151162790697675, 'eval_recall': 0.5256410256410257, 'eval_runtime': 1.9202, 'eval_samples_per_second': 1194.151, 'eval_steps_per_second': 149.464, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:38,401 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:38,401 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:38,401 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:40,482 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2795
[INFO|configuration_utils.py:439] 2022-07-10 01:19:40,484 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2795/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:40,614 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2795/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:40,878 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2580] due to args.save_total_limit
{'eval_loss': 0.22681459784507751, 'eval_accuracy': 0.9245529873528129, 'eval_f1': 0.5986078886310905, 'eval_precision': 0.6548223350253807, 'eval_recall': 0.5512820512820513, 'eval_runtime': 2.0773, 'eval_samples_per_second': 1103.856, 'eval_steps_per_second': 138.163, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:45,778 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:45,778 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:45,778 >>   Batch size = 8
{'loss': 0.1221, 'learning_rate': 7.591069330199765e-05, 'epoch': 13.95}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:47,782 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3010
[INFO|configuration_utils.py:439] 2022-07-10 01:19:47,787 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3010/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:47,912 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3010/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:48,168 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2795] due to args.save_total_limit
{'eval_loss': 0.24762172996997833, 'eval_accuracy': 0.9210641081552551, 'eval_f1': 0.6387225548902197, 'eval_precision': 0.599250936329588, 'eval_recall': 0.6837606837606838, 'eval_runtime': 2.0006, 'eval_samples_per_second': 1146.17, 'eval_steps_per_second': 143.459, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-10 01:19:52,733 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:52,733 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:52,733 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 01:19:54,709 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3225
[INFO|configuration_utils.py:439] 2022-07-10 01:19:54,711 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3225/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:54,839 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3225/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-10 01:19:55,096 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3010] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 01:19:55,132 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 01:19:55,133 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1075 (score: 0.17165257036685944).
[INFO|trainer.py:2139] 2022-07-10 01:19:55,197 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2
[INFO|configuration_utils.py:439] 2022-07-10 01:19:55,199 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/config.json
[INFO|modeling_utils.py:1084] 2022-07-10 01:19:55,321 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/amazongoogle-COMBO-False-PROB-0.10-AUG-typo32-1e-04-TinyBERT_General_4L_312D/2/pytorch_model.bin
{'eval_loss': 0.2784985601902008, 'eval_accuracy': 0.9044919319668556, 'eval_f1': 0.6256410256410257, 'eval_precision': 0.5213675213675214, 'eval_recall': 0.782051282051282, 'eval_runtime': 1.9727, 'eval_samples_per_second': 1162.351, 'eval_steps_per_second': 145.484, 'epoch': 15.0}
{'train_runtime': 106.3693, 'train_samples_per_second': 3231.197, 'train_steps_per_second': 101.063, 'train_loss': 0.1830432259019955, 'epoch': 15.0}
***** train metrics *****
  epoch                    =       15.0
  train_loss               =      0.183
  train_runtime            = 0:01:46.36
  train_samples            =       6874
  train_samples_per_second =   3231.197
  train_steps_per_second   =    101.063
07/10/2022 01:19:55 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 01:19:55,327 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 01:19:55,327 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:55,327 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.17165257036685944, 'eval_accuracy': 0.9254252071522023, 'eval_f1': 0.6013986013986014, 'eval_precision': 0.6615384615384615, 'eval_recall': 0.5512820512820513, 'eval_runtime': 1.8903, 'eval_samples_per_second': 1213.048, 'eval_steps_per_second': 151.829, 'epoch': 15.0}
***** eval metrics *****
  epoch                   =       15.0
  eval_accuracy           =     0.9254
  eval_f1                 =     0.6014
  eval_loss               =     0.1717
  eval_precision          =     0.6615
  eval_recall             =     0.5513
  eval_runtime            = 0:00:01.89
  eval_samples            =       2293
  eval_samples_per_second =   1213.048
  eval_steps_per_second   =    151.829
07/10/2022 01:19:57 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 01:19:57,255 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 01:19:57,255 >>   Num examples = 2293
[INFO|trainer.py:2394] 2022-07-10 01:19:57,255 >>   Batch size = 8
***** predict metrics *****
  predict_accuracy           =      0.928
  predict_f1                 =     0.5885
  predict_loss               =      0.176
  predict_precision          =     0.7066
  predict_recall             =     0.5043
  predict_runtime            = 0:00:02.30
  predict_samples            =       2293
  predict_samples_per_second =    994.864
  predict_steps_per_second   =    124.521

============================= JOB FEEDBACK =============================

NodeName=uc2n508
Job ID: 20885376
Cluster: uc2
User/Group: ma_dmittal/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:12:47
CPU Efficiency: 17.92% of 01:11:20 core-walltime
Job Wall-clock time: 00:07:08
Memory Utilized: 13.99 GB
Memory Efficiency: 0.00% of 0.00 MB
