07/10/2022 14:49:48 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
07/10/2022 14:49:48 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=False,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/runs/Jul10_14-49-48_uc2n509.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=loss,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=64,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
[INFO|tokenization_auto.py:344] 2022-07-10 14:49:49,166 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 14:49:49,600 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:49:49,601 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:52,241 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:52,241 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:52,242 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:52,242 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:52,242 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 14:49:52,685 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:49:52,685 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 14:49:52,709 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 14:49:52,709 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 14:49:52,709 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 14:49:53,153 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:49:53,154 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 14:49:53,165 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-10 14:49:53,755 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 14:49:54,188 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 14:49:54,190 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:49:57,263 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 14:49:57,711 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 14:49:57,711 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:648] 2022-07-10 14:49:58,211 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 14:49:58,211 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_auto.py:344] 2022-07-10 14:50:00,543 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 14:50:00,982 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:00,983 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:03,635 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:03,636 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:03,636 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:03,636 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:03,636 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 14:50:04,069 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:04,069 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 14:50:04,167 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 14:50:04,167 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 14:50:04,167 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 14:50:04,602 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:04,603 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 14:50:04,614 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-10 14:50:05,228 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 14:50:05,666 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:05,666 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:08,302 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:08,302 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:08,302 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:08,302 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 14:50:08,302 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 14:50:08,737 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:08,738 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 14:50:08,759 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 14:50:08,759 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 14:50:08,759 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 14:50:09,196 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:09,197 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 14:50:09,208 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 14:50:09,731 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:50:09,731 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 14:50:10,210 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 14:50:10,352 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.3.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.0.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'fit_denses.4.weight', 'fit_denses.2.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.4.bias', 'fit_denses.0.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2022-07-10 14:50:10,352 >> All the weights of BertModel were initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|trainer.py:457] 2022-07-10 14:50:22,597 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 14:50:22,605 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 14:50:22,605 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-10 14:50:22,605 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 14:50:22,605 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1283] 2022-07-10 14:50:22,606 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1284] 2022-07-10 14:50:22,606 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 14:50:22,606 >>   Total optimization steps = 4500
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:50:42,816 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:50:42,816 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:50:42,817 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:50:45,591 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-90
[INFO|trainer.py:2148] 2022-07-10 14:50:45,591 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 1.1631648540496826, 'eval_accuracy': 0.8674321503131524, 'eval_f1': 0.5171102661596958, 'eval_precision': 0.425, 'eval_recall': 0.6601941747572816, 'eval_runtime': 2.7704, 'eval_samples_per_second': 691.585, 'eval_steps_per_second': 86.629, 'epoch': 1.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:50:48,089 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:50:48,089 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:50:48,090 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:50:50,769 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-180
[INFO|trainer.py:2148] 2022-07-10 14:50:50,769 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:50:50,903 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-90] due to args.save_total_limit
{'eval_loss': 0.9910458326339722, 'eval_accuracy': 0.9707724425887265, 'eval_f1': 0.8502673796791443, 'eval_precision': 0.9464285714285714, 'eval_recall': 0.7718446601941747, 'eval_runtime': 2.6754, 'eval_samples_per_second': 716.155, 'eval_steps_per_second': 89.706, 'epoch': 2.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:50:53,261 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:50:53,261 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:50:53,261 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:50:55,967 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-270
[INFO|trainer.py:2148] 2022-07-10 14:50:55,967 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:50:56,103 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.8056873679161072, 'eval_accuracy': 0.9859081419624217, 'eval_f1': 0.9316455696202532, 'eval_precision': 0.9735449735449735, 'eval_recall': 0.8932038834951457, 'eval_runtime': 2.7021, 'eval_samples_per_second': 709.076, 'eval_steps_per_second': 88.82, 'epoch': 3.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:50:58,447 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:50:58,447 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:50:58,447 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:01,143 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-360
[INFO|trainer.py:2148] 2022-07-10 14:51:01,144 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:01,273 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-270] due to args.save_total_limit
{'eval_loss': 0.6725862622261047, 'eval_accuracy': 0.9879958246346555, 'eval_f1': 0.9434889434889434, 'eval_precision': 0.9552238805970149, 'eval_recall': 0.9320388349514563, 'eval_runtime': 2.6929, 'eval_samples_per_second': 711.511, 'eval_steps_per_second': 89.125, 'epoch': 4.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:03,701 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:03,701 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:03,701 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:06,429 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-450
[INFO|trainer.py:2148] 2022-07-10 14:51:06,429 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:06,559 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-360] due to args.save_total_limit
{'eval_loss': 0.5725588798522949, 'eval_accuracy': 0.9853862212943633, 'eval_f1': 0.9326923076923078, 'eval_precision': 0.9238095238095239, 'eval_recall': 0.941747572815534, 'eval_runtime': 2.7241, 'eval_samples_per_second': 703.359, 'eval_steps_per_second': 88.103, 'epoch': 5.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:08,989 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:08,989 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:08,989 >>   Batch size = 8
{'loss': 0.9949, 'learning_rate': 9.35672514619883e-05, 'epoch': 5.56}
[INFO|trainer.py:2139] 2022-07-10 14:51:11,771 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-540
[INFO|trainer.py:2148] 2022-07-10 14:51:11,771 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:11,903 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-450] due to args.save_total_limit
{'eval_loss': 0.49722251296043396, 'eval_accuracy': 0.9838204592901879, 'eval_f1': 0.9263657957244654, 'eval_precision': 0.9069767441860465, 'eval_recall': 0.9466019417475728, 'eval_runtime': 2.7781, 'eval_samples_per_second': 689.68, 'eval_steps_per_second': 86.39, 'epoch': 6.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:14,231 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:14,231 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:14,231 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:16,996 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-630
[INFO|trainer.py:2148] 2022-07-10 14:51:16,996 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:17,132 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-540] due to args.save_total_limit
{'eval_loss': 0.4406307339668274, 'eval_accuracy': 0.982776617954071, 'eval_f1': 0.9227166276346603, 'eval_precision': 0.8914027149321267, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.7606, 'eval_samples_per_second': 694.045, 'eval_steps_per_second': 86.937, 'epoch': 7.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:19,422 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:19,422 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:19,422 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:22,240 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-720
[INFO|trainer.py:2148] 2022-07-10 14:51:22,240 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:22,373 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-630] due to args.save_total_limit
{'eval_loss': 0.39617621898651123, 'eval_accuracy': 0.9817327766179541, 'eval_f1': 0.9187935034802784, 'eval_precision': 0.88, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8139, 'eval_samples_per_second': 680.914, 'eval_steps_per_second': 85.292, 'epoch': 8.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:24,708 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:24,709 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:24,709 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:27,435 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-810
[INFO|trainer.py:2148] 2022-07-10 14:51:27,435 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:27,581 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.36200040578842163, 'eval_accuracy': 0.9791231732776617, 'eval_f1': 0.9082568807339451, 'eval_precision': 0.8608695652173913, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7218, 'eval_samples_per_second': 703.935, 'eval_steps_per_second': 88.176, 'epoch': 9.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:29,989 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:29,989 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:29,989 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:32,684 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-900
[INFO|trainer.py:2148] 2022-07-10 14:51:32,684 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:32,819 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-810] due to args.save_total_limit
{'eval_loss': 0.3348003625869751, 'eval_accuracy': 0.9770354906054279, 'eval_f1': 0.9, 'eval_precision': 0.8461538461538461, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6907, 'eval_samples_per_second': 712.086, 'eval_steps_per_second': 89.197, 'epoch': 10.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:35,177 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:35,177 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:35,177 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:37,933 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-990
[INFO|trainer.py:2148] 2022-07-10 14:51:37,933 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:38,073 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.31637459993362427, 'eval_accuracy': 0.9728601252609603, 'eval_f1': 0.8839285714285714, 'eval_precision': 0.8181818181818182, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7522, 'eval_samples_per_second': 696.182, 'eval_steps_per_second': 87.204, 'epoch': 11.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:40,406 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:40,406 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:40,406 >>   Batch size = 8
{'loss': 0.5649, 'learning_rate': 8.187134502923976e-05, 'epoch': 11.11}
[INFO|trainer.py:2139] 2022-07-10 14:51:43,280 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1080
[INFO|trainer.py:2148] 2022-07-10 14:51:43,280 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:43,412 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-990] due to args.save_total_limit
{'eval_loss': 0.297743558883667, 'eval_accuracy': 0.9728601252609603, 'eval_f1': 0.8839285714285714, 'eval_precision': 0.8181818181818182, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8706, 'eval_samples_per_second': 667.453, 'eval_steps_per_second': 83.606, 'epoch': 12.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:45,695 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:45,695 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:45,695 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:48,392 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1170
[INFO|trainer.py:2148] 2022-07-10 14:51:48,392 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:48,525 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.2839932143688202, 'eval_accuracy': 0.971294363256785, 'eval_f1': 0.878048780487805, 'eval_precision': 0.8081632653061225, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.693, 'eval_samples_per_second': 711.471, 'eval_steps_per_second': 89.12, 'epoch': 13.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:50,878 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:50,878 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:50,878 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:53,642 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1260
[INFO|trainer.py:2148] 2022-07-10 14:51:53,642 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:53,774 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1170] due to args.save_total_limit
{'eval_loss': 0.272240549325943, 'eval_accuracy': 0.971294363256785, 'eval_f1': 0.878048780487805, 'eval_precision': 0.8081632653061225, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7599, 'eval_samples_per_second': 694.237, 'eval_steps_per_second': 86.961, 'epoch': 14.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:51:56,075 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:51:56,076 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:51:56,076 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:51:58,898 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1350
[INFO|trainer.py:2148] 2022-07-10 14:51:58,898 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:51:59,030 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1260] due to args.save_total_limit
{'eval_loss': 0.26481568813323975, 'eval_accuracy': 0.9686847599164927, 'eval_f1': 0.868421052631579, 'eval_precision': 0.792, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8185, 'eval_samples_per_second': 679.806, 'eval_steps_per_second': 85.153, 'epoch': 15.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:01,358 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:01,358 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:01,358 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:04,152 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1440
[INFO|trainer.py:2148] 2022-07-10 14:52:04,153 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:04,287 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1350] due to args.save_total_limit
{'eval_loss': 0.25783008337020874, 'eval_accuracy': 0.965553235908142, 'eval_f1': 0.8571428571428572, 'eval_precision': 0.7734375, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7904, 'eval_samples_per_second': 686.638, 'eval_steps_per_second': 86.009, 'epoch': 16.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:06,635 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:06,635 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:06,635 >>   Batch size = 8
{'loss': 0.3984, 'learning_rate': 7.017543859649122e-05, 'epoch': 16.67}
[INFO|trainer.py:2139] 2022-07-10 14:52:09,456 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1530
[INFO|trainer.py:2148] 2022-07-10 14:52:09,457 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:09,591 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.25246381759643555, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8172, 'eval_samples_per_second': 680.119, 'eval_steps_per_second': 85.192, 'epoch': 17.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:11,949 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:11,950 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:11,950 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:14,671 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1620
[INFO|trainer.py:2148] 2022-07-10 14:52:14,671 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:14,805 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1530] due to args.save_total_limit
{'eval_loss': 0.24543407559394836, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7174, 'eval_samples_per_second': 705.084, 'eval_steps_per_second': 88.32, 'epoch': 18.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:17,229 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:17,229 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:17,229 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:19,997 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1710
[INFO|trainer.py:2148] 2022-07-10 14:52:19,997 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:20,130 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1620] due to args.save_total_limit
{'eval_loss': 0.241327166557312, 'eval_accuracy': 0.965553235908142, 'eval_f1': 0.8571428571428572, 'eval_precision': 0.7734375, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7635, 'eval_samples_per_second': 693.319, 'eval_steps_per_second': 86.846, 'epoch': 19.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:22,493 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:22,493 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:22,493 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:25,271 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1800
[INFO|trainer.py:2148] 2022-07-10 14:52:25,271 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:25,410 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1710] due to args.save_total_limit
{'eval_loss': 0.23756946623325348, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7738, 'eval_samples_per_second': 690.751, 'eval_steps_per_second': 86.524, 'epoch': 20.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:27,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:27,791 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:27,791 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:30,467 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1890
[INFO|trainer.py:2148] 2022-07-10 14:52:30,467 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:30,602 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.23283681273460388, 'eval_accuracy': 0.9650313152400835, 'eval_f1': 0.8552915766738662, 'eval_precision': 0.7704280155642024, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6727, 'eval_samples_per_second': 716.868, 'eval_steps_per_second': 89.796, 'epoch': 21.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:32,952 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:32,952 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:32,952 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:35,768 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1980
[INFO|trainer.py:2148] 2022-07-10 14:52:35,768 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:35,902 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1890] due to args.save_total_limit
{'eval_loss': 0.23135621845722198, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.812, 'eval_samples_per_second': 681.356, 'eval_steps_per_second': 85.347, 'epoch': 22.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:38,313 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:38,314 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:38,314 >>   Batch size = 8
{'loss': 0.3326, 'learning_rate': 5.847953216374269e-05, 'epoch': 22.22}
[INFO|trainer.py:2139] 2022-07-10 14:52:41,113 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2070
[INFO|trainer.py:2148] 2022-07-10 14:52:41,113 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:41,251 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-1980] due to args.save_total_limit
{'eval_loss': 0.2295110821723938, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8461538461538461, 'eval_precision': 0.7557251908396947, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7956, 'eval_samples_per_second': 685.365, 'eval_steps_per_second': 85.85, 'epoch': 23.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:43,569 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:43,569 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:43,569 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:46,494 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2160
[INFO|trainer.py:2148] 2022-07-10 14:52:46,494 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:46,628 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2070] due to args.save_total_limit
{'eval_loss': 0.2272794544696808, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8461538461538461, 'eval_precision': 0.7557251908396947, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.921, 'eval_samples_per_second': 655.935, 'eval_steps_per_second': 82.163, 'epoch': 24.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:48,933 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:48,933 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:48,933 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:52,257 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2250
[INFO|trainer.py:2148] 2022-07-10 14:52:52,258 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:52,400 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.2255878746509552, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8461538461538461, 'eval_precision': 0.7557251908396947, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3197, 'eval_samples_per_second': 577.162, 'eval_steps_per_second': 72.296, 'epoch': 25.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:52:54,852 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:52:54,852 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:52:54,852 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:52:57,581 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2340
[INFO|trainer.py:2148] 2022-07-10 14:52:57,581 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:52:57,717 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2250] due to args.save_total_limit
{'eval_loss': 0.22275981307029724, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8479657387580299, 'eval_precision': 0.7586206896551724, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7253, 'eval_samples_per_second': 703.046, 'eval_steps_per_second': 88.064, 'epoch': 26.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:00,009 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:00,009 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:00,009 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:02,711 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2430
[INFO|trainer.py:2148] 2022-07-10 14:53:02,711 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:02,846 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2340] due to args.save_total_limit
{'eval_loss': 0.22255893051624298, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8461538461538461, 'eval_precision': 0.7557251908396947, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.698, 'eval_samples_per_second': 710.151, 'eval_steps_per_second': 88.954, 'epoch': 27.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:05,203 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:05,203 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:05,203 >>   Batch size = 8
{'loss': 0.3024, 'learning_rate': 4.678362573099415e-05, 'epoch': 27.78}
[INFO|trainer.py:2139] 2022-07-10 14:53:08,063 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2520
[INFO|trainer.py:2148] 2022-07-10 14:53:08,063 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:08,199 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2430] due to args.save_total_limit
{'eval_loss': 0.22166548669338226, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8461538461538461, 'eval_precision': 0.7557251908396947, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8517, 'eval_samples_per_second': 671.888, 'eval_steps_per_second': 84.161, 'epoch': 28.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:10,562 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:10,562 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:10,562 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:13,228 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2610
[INFO|trainer.py:2148] 2022-07-10 14:53:13,228 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:13,361 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.2201133519411087, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8479657387580299, 'eval_precision': 0.7586206896551724, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6623, 'eval_samples_per_second': 719.682, 'eval_steps_per_second': 90.148, 'epoch': 29.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:15,729 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:15,729 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:15,729 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:18,456 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2700
[INFO|trainer.py:2148] 2022-07-10 14:53:18,456 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:18,588 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2610] due to args.save_total_limit
{'eval_loss': 0.2184886485338211, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8479657387580299, 'eval_precision': 0.7586206896551724, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7232, 'eval_samples_per_second': 703.573, 'eval_steps_per_second': 88.13, 'epoch': 30.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:20,917 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:20,917 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:20,917 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:23,814 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2790
[INFO|trainer.py:2148] 2022-07-10 14:53:23,814 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:23,966 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2700] due to args.save_total_limit
{'eval_loss': 0.21716199815273285, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8479657387580299, 'eval_precision': 0.7586206896551724, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8925, 'eval_samples_per_second': 662.4, 'eval_steps_per_second': 82.973, 'epoch': 31.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:26,430 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:26,430 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:26,430 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:29,487 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2880
[INFO|trainer.py:2148] 2022-07-10 14:53:29,487 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:29,628 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2790] due to args.save_total_limit
{'eval_loss': 0.21696746349334717, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.0527, 'eval_samples_per_second': 627.647, 'eval_steps_per_second': 78.62, 'epoch': 32.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:32,045 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:32,045 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:32,045 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:35,013 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2970
[INFO|trainer.py:2148] 2022-07-10 14:53:35,013 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:35,146 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.21524681150913239, 'eval_accuracy': 0.9634655532359081, 'eval_f1': 0.8504273504273505, 'eval_precision': 0.7595419847328244, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.9642, 'eval_samples_per_second': 646.37, 'eval_steps_per_second': 80.965, 'epoch': 33.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:37,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:37,462 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:37,462 >>   Batch size = 8
{'loss': 0.2831, 'learning_rate': 3.508771929824561e-05, 'epoch': 33.33}
[INFO|trainer.py:2139] 2022-07-10 14:53:40,260 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3060
[INFO|trainer.py:2148] 2022-07-10 14:53:40,261 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.2157532274723053, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.794, 'eval_samples_per_second': 685.754, 'eval_steps_per_second': 85.898, 'epoch': 34.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:42,918 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:42,918 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:42,918 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:45,864 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3150
[INFO|trainer.py:2148] 2022-07-10 14:53:45,864 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:46,009 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-2970] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 14:53:46,025 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.21523359417915344, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.9416, 'eval_samples_per_second': 651.353, 'eval_steps_per_second': 81.589, 'epoch': 35.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:48,485 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:48,485 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:48,485 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:51,472 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3240
[INFO|trainer.py:2148] 2022-07-10 14:53:51,472 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:51,619 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3150] due to args.save_total_limit
{'eval_loss': 0.21423152089118958, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.9833, 'eval_samples_per_second': 642.25, 'eval_steps_per_second': 80.449, 'epoch': 36.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:54,016 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:54,016 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:54,016 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:53:56,885 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3330
[INFO|trainer.py:2148] 2022-07-10 14:53:56,885 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:53:57,025 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.2140500843524933, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.8642, 'eval_samples_per_second': 668.947, 'eval_steps_per_second': 83.793, 'epoch': 37.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:53:59,474 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:53:59,475 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:53:59,475 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:02,348 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3420
[INFO|trainer.py:2148] 2022-07-10 14:54:02,348 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:02,487 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3330] due to args.save_total_limit
{'eval_loss': 0.21356184780597687, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.8696, 'eval_samples_per_second': 667.686, 'eval_steps_per_second': 83.635, 'epoch': 38.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:04,919 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:04,919 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:04,919 >>   Batch size = 8
{'loss': 0.2659, 'learning_rate': 2.3391812865497074e-05, 'epoch': 38.89}
[INFO|trainer.py:2139] 2022-07-10 14:54:07,617 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3510
[INFO|trainer.py:2148] 2022-07-10 14:54:07,618 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:07,760 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.2127821445465088, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.6947, 'eval_samples_per_second': 711.032, 'eval_steps_per_second': 89.065, 'epoch': 39.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:10,030 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:10,030 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:10,030 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:12,812 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3600
[INFO|trainer.py:2148] 2022-07-10 14:54:12,812 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:12,947 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3510] due to args.save_total_limit
{'eval_loss': 0.21217475831508636, 'eval_accuracy': 0.9634655532359081, 'eval_f1': 0.8504273504273505, 'eval_precision': 0.7595419847328244, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7782, 'eval_samples_per_second': 689.663, 'eval_steps_per_second': 86.388, 'epoch': 40.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:15,355 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:15,355 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:15,355 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:18,170 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3690
[INFO|trainer.py:2148] 2022-07-10 14:54:18,170 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:18,304 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3600] due to args.save_total_limit
{'eval_loss': 0.21189600229263306, 'eval_accuracy': 0.9634655532359081, 'eval_f1': 0.8504273504273505, 'eval_precision': 0.7595419847328244, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.8106, 'eval_samples_per_second': 681.708, 'eval_steps_per_second': 85.391, 'epoch': 41.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:20,575 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:20,575 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:20,575 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:23,301 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3780
[INFO|trainer.py:2148] 2022-07-10 14:54:23,301 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:23,437 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3690] due to args.save_total_limit
{'eval_loss': 0.2118278294801712, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7219, 'eval_samples_per_second': 703.918, 'eval_steps_per_second': 88.173, 'epoch': 42.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:25,786 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:25,787 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:25,787 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:28,542 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3870
[INFO|trainer.py:2148] 2022-07-10 14:54:28,542 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:28,678 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3780] due to args.save_total_limit
{'eval_loss': 0.21144922077655792, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7516, 'eval_samples_per_second': 696.318, 'eval_steps_per_second': 87.221, 'epoch': 43.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:30,962 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:30,962 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:30,962 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:33,759 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3960
[INFO|trainer.py:2148] 2022-07-10 14:54:33,759 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:33,899 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3870] due to args.save_total_limit
{'eval_loss': 0.21127350628376007, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8486140724946697, 'eval_precision': 0.7566539923954373, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7928, 'eval_samples_per_second': 686.056, 'eval_steps_per_second': 85.936, 'epoch': 44.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:36,278 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:36,278 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:36,278 >>   Batch size = 8
{'loss': 0.261, 'learning_rate': 1.1695906432748537e-05, 'epoch': 44.44}
[INFO|trainer.py:2139] 2022-07-10 14:54:39,246 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4050
[INFO|trainer.py:2148] 2022-07-10 14:54:39,246 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:39,383 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-3960] due to args.save_total_limit
{'eval_loss': 0.21120530366897583, 'eval_accuracy': 0.9618997912317327, 'eval_f1': 0.8450106157112527, 'eval_precision': 0.7509433962264151, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.964, 'eval_samples_per_second': 646.417, 'eval_steps_per_second': 80.971, 'epoch': 45.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:41,705 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:41,706 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:41,706 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:44,613 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4140
[INFO|trainer.py:2148] 2022-07-10 14:54:44,613 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.2113395780324936, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.847457627118644, 'eval_precision': 0.7518796992481203, 'eval_recall': 0.970873786407767, 'eval_runtime': 2.9032, 'eval_samples_per_second': 659.951, 'eval_steps_per_second': 82.666, 'epoch': 46.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:47,031 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:47,031 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:47,031 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:49,961 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4230
[INFO|trainer.py:2148] 2022-07-10 14:54:49,961 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:50,094 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4140] due to args.save_total_limit
{'eval_loss': 0.2113468497991562, 'eval_accuracy': 0.9613778705636743, 'eval_f1': 0.8438818565400845, 'eval_precision': 0.746268656716418, 'eval_recall': 0.970873786407767, 'eval_runtime': 2.926, 'eval_samples_per_second': 654.815, 'eval_steps_per_second': 82.023, 'epoch': 47.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:52,437 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:52,437 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:52,437 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:54:55,101 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4320
[INFO|trainer.py:2148] 2022-07-10 14:54:55,101 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:54:55,233 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4230] due to args.save_total_limit
{'eval_loss': 0.21123172342777252, 'eval_accuracy': 0.9618997912317327, 'eval_f1': 0.8456659619450317, 'eval_precision': 0.7490636704119851, 'eval_recall': 0.970873786407767, 'eval_runtime': 2.6596, 'eval_samples_per_second': 720.398, 'eval_steps_per_second': 90.238, 'epoch': 48.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:54:57,581 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:54:57,581 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:54:57,581 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-10 14:55:00,384 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4410
[INFO|trainer.py:2148] 2022-07-10 14:55:00,384 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:00,519 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4320] due to args.save_total_limit
{'eval_loss': 0.2112203985452652, 'eval_accuracy': 0.9613778705636743, 'eval_f1': 0.8438818565400845, 'eval_precision': 0.746268656716418, 'eval_recall': 0.970873786407767, 'eval_runtime': 2.7986, 'eval_samples_per_second': 684.621, 'eval_steps_per_second': 85.756, 'epoch': 49.0}
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:02,855 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:02,856 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:02,856 >>   Batch size = 8
{'loss': 0.2633, 'learning_rate': 2.339181286549708e-08, 'epoch': 50.0}
[INFO|trainer.py:2139] 2022-07-10 14:55:05,659 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4500
[INFO|trainer.py:2148] 2022-07-10 14:55:05,659 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:05,792 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4410] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 14:55:05,804 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 14:55:05,805 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0/checkpoint-4050 (score: 0.21120530366897583).
[INFO|trainer.py:2139] 2022-07-10 14:55:05,870 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/0
[INFO|trainer.py:2148] 2022-07-10 14:55:05,870 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.21120817959308624, 'eval_accuracy': 0.9618997912317327, 'eval_f1': 0.8456659619450317, 'eval_precision': 0.7490636704119851, 'eval_recall': 0.970873786407767, 'eval_runtime': 2.7992, 'eval_samples_per_second': 684.485, 'eval_steps_per_second': 85.739, 'epoch': 50.0}
{'train_runtime': 283.2589, 'train_samples_per_second': 1013.737, 'train_steps_per_second': 15.887, 'train_loss': 0.4073884039984809, 'epoch': 50.0}
***** train metrics *****
  epoch                    =       50.0
  train_loss               =     0.4074
  train_runtime            = 0:04:43.25
  train_samples            =       5743
  train_samples_per_second =   1013.737
  train_steps_per_second   =     15.887
07/10/2022 14:55:05 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 14:55:05,997 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:05,998 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:05,998 >>   Batch size = 8
{'eval_loss': 0.21120530366897583, 'eval_accuracy': 0.9618997912317327, 'eval_f1': 0.8450106157112527, 'eval_precision': 0.7509433962264151, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.688, 'eval_samples_per_second': 712.798, 'eval_steps_per_second': 89.286, 'epoch': 50.0}
***** eval metrics *****
  epoch                   =       50.0
  eval_accuracy           =     0.9619
  eval_f1                 =      0.845
  eval_loss               =     0.2112
  eval_precision          =     0.7509
  eval_recall             =      0.966
  eval_runtime            = 0:00:02.68
  eval_samples            =       1916
  eval_samples_per_second =    712.798
  eval_steps_per_second   =     89.286
07/10/2022 14:55:08 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 14:55:08,696 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 14:55:08,696 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:08,696 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-10 14:55:11,858 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 14:55:11,858 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 14:55:12,307 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 14:55:12,458 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.3.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.0.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'fit_denses.4.weight', 'fit_denses.2.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.4.bias', 'fit_denses.0.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2022-07-10 14:55:12,459 >> All the weights of BertModel were initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|trainer.py:457] 2022-07-10 14:55:12,683 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 14:55:12,688 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 14:55:12,688 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-10 14:55:12,688 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 14:55:12,688 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1283] 2022-07-10 14:55:12,688 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1284] 2022-07-10 14:55:12,688 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 14:55:12,688 >>   Total optimization steps = 4500
***** predict metrics *****
  predict_accuracy           =     0.9525
  predict_f1                 =     0.7982
  predict_loss               =     0.3872
  predict_precision          =     0.7347
  predict_recall             =     0.8738
  predict_runtime            = 0:00:02.72
  predict_samples            =       1916
  predict_samples_per_second =    702.466
  predict_steps_per_second   =     87.992
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:15,327 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:15,327 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:15,327 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:17,954 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-90
[INFO|trainer.py:2148] 2022-07-10 14:55:17,955 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 1.2052706480026245, 'eval_accuracy': 0.8538622129436325, 'eval_f1': 0.3859649122807018, 'eval_precision': 0.352, 'eval_recall': 0.42718446601941745, 'eval_runtime': 2.6234, 'eval_samples_per_second': 730.339, 'eval_steps_per_second': 91.483, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:20,511 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:20,512 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:20,512 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:23,370 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-180
[INFO|trainer.py:2148] 2022-07-10 14:55:23,370 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:23,509 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-90] due to args.save_total_limit
{'eval_loss': 1.0301566123962402, 'eval_accuracy': 0.958768267223382, 'eval_f1': 0.7774647887323944, 'eval_precision': 0.9261744966442953, 'eval_recall': 0.6699029126213593, 'eval_runtime': 2.8542, 'eval_samples_per_second': 671.298, 'eval_steps_per_second': 84.087, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:25,838 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:25,839 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:25,839 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:28,437 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-270
[INFO|trainer.py:2148] 2022-07-10 14:55:28,437 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:28,567 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.8418425917625427, 'eval_accuracy': 0.9832985386221295, 'eval_f1': 0.9179487179487178, 'eval_precision': 0.9728260869565217, 'eval_recall': 0.8689320388349514, 'eval_runtime': 2.5948, 'eval_samples_per_second': 738.405, 'eval_steps_per_second': 92.493, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:31,093 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:31,093 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:31,093 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:33,873 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-360
[INFO|trainer.py:2148] 2022-07-10 14:55:33,873 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:34,015 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-270] due to args.save_total_limit
{'eval_loss': 0.699755072593689, 'eval_accuracy': 0.9874739039665971, 'eval_f1': 0.9405940594059405, 'eval_precision': 0.9595959595959596, 'eval_recall': 0.9223300970873787, 'eval_runtime': 2.7763, 'eval_samples_per_second': 690.117, 'eval_steps_per_second': 86.445, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:36,740 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:36,741 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:36,741 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:39,332 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-450
[INFO|trainer.py:2148] 2022-07-10 14:55:39,332 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:39,464 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-360] due to args.save_total_limit
{'eval_loss': 0.5956997871398926, 'eval_accuracy': 0.9859081419624217, 'eval_f1': 0.9343065693430658, 'eval_precision': 0.9365853658536586, 'eval_recall': 0.9320388349514563, 'eval_runtime': 2.5874, 'eval_samples_per_second': 740.501, 'eval_steps_per_second': 92.756, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:42,070 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:42,070 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:42,071 >>   Batch size = 8
{'loss': 1.0188, 'learning_rate': 9.35672514619883e-05, 'epoch': 5.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:44,876 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-540
[INFO|trainer.py:2148] 2022-07-10 14:55:44,876 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:45,018 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-450] due to args.save_total_limit
{'eval_loss': 0.5149102210998535, 'eval_accuracy': 0.9843423799582464, 'eval_f1': 0.9292452830188679, 'eval_precision': 0.9036697247706422, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.8017, 'eval_samples_per_second': 683.879, 'eval_steps_per_second': 85.663, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:47,636 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:47,636 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:47,637 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:50,412 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-630
[INFO|trainer.py:2148] 2022-07-10 14:55:50,412 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:50,545 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-540] due to args.save_total_limit
{'eval_loss': 0.452433705329895, 'eval_accuracy': 0.9832985386221295, 'eval_f1': 0.9248826291079812, 'eval_precision': 0.8954545454545455, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.7715, 'eval_samples_per_second': 691.317, 'eval_steps_per_second': 86.595, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:53,147 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:53,147 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:53,147 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:55:56,530 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-720
[INFO|trainer.py:2148] 2022-07-10 14:55:56,530 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:55:56,674 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-630] due to args.save_total_limit
{'eval_loss': 0.40631434321403503, 'eval_accuracy': 0.9796450939457203, 'eval_f1': 0.9099307159353348, 'eval_precision': 0.8678414096916299, 'eval_recall': 0.9563106796116505, 'eval_runtime': 3.3759, 'eval_samples_per_second': 567.551, 'eval_steps_per_second': 71.092, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:55:59,489 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:55:59,489 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:55:59,489 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:02,218 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-810
[INFO|trainer.py:2148] 2022-07-10 14:56:02,218 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:02,352 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.36794981360435486, 'eval_accuracy': 0.9796450939457203, 'eval_f1': 0.9103448275862069, 'eval_precision': 0.8646288209606987, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7248, 'eval_samples_per_second': 703.183, 'eval_steps_per_second': 88.081, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:04,748 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:04,748 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:04,749 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:08,098 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-900
[INFO|trainer.py:2148] 2022-07-10 14:56:08,098 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:08,236 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-810] due to args.save_total_limit
{'eval_loss': 0.3379455804824829, 'eval_accuracy': 0.9786012526096033, 'eval_f1': 0.9061784897025171, 'eval_precision': 0.8571428571428571, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3451, 'eval_samples_per_second': 572.782, 'eval_steps_per_second': 71.747, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:10,996 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:10,996 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:10,997 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:14,086 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-990
[INFO|trainer.py:2148] 2022-07-10 14:56:14,086 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:14,227 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.314978688955307, 'eval_accuracy': 0.9765135699373695, 'eval_f1': 0.8979591836734694, 'eval_precision': 0.8425531914893617, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.0856, 'eval_samples_per_second': 620.941, 'eval_steps_per_second': 77.78, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:16,947 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:16,947 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:16,947 >>   Batch size = 8
{'loss': 0.5714, 'learning_rate': 8.187134502923976e-05, 'epoch': 11.11}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:19,905 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1080
[INFO|trainer.py:2148] 2022-07-10 14:56:19,905 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:20,042 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-990] due to args.save_total_limit
{'eval_loss': 0.29780229926109314, 'eval_accuracy': 0.9765135699373695, 'eval_f1': 0.8979591836734694, 'eval_precision': 0.8425531914893617, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.9538, 'eval_samples_per_second': 648.658, 'eval_steps_per_second': 81.252, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:22,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:22,404 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:22,404 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:25,543 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1170
[INFO|trainer.py:2148] 2022-07-10 14:56:25,543 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:25,681 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.2830311357975006, 'eval_accuracy': 0.975991649269311, 'eval_f1': 0.8959276018099547, 'eval_precision': 0.8389830508474576, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.135, 'eval_samples_per_second': 611.161, 'eval_steps_per_second': 76.555, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:28,489 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:28,489 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:28,489 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:31,310 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1260
[INFO|trainer.py:2148] 2022-07-10 14:56:31,310 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:31,447 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1170] due to args.save_total_limit
{'eval_loss': 0.27331146597862244, 'eval_accuracy': 0.9728601252609603, 'eval_f1': 0.8839285714285714, 'eval_precision': 0.8181818181818182, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8172, 'eval_samples_per_second': 680.109, 'eval_steps_per_second': 85.191, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:33,609 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:33,610 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:33,610 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:37,086 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1350
[INFO|trainer.py:2148] 2022-07-10 14:56:37,087 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:37,225 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1260] due to args.save_total_limit
{'eval_loss': 0.26519671082496643, 'eval_accuracy': 0.9707724425887265, 'eval_f1': 0.8761061946902655, 'eval_precision': 0.8048780487804879, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.4729, 'eval_samples_per_second': 551.703, 'eval_steps_per_second': 69.107, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:40,038 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:40,039 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:40,039 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:42,739 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1440
[INFO|trainer.py:2148] 2022-07-10 14:56:42,739 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:42,891 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1350] due to args.save_total_limit
{'eval_loss': 0.257431298494339, 'eval_accuracy': 0.9697286012526096, 'eval_f1': 0.8722466960352422, 'eval_precision': 0.7983870967741935, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6961, 'eval_samples_per_second': 710.661, 'eval_steps_per_second': 89.018, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:45,738 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:45,738 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:45,738 >>   Batch size = 8
{'loss': 0.4039, 'learning_rate': 7.017543859649122e-05, 'epoch': 16.67}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:48,416 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1530
[INFO|trainer.py:2148] 2022-07-10 14:56:48,416 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:48,564 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.24975639581680298, 'eval_accuracy': 0.9707724425887265, 'eval_f1': 0.8761061946902655, 'eval_precision': 0.8048780487804879, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6746, 'eval_samples_per_second': 716.359, 'eval_steps_per_second': 89.732, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:51,465 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:51,465 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:51,465 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:56:54,805 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1620
[INFO|trainer.py:2148] 2022-07-10 14:56:54,805 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:56:54,950 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1530] due to args.save_total_limit
{'eval_loss': 0.24388186633586884, 'eval_accuracy': 0.9702505219206681, 'eval_f1': 0.8741721854304636, 'eval_precision': 0.8016194331983806, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3362, 'eval_samples_per_second': 574.306, 'eval_steps_per_second': 71.938, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:56:57,884 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:56:57,884 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:56:57,884 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:00,498 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1710
[INFO|trainer.py:2148] 2022-07-10 14:57:00,498 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:00,635 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1620] due to args.save_total_limit
{'eval_loss': 0.23938977718353271, 'eval_accuracy': 0.9692066805845512, 'eval_f1': 0.8703296703296702, 'eval_precision': 0.7951807228915663, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6101, 'eval_samples_per_second': 734.072, 'eval_steps_per_second': 91.951, 'epoch': 19.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:03,119 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:03,119 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:03,119 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:05,846 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1800
[INFO|trainer.py:2148] 2022-07-10 14:57:05,846 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:06,000 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1710] due to args.save_total_limit
{'eval_loss': 0.23580975830554962, 'eval_accuracy': 0.9692066805845512, 'eval_f1': 0.8703296703296702, 'eval_precision': 0.7951807228915663, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7226, 'eval_samples_per_second': 703.731, 'eval_steps_per_second': 88.15, 'epoch': 20.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:08,469 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:08,470 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:08,470 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:11,819 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1890
[INFO|trainer.py:2148] 2022-07-10 14:57:11,819 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:11,954 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.2317827045917511, 'eval_accuracy': 0.9697286012526096, 'eval_f1': 0.8722466960352422, 'eval_precision': 0.7983870967741935, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3456, 'eval_samples_per_second': 572.693, 'eval_steps_per_second': 71.736, 'epoch': 21.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:14,403 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:14,403 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:14,403 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:17,091 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1980
[INFO|trainer.py:2148] 2022-07-10 14:57:17,092 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:17,224 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1890] due to args.save_total_limit
{'eval_loss': 0.22954334318637848, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6843, 'eval_samples_per_second': 713.771, 'eval_steps_per_second': 89.408, 'epoch': 22.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:19,648 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:19,648 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:19,648 >>   Batch size = 8
{'loss': 0.3352, 'learning_rate': 5.847953216374269e-05, 'epoch': 22.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:22,960 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2070
[INFO|trainer.py:2148] 2022-07-10 14:57:22,961 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:23,102 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-1980] due to args.save_total_limit
{'eval_loss': 0.22635295987129211, 'eval_accuracy': 0.9681628392484343, 'eval_f1': 0.8665207877461707, 'eval_precision': 0.7888446215139442, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3083, 'eval_samples_per_second': 579.15, 'eval_steps_per_second': 72.545, 'epoch': 23.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:25,703 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:25,703 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:25,703 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:28,497 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2160
[INFO|trainer.py:2148] 2022-07-10 14:57:28,497 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:28,638 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2070] due to args.save_total_limit
{'eval_loss': 0.22475387156009674, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7898, 'eval_samples_per_second': 686.78, 'eval_steps_per_second': 86.027, 'epoch': 24.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:31,462 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:31,462 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:31,462 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:34,151 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2250
[INFO|trainer.py:2148] 2022-07-10 14:57:34,151 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:34,289 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.22369486093521118, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.859002169197397, 'eval_precision': 0.7764705882352941, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6853, 'eval_samples_per_second': 713.502, 'eval_steps_per_second': 89.374, 'epoch': 25.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:36,797 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:36,798 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:36,798 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:39,951 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2340
[INFO|trainer.py:2148] 2022-07-10 14:57:39,951 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:40,097 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2250] due to args.save_total_limit
{'eval_loss': 0.2217400223016739, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.1491, 'eval_samples_per_second': 608.432, 'eval_steps_per_second': 76.213, 'epoch': 26.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:42,971 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:42,971 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:42,971 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:45,748 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2430
[INFO|trainer.py:2148] 2022-07-10 14:57:45,749 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:45,894 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2340] due to args.save_total_limit
{'eval_loss': 0.2195976972579956, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7735, 'eval_samples_per_second': 690.82, 'eval_steps_per_second': 86.533, 'epoch': 27.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:48,707 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:48,707 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:48,707 >>   Batch size = 8
{'loss': 0.2997, 'learning_rate': 4.678362573099415e-05, 'epoch': 27.78}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:51,958 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2520
[INFO|trainer.py:2148] 2022-07-10 14:57:51,958 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:52,098 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2430] due to args.save_total_limit
{'eval_loss': 0.2187461107969284, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.2468, 'eval_samples_per_second': 590.125, 'eval_steps_per_second': 73.92, 'epoch': 28.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:57:54,915 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:57:54,915 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:57:54,915 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:57:57,500 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2610
[INFO|trainer.py:2148] 2022-07-10 14:57:57,500 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:57:57,635 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.21790413558483124, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5807, 'eval_samples_per_second': 742.427, 'eval_steps_per_second': 92.997, 'epoch': 29.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:00,155 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:00,156 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:00,156 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:02,832 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2700
[INFO|trainer.py:2148] 2022-07-10 14:58:02,832 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:02,964 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2610] due to args.save_total_limit
{'eval_loss': 0.2166772484779358, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6725, 'eval_samples_per_second': 716.92, 'eval_steps_per_second': 89.802, 'epoch': 30.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:05,528 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:05,529 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:05,529 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:08,275 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2790
[INFO|trainer.py:2148] 2022-07-10 14:58:08,275 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:08,413 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2700] due to args.save_total_limit
{'eval_loss': 0.21661947667598724, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8614718614718615, 'eval_precision': 0.77734375, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7411, 'eval_samples_per_second': 698.984, 'eval_steps_per_second': 87.555, 'epoch': 31.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:10,824 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:10,824 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:10,824 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:13,914 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2880
[INFO|trainer.py:2148] 2022-07-10 14:58:13,914 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:14,047 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2790] due to args.save_total_limit
{'eval_loss': 0.2158372402191162, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.0859, 'eval_samples_per_second': 620.885, 'eval_steps_per_second': 77.773, 'epoch': 32.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:16,466 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:16,466 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:16,466 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:19,439 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2970
[INFO|trainer.py:2148] 2022-07-10 14:58:19,439 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:19,580 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.2148107886314392, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.9669, 'eval_samples_per_second': 645.794, 'eval_steps_per_second': 80.893, 'epoch': 33.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:22,332 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:22,333 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:22,333 >>   Batch size = 8
{'loss': 0.2807, 'learning_rate': 3.508771929824561e-05, 'epoch': 33.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:25,146 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3060
[INFO|trainer.py:2148] 2022-07-10 14:58:25,146 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:25,292 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-2970] due to args.save_total_limit
{'eval_loss': 0.2136424034833908, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.809, 'eval_samples_per_second': 682.093, 'eval_steps_per_second': 85.44, 'epoch': 34.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:28,176 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:28,176 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:28,176 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:31,250 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3150
[INFO|trainer.py:2148] 2022-07-10 14:58:31,250 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:31,393 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.21331781148910522, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.0697, 'eval_samples_per_second': 624.156, 'eval_steps_per_second': 78.182, 'epoch': 35.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:34,193 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:34,193 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:34,193 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:37,439 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3240
[INFO|trainer.py:2148] 2022-07-10 14:58:37,440 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:37,578 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3150] due to args.save_total_limit
{'eval_loss': 0.21251648664474487, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.2422, 'eval_samples_per_second': 590.957, 'eval_steps_per_second': 74.024, 'epoch': 36.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:40,574 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:40,575 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:40,575 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:43,117 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3330
[INFO|trainer.py:2148] 2022-07-10 14:58:43,118 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:43,252 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.21088942885398865, 'eval_accuracy': 0.9676409185803758, 'eval_f1': 0.8652173913043477, 'eval_precision': 0.7834645669291339, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.5389, 'eval_samples_per_second': 754.66, 'eval_steps_per_second': 94.529, 'epoch': 37.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:45,562 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:45,562 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:45,562 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:48,282 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3420
[INFO|trainer.py:2148] 2022-07-10 14:58:48,282 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:48,415 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3330] due to args.save_total_limit
{'eval_loss': 0.21088401973247528, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7152, 'eval_samples_per_second': 705.666, 'eval_steps_per_second': 88.392, 'epoch': 38.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:50,817 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:50,817 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:50,817 >>   Batch size = 8
{'loss': 0.2653, 'learning_rate': 2.3391812865497074e-05, 'epoch': 38.89}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:53,953 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3510
[INFO|trainer.py:2148] 2022-07-10 14:58:53,953 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:54,101 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.2108253687620163, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.132, 'eval_samples_per_second': 611.755, 'eval_steps_per_second': 76.629, 'epoch': 39.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:58:56,424 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:58:56,424 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:58:56,424 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:58:59,738 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3600
[INFO|trainer.py:2148] 2022-07-10 14:58:59,738 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:58:59,876 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3510] due to args.save_total_limit
{'eval_loss': 0.2107429951429367, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.3102, 'eval_samples_per_second': 578.811, 'eval_steps_per_second': 72.502, 'epoch': 40.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:02,422 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:02,422 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:02,422 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:06,052 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3690
[INFO|trainer.py:2148] 2022-07-10 14:59:06,052 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:06,199 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3600] due to args.save_total_limit
{'eval_loss': 0.2104521244764328, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.6254, 'eval_samples_per_second': 528.496, 'eval_steps_per_second': 66.2, 'epoch': 41.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:08,966 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:08,966 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:08,966 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:11,991 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3780
[INFO|trainer.py:2148] 2022-07-10 14:59:11,991 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:12,132 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3690] due to args.save_total_limit
{'eval_loss': 0.21031194925308228, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.0206, 'eval_samples_per_second': 634.312, 'eval_steps_per_second': 79.455, 'epoch': 42.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:14,926 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:14,926 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:14,926 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:17,490 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3870
[INFO|trainer.py:2148] 2022-07-10 14:59:17,491 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:17,641 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3780] due to args.save_total_limit
{'eval_loss': 0.2098207026720047, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.5606, 'eval_samples_per_second': 748.276, 'eval_steps_per_second': 93.73, 'epoch': 43.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:20,622 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:20,622 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:20,622 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:23,332 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3960
[INFO|trainer.py:2148] 2022-07-10 14:59:23,332 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.20987002551555634, 'eval_accuracy': 0.9660751565762005, 'eval_f1': 0.8596112311015119, 'eval_precision': 0.77431906614786, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7056, 'eval_samples_per_second': 708.168, 'eval_steps_per_second': 88.706, 'epoch': 44.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:26,210 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:26,210 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:26,211 >>   Batch size = 8
{'loss': 0.2607, 'learning_rate': 1.1695906432748537e-05, 'epoch': 44.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:28,797 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4050
[INFO|trainer.py:2148] 2022-07-10 14:59:28,798 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:28,936 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3870] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 14:59:28,952 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-3960] due to args.save_total_limit
{'eval_loss': 0.20955978333950043, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.583, 'eval_samples_per_second': 741.767, 'eval_steps_per_second': 92.914, 'epoch': 45.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:31,589 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:31,589 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:31,589 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:34,293 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4140
[INFO|trainer.py:2148] 2022-07-10 14:59:34,293 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:34,433 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4050] due to args.save_total_limit
{'eval_loss': 0.20936435461044312, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.6997, 'eval_samples_per_second': 709.696, 'eval_steps_per_second': 88.897, 'epoch': 46.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:37,135 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:37,135 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:37,135 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:40,272 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4230
[INFO|trainer.py:2148] 2022-07-10 14:59:40,272 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:40,415 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4140] due to args.save_total_limit
{'eval_loss': 0.20930516719818115, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 3.1332, 'eval_samples_per_second': 611.518, 'eval_steps_per_second': 76.599, 'epoch': 47.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:43,164 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:43,164 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:43,164 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:45,852 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4320
[INFO|trainer.py:2148] 2022-07-10 14:59:45,853 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:45,992 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4230] due to args.save_total_limit
{'eval_loss': 0.20926731824874878, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.6843, 'eval_samples_per_second': 713.781, 'eval_steps_per_second': 89.409, 'epoch': 48.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:48,611 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:48,611 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:48,611 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:51,211 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4410
[INFO|trainer.py:2148] 2022-07-10 14:59:51,211 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:51,352 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4320] due to args.save_total_limit
{'eval_loss': 0.20911192893981934, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.5954, 'eval_samples_per_second': 738.227, 'eval_steps_per_second': 92.471, 'epoch': 49.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 14:59:53,669 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:53,669 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:53,669 >>   Batch size = 8
{'loss': 0.2554, 'learning_rate': 2.339181286549708e-08, 'epoch': 50.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 14:59:56,238 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4500
[INFO|trainer.py:2148] 2022-07-10 14:59:56,238 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 14:59:56,374 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4410] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 14:59:56,385 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 14:59:56,385 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1/checkpoint-4500 (score: 0.2091105580329895).
[INFO|trainer.py:2139] 2022-07-10 14:59:56,450 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/1
[INFO|trainer.py:2148] 2022-07-10 14:59:56,450 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.2091105580329895, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.5648, 'eval_samples_per_second': 747.032, 'eval_steps_per_second': 93.574, 'epoch': 50.0}
{'train_runtime': 283.7561, 'train_samples_per_second': 1011.961, 'train_steps_per_second': 15.859, 'train_loss': 0.410130126953125, 'epoch': 50.0}
***** train metrics *****
  epoch                    =       50.0
  train_loss               =     0.4101
  train_runtime            = 0:04:43.75
  train_samples            =       5743
  train_samples_per_second =   1011.961
  train_steps_per_second   =     15.859
07/10/2022 14:59:56 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 14:59:56,581 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 14:59:56,582 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:56,582 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.2091105580329895, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8633405639913232, 'eval_precision': 0.7803921568627451, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7127, 'eval_samples_per_second': 706.308, 'eval_steps_per_second': 88.473, 'epoch': 50.0}
***** eval metrics *****
  epoch                   =       50.0
  eval_accuracy           =     0.9671
  eval_f1                 =     0.8633
  eval_loss               =     0.2091
  eval_precision          =     0.7804
  eval_recall             =      0.966
  eval_runtime            = 0:00:02.71
  eval_samples            =       1916
  eval_samples_per_second =    706.308
  eval_steps_per_second   =     88.473
07/10/2022 14:59:59 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 14:59:59,305 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 14:59:59,305 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 14:59:59,305 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-10 15:00:02,483 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 15:00:02,483 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 15:00:02,919 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 15:00:03,072 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'fit_denses.2.bias', 'fit_denses.3.bias', 'fit_denses.3.weight', 'fit_denses.1.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.0.bias', 'cls.seq_relationship.weight', 'fit_denses.1.bias', 'fit_denses.4.weight', 'fit_denses.2.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'fit_denses.4.bias', 'fit_denses.0.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2022-07-10 15:00:03,072 >> All the weights of BertModel were initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|trainer.py:457] 2022-07-10 15:00:03,289 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 15:00:03,295 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 15:00:03,295 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-10 15:00:03,295 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-10 15:00:03,295 >>   Instantaneous batch size per device = 64
[INFO|trainer.py:1283] 2022-07-10 15:00:03,295 >>   Total train batch size (w. parallel, distributed & accumulation) = 64
[INFO|trainer.py:1284] 2022-07-10 15:00:03,295 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 15:00:03,295 >>   Total optimization steps = 4500
***** predict metrics *****
  predict_accuracy           =     0.9535
  predict_f1                 =     0.8035
  predict_loss               =     0.3825
  predict_precision          =     0.7368
  predict_recall             =     0.8835
  predict_runtime            = 0:00:02.72
  predict_samples            =       1916
  predict_samples_per_second =    703.853
  predict_steps_per_second   =     88.165
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:05,782 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:05,782 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:05,782 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:09,158 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-90
[INFO|trainer.py:2148] 2022-07-10 15:00:09,158 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 1.1843243837356567, 'eval_accuracy': 0.8663883089770354, 'eval_f1': 0.42600896860986553, 'eval_precision': 0.3958333333333333, 'eval_recall': 0.46116504854368934, 'eval_runtime': 3.3716, 'eval_samples_per_second': 568.281, 'eval_steps_per_second': 71.183, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:12,400 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:12,400 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:12,400 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:15,133 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-180
[INFO|trainer.py:2148] 2022-07-10 15:00:15,134 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:15,268 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-90] due to args.save_total_limit
{'eval_loss': 1.0125904083251953, 'eval_accuracy': 0.9561586638830898, 'eval_f1': 0.7543859649122806, 'eval_precision': 0.9485294117647058, 'eval_recall': 0.6262135922330098, 'eval_runtime': 2.7298, 'eval_samples_per_second': 701.883, 'eval_steps_per_second': 87.919, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:17,881 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:17,882 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:17,882 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:21,042 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-270
[INFO|trainer.py:2148] 2022-07-10 15:00:21,042 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:21,188 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.8334196209907532, 'eval_accuracy': 0.9843423799582464, 'eval_f1': 0.9238578680203046, 'eval_precision': 0.9680851063829787, 'eval_recall': 0.883495145631068, 'eval_runtime': 3.1565, 'eval_samples_per_second': 606.993, 'eval_steps_per_second': 76.032, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:24,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:24,225 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:24,225 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:26,924 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-360
[INFO|trainer.py:2148] 2022-07-10 15:00:26,924 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:27,062 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-270] due to args.save_total_limit
{'eval_loss': 0.69757080078125, 'eval_accuracy': 0.9838204592901879, 'eval_f1': 0.9230769230769231, 'eval_precision': 0.9441624365482234, 'eval_recall': 0.9029126213592233, 'eval_runtime': 2.6958, 'eval_samples_per_second': 710.742, 'eval_steps_per_second': 89.028, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:29,706 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:29,706 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:29,706 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:32,412 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-450
[INFO|trainer.py:2148] 2022-07-10 15:00:32,413 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:32,548 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-360] due to args.save_total_limit
{'eval_loss': 0.591886043548584, 'eval_accuracy': 0.9848643006263048, 'eval_f1': 0.9294403892944039, 'eval_precision': 0.9317073170731708, 'eval_recall': 0.9271844660194175, 'eval_runtime': 2.7021, 'eval_samples_per_second': 709.072, 'eval_steps_per_second': 88.819, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:35,270 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:35,270 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:35,270 >>   Batch size = 8
{'loss': 1.0056, 'learning_rate': 9.35672514619883e-05, 'epoch': 5.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:38,127 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-540
[INFO|trainer.py:2148] 2022-07-10 15:00:38,127 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:38,263 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-450] due to args.save_total_limit
{'eval_loss': 0.5138486623764038, 'eval_accuracy': 0.982776617954071, 'eval_f1': 0.9216152019002375, 'eval_precision': 0.9023255813953488, 'eval_recall': 0.941747572815534, 'eval_runtime': 2.8533, 'eval_samples_per_second': 671.506, 'eval_steps_per_second': 84.113, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:40,920 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:40,921 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:40,921 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:43,774 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-630
[INFO|trainer.py:2148] 2022-07-10 15:00:43,774 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:43,916 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-540] due to args.save_total_limit
{'eval_loss': 0.4521700441837311, 'eval_accuracy': 0.9796450939457203, 'eval_f1': 0.9090909090909092, 'eval_precision': 0.874439461883408, 'eval_recall': 0.9466019417475728, 'eval_runtime': 2.8489, 'eval_samples_per_second': 672.534, 'eval_steps_per_second': 84.242, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:46,344 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:46,344 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:46,344 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:49,878 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-720
[INFO|trainer.py:2148] 2022-07-10 15:00:49,878 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:50,017 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-630] due to args.save_total_limit
{'eval_loss': 0.40510520339012146, 'eval_accuracy': 0.9780793319415448, 'eval_f1': 0.9027777777777778, 'eval_precision': 0.8628318584070797, 'eval_recall': 0.9466019417475728, 'eval_runtime': 3.5296, 'eval_samples_per_second': 542.836, 'eval_steps_per_second': 67.996, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:52,948 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:52,948 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:52,948 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:00:56,053 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-810
[INFO|trainer.py:2148] 2022-07-10 15:00:56,054 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:00:56,195 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.3687604069709778, 'eval_accuracy': 0.9754697286012526, 'eval_f1': 0.8924485125858124, 'eval_precision': 0.8441558441558441, 'eval_recall': 0.9466019417475728, 'eval_runtime': 3.1013, 'eval_samples_per_second': 617.799, 'eval_steps_per_second': 77.386, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:00:59,088 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:00:59,089 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:00:59,089 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:02,146 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-900
[INFO|trainer.py:2148] 2022-07-10 15:01:02,146 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:02,286 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-810] due to args.save_total_limit
{'eval_loss': 0.33950141072273254, 'eval_accuracy': 0.9749478079331941, 'eval_f1': 0.8909090909090909, 'eval_precision': 0.8376068376068376, 'eval_recall': 0.9514563106796117, 'eval_runtime': 3.0534, 'eval_samples_per_second': 627.506, 'eval_steps_per_second': 78.602, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:05,226 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:05,226 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:05,227 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:07,921 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-990
[INFO|trainer.py:2148] 2022-07-10 15:01:07,921 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:08,074 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.3178923726081848, 'eval_accuracy': 0.9749478079331941, 'eval_f1': 0.8914027149321267, 'eval_precision': 0.8347457627118644, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.6908, 'eval_samples_per_second': 712.052, 'eval_steps_per_second': 89.192, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:10,630 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:10,630 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:10,630 >>   Batch size = 8
{'loss': 0.5657, 'learning_rate': 8.187134502923976e-05, 'epoch': 11.11}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:13,731 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1080
[INFO|trainer.py:2148] 2022-07-10 15:01:13,731 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:13,869 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-990] due to args.save_total_limit
{'eval_loss': 0.2979550361633301, 'eval_accuracy': 0.9754697286012526, 'eval_f1': 0.8934240362811792, 'eval_precision': 0.8382978723404255, 'eval_recall': 0.9563106796116505, 'eval_runtime': 3.0972, 'eval_samples_per_second': 618.625, 'eval_steps_per_second': 77.49, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:16,864 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:16,864 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:16,864 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:19,559 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1170
[INFO|trainer.py:2148] 2022-07-10 15:01:19,559 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:19,705 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.2822071313858032, 'eval_accuracy': 0.9739039665970772, 'eval_f1': 0.8873873873873874, 'eval_precision': 0.8277310924369747, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.6915, 'eval_samples_per_second': 711.882, 'eval_steps_per_second': 89.171, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:22,753 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:22,753 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:22,753 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:25,658 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1260
[INFO|trainer.py:2148] 2022-07-10 15:01:25,658 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:25,809 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1170] due to args.save_total_limit
{'eval_loss': 0.27260613441467285, 'eval_accuracy': 0.9702505219206681, 'eval_f1': 0.8736141906873613, 'eval_precision': 0.8040816326530612, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.9003, 'eval_samples_per_second': 660.62, 'eval_steps_per_second': 82.75, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:28,270 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:28,270 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:28,270 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:31,069 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1350
[INFO|trainer.py:2148] 2022-07-10 15:01:31,069 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:31,204 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1260] due to args.save_total_limit
{'eval_loss': 0.2633102834224701, 'eval_accuracy': 0.9692066805845512, 'eval_f1': 0.869757174392936, 'eval_precision': 0.7975708502024291, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.7954, 'eval_samples_per_second': 685.403, 'eval_steps_per_second': 85.854, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:33,953 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:33,953 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:33,953 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:36,609 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1440
[INFO|trainer.py:2148] 2022-07-10 15:01:36,609 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:36,751 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1350] due to args.save_total_limit
{'eval_loss': 0.2561704218387604, 'eval_accuracy': 0.9676409185803758, 'eval_f1': 0.8640350877192983, 'eval_precision': 0.788, 'eval_recall': 0.9563106796116505, 'eval_runtime': 2.6517, 'eval_samples_per_second': 722.543, 'eval_steps_per_second': 90.506, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:39,435 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:39,435 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:39,435 >>   Batch size = 8
{'loss': 0.3996, 'learning_rate': 7.017543859649122e-05, 'epoch': 16.67}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:42,105 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1530
[INFO|trainer.py:2148] 2022-07-10 15:01:42,105 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:42,243 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.25081226229667664, 'eval_accuracy': 0.965553235908142, 'eval_f1': 0.8571428571428572, 'eval_precision': 0.7734375, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6657, 'eval_samples_per_second': 718.766, 'eval_steps_per_second': 90.033, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:44,914 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:44,914 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:44,914 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:47,501 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1620
[INFO|trainer.py:2148] 2022-07-10 15:01:47,502 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:47,634 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1530] due to args.save_total_limit
{'eval_loss': 0.2434806376695633, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5834, 'eval_samples_per_second': 741.669, 'eval_steps_per_second': 92.902, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:50,301 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:50,301 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:50,301 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:52,999 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1710
[INFO|trainer.py:2148] 2022-07-10 15:01:52,999 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:53,136 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1620] due to args.save_total_limit
{'eval_loss': 0.23838911950588226, 'eval_accuracy': 0.9686847599164927, 'eval_f1': 0.868421052631579, 'eval_precision': 0.792, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6936, 'eval_samples_per_second': 711.321, 'eval_steps_per_second': 89.101, 'epoch': 19.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:01:55,831 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:01:55,831 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:01:55,831 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:01:58,784 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1800
[INFO|trainer.py:2148] 2022-07-10 15:01:58,784 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:01:58,926 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1710] due to args.save_total_limit
{'eval_loss': 0.23530808091163635, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.9488, 'eval_samples_per_second': 649.758, 'eval_steps_per_second': 81.389, 'epoch': 20.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:01,805 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:01,805 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:01,805 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:04,398 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1890
[INFO|trainer.py:2148] 2022-07-10 15:02:04,398 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:04,536 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.23165644705295563, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5884, 'eval_samples_per_second': 740.216, 'eval_steps_per_second': 92.72, 'epoch': 21.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:07,009 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:07,009 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:07,009 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:09,627 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1980
[INFO|trainer.py:2148] 2022-07-10 15:02:09,627 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:09,760 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1890] due to args.save_total_limit
{'eval_loss': 0.22935611009597778, 'eval_accuracy': 0.9671189979123174, 'eval_f1': 0.8627450980392158, 'eval_precision': 0.782608695652174, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6133, 'eval_samples_per_second': 733.168, 'eval_steps_per_second': 91.837, 'epoch': 22.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:12,486 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:12,487 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:12,487 >>   Batch size = 8
{'loss': 0.3348, 'learning_rate': 5.847953216374269e-05, 'epoch': 22.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:15,240 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2070
[INFO|trainer.py:2148] 2022-07-10 15:02:15,240 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:15,374 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-1980] due to args.save_total_limit
{'eval_loss': 0.22727084159851074, 'eval_accuracy': 0.9665970772442589, 'eval_f1': 0.8608695652173913, 'eval_precision': 0.7795275590551181, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7498, 'eval_samples_per_second': 696.767, 'eval_steps_per_second': 87.278, 'epoch': 23.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:18,190 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:18,191 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:18,191 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:20,825 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2160
[INFO|trainer.py:2148] 2022-07-10 15:02:20,825 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:20,963 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2070] due to args.save_total_limit
{'eval_loss': 0.22374853491783142, 'eval_accuracy': 0.9676409185803758, 'eval_f1': 0.8646288209606987, 'eval_precision': 0.7857142857142857, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6307, 'eval_samples_per_second': 728.334, 'eval_steps_per_second': 91.232, 'epoch': 24.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:23,455 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:23,455 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:23,455 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:26,279 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2250
[INFO|trainer.py:2148] 2022-07-10 15:02:26,280 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:26,426 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.22304262220859528, 'eval_accuracy': 0.965553235908142, 'eval_f1': 0.8571428571428572, 'eval_precision': 0.7734375, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8202, 'eval_samples_per_second': 679.377, 'eval_steps_per_second': 85.099, 'epoch': 25.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:29,339 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:29,339 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:29,339 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:31,986 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2340
[INFO|trainer.py:2148] 2022-07-10 15:02:31,986 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:32,121 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2250] due to args.save_total_limit
{'eval_loss': 0.22163552045822144, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6432, 'eval_samples_per_second': 724.892, 'eval_steps_per_second': 90.801, 'epoch': 26.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:34,753 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:34,754 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:34,754 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:37,919 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2430
[INFO|trainer.py:2148] 2022-07-10 15:02:37,919 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:38,061 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2340] due to args.save_total_limit
{'eval_loss': 0.22054465115070343, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.1613, 'eval_samples_per_second': 606.082, 'eval_steps_per_second': 75.918, 'epoch': 27.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:40,980 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:40,980 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:40,980 >>   Batch size = 8
{'loss': 0.2954, 'learning_rate': 4.678362573099415e-05, 'epoch': 27.78}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:44,046 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2520
[INFO|trainer.py:2148] 2022-07-10 15:02:44,046 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:44,190 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2430] due to args.save_total_limit
{'eval_loss': 0.2193637639284134, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.0619, 'eval_samples_per_second': 625.761, 'eval_steps_per_second': 78.383, 'epoch': 28.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:47,070 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:47,071 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:47,071 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:49,942 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2610
[INFO|trainer.py:2148] 2022-07-10 15:02:49,942 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:50,098 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.21748021245002747, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8673, 'eval_samples_per_second': 668.219, 'eval_steps_per_second': 83.702, 'epoch': 29.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:53,198 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:53,198 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:53,198 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:02:55,776 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2700
[INFO|trainer.py:2148] 2022-07-10 15:02:55,776 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:02:55,910 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2610] due to args.save_total_limit
{'eval_loss': 0.21655014157295227, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5744, 'eval_samples_per_second': 744.256, 'eval_steps_per_second': 93.226, 'epoch': 30.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:02:58,473 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:02:58,473 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:02:58,473 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:01,840 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2790
[INFO|trainer.py:2148] 2022-07-10 15:03:01,840 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:01,983 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2700] due to args.save_total_limit
{'eval_loss': 0.2151946723461151, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3625, 'eval_samples_per_second': 569.82, 'eval_steps_per_second': 71.376, 'epoch': 31.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:04,854 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:04,855 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:04,855 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:07,550 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2880
[INFO|trainer.py:2148] 2022-07-10 15:03:07,550 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:07,688 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2790] due to args.save_total_limit
{'eval_loss': 0.214151531457901, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6915, 'eval_samples_per_second': 711.858, 'eval_steps_per_second': 89.168, 'epoch': 32.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:10,229 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:10,229 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:10,229 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:13,736 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2970
[INFO|trainer.py:2148] 2022-07-10 15:03:13,736 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:13,877 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.21388553082942963, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.5024, 'eval_samples_per_second': 547.052, 'eval_steps_per_second': 68.524, 'epoch': 33.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:16,655 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:16,655 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:16,655 >>   Batch size = 8
{'loss': 0.2767, 'learning_rate': 3.508771929824561e-05, 'epoch': 33.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:19,865 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3060
[INFO|trainer.py:2148] 2022-07-10 15:03:19,866 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:20,004 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-2970] due to args.save_total_limit
{'eval_loss': 0.2125190943479538, 'eval_accuracy': 0.9650313152400835, 'eval_f1': 0.8552915766738662, 'eval_precision': 0.7704280155642024, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.206, 'eval_samples_per_second': 597.633, 'eval_steps_per_second': 74.86, 'epoch': 34.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:22,763 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:22,763 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:22,763 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:25,345 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3150
[INFO|trainer.py:2148] 2022-07-10 15:03:25,345 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:25,486 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.21220117807388306, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5779, 'eval_samples_per_second': 743.241, 'eval_steps_per_second': 93.099, 'epoch': 35.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:28,200 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:28,201 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:28,201 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:31,094 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3240
[INFO|trainer.py:2148] 2022-07-10 15:03:31,094 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:31,233 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3150] due to args.save_total_limit
{'eval_loss': 0.21174819767475128, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.8891, 'eval_samples_per_second': 663.18, 'eval_steps_per_second': 83.071, 'epoch': 36.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:34,119 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:34,119 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:34,119 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:37,463 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3330
[INFO|trainer.py:2148] 2022-07-10 15:03:37,463 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:37,601 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.2113240510225296, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3392, 'eval_samples_per_second': 573.787, 'eval_steps_per_second': 71.873, 'epoch': 37.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:40,222 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:40,225 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:40,225 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:42,997 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3420
[INFO|trainer.py:2148] 2022-07-10 15:03:42,997 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:43,133 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3330] due to args.save_total_limit
{'eval_loss': 0.2106820046901703, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7701, 'eval_samples_per_second': 691.665, 'eval_steps_per_second': 86.639, 'epoch': 38.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:45,916 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:45,916 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:45,916 >>   Batch size = 8
{'loss': 0.2671, 'learning_rate': 2.3391812865497074e-05, 'epoch': 38.89}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:49,066 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3510
[INFO|trainer.py:2148] 2022-07-10 15:03:49,066 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:49,215 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.21026943624019623, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.853448275862069, 'eval_precision': 0.7674418604651163, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.1455, 'eval_samples_per_second': 609.127, 'eval_steps_per_second': 76.3, 'epoch': 39.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:52,208 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:52,208 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:52,208 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:03:55,113 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3600
[INFO|trainer.py:2148] 2022-07-10 15:03:55,113 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:03:55,264 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3510] due to args.save_total_limit
{'eval_loss': 0.21014690399169922, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.9003, 'eval_samples_per_second': 660.623, 'eval_steps_per_second': 82.75, 'epoch': 40.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:03:58,126 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:03:58,126 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:03:58,127 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:01,482 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3690
[INFO|trainer.py:2148] 2022-07-10 15:04:01,482 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:01,636 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3600] due to args.save_total_limit
{'eval_loss': 0.2100096493959427, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 3.3515, 'eval_samples_per_second': 571.689, 'eval_steps_per_second': 71.61, 'epoch': 41.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:04,594 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:04,595 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:04,595 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:07,273 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3780
[INFO|trainer.py:2148] 2022-07-10 15:04:07,273 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.21009188890457153, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6736, 'eval_samples_per_second': 716.623, 'eval_steps_per_second': 89.765, 'epoch': 42.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:10,333 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:10,333 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:10,333 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:12,860 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3870
[INFO|trainer.py:2148] 2022-07-10 15:04:12,861 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:12,996 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3690] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 15:04:13,012 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3780] due to args.save_total_limit
{'eval_loss': 0.20992985367774963, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.5233, 'eval_samples_per_second': 759.31, 'eval_steps_per_second': 95.112, 'epoch': 43.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:15,801 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:15,801 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:15,802 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:18,461 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3960
[INFO|trainer.py:2148] 2022-07-10 15:04:18,461 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.20997105538845062, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.655, 'eval_samples_per_second': 721.66, 'eval_steps_per_second': 90.396, 'epoch': 44.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:21,245 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:21,246 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:21,246 >>   Batch size = 8
{'loss': 0.2635, 'learning_rate': 1.1695906432748537e-05, 'epoch': 44.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:24,023 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4050
[INFO|trainer.py:2148] 2022-07-10 15:04:24,023 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:24,158 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3870] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-10 15:04:24,172 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-3960] due to args.save_total_limit
{'eval_loss': 0.2097725123167038, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.7726, 'eval_samples_per_second': 691.048, 'eval_steps_per_second': 86.561, 'epoch': 45.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:26,987 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:26,987 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:26,987 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:29,717 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4140
[INFO|trainer.py:2148] 2022-07-10 15:04:29,717 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:29,851 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4050] due to args.save_total_limit
{'eval_loss': 0.2096816599369049, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.8540772532188841, 'eval_precision': 0.7653846153846153, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7254, 'eval_samples_per_second': 703.022, 'eval_steps_per_second': 88.061, 'epoch': 46.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:32,376 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:32,376 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:32,376 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:35,119 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4230
[INFO|trainer.py:2148] 2022-07-10 15:04:35,119 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:35,260 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4140] due to args.save_total_limit
{'eval_loss': 0.20955704152584076, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.8540772532188841, 'eval_precision': 0.7653846153846153, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.7392, 'eval_samples_per_second': 699.487, 'eval_steps_per_second': 87.618, 'epoch': 47.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:37,796 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:37,796 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:37,796 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:40,444 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4320
[INFO|trainer.py:2148] 2022-07-10 15:04:40,445 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:40,587 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4230] due to args.save_total_limit
{'eval_loss': 0.20936231315135956, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8516129032258064, 'eval_precision': 0.7644787644787645, 'eval_recall': 0.9611650485436893, 'eval_runtime': 2.6444, 'eval_samples_per_second': 724.539, 'eval_steps_per_second': 90.756, 'epoch': 48.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:42,790 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:42,790 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:42,790 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:45,333 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4410
[INFO|trainer.py:2148] 2022-07-10 15:04:45,333 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:45,464 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4320] due to args.save_total_limit
{'eval_loss': 0.20932093262672424, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.8540772532188841, 'eval_precision': 0.7653846153846153, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.5388, 'eval_samples_per_second': 754.688, 'eval_steps_per_second': 94.533, 'epoch': 49.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[INFO|trainer.py:2389] 2022-07-10 15:04:48,023 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:48,023 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:48,023 >>   Batch size = 8
{'loss': 0.2533, 'learning_rate': 2.339181286549708e-08, 'epoch': 50.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-10 15:04:50,893 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4500
[INFO|trainer.py:2148] 2022-07-10 15:04:50,893 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 15:04:51,029 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4410] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 15:04:51,040 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-10 15:04:51,040 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2/checkpoint-4500 (score: 0.2093072533607483).
[INFO|trainer.py:2139] 2022-07-10 15:04:51,105 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive-ft-siamese/abtbuy-clean-COMBO-False-PROB-0.10-AUG-typo1024-typo1e-04-0.07-frozen-TinyBERT_General_4L_312D/2
[INFO|trainer.py:2148] 2022-07-10 15:04:51,105 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'eval_loss': 0.2093072533607483, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.8540772532188841, 'eval_precision': 0.7653846153846153, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.8656, 'eval_samples_per_second': 668.627, 'eval_steps_per_second': 83.753, 'epoch': 50.0}
{'train_runtime': 287.8044, 'train_samples_per_second': 997.726, 'train_steps_per_second': 15.636, 'train_loss': 0.40684752400716145, 'epoch': 50.0}
***** train metrics *****
  epoch                    =       50.0
  train_loss               =     0.4068
  train_runtime            = 0:04:47.80
  train_samples            =       5743
  train_samples_per_second =    997.726
  train_steps_per_second   =     15.636
07/10/2022 15:04:51 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-10 15:04:51,232 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-10 15:04:51,232 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:51,232 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.2093072533607483, 'eval_accuracy': 0.964509394572025, 'eval_f1': 0.8540772532188841, 'eval_precision': 0.7653846153846153, 'eval_recall': 0.9660194174757282, 'eval_runtime': 2.6151, 'eval_samples_per_second': 732.669, 'eval_steps_per_second': 91.775, 'epoch': 50.0}
***** eval metrics *****
  epoch                   =       50.0
  eval_accuracy           =     0.9645
  eval_f1                 =     0.8541
  eval_loss               =     0.2093
  eval_precision          =     0.7654
  eval_recall             =      0.966
  eval_runtime            = 0:00:02.61
  eval_samples            =       1916
  eval_samples_per_second =    732.669
  eval_steps_per_second   =     91.775
07/10/2022 15:04:53 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-10 15:04:53,864 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-10 15:04:53,864 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-10 15:04:53,864 >>   Batch size = 8
***** predict metrics *****
  predict_accuracy           =     0.9499
  predict_f1                 =     0.7913
  predict_loss               =     0.3858
  predict_precision          =     0.7165
  predict_recall             =     0.8835
  predict_runtime            = 0:00:02.62
  predict_samples            =       1916
  predict_samples_per_second =    730.754
  predict_steps_per_second   =     91.535

============================= JOB FEEDBACK =============================

NodeName=uc2n509
Job ID: 20886189
Cluster: uc2
User/Group: ma_dmittal/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:42:00
CPU Efficiency: 26.92% of 02:36:00 core-walltime
Job Wall-clock time: 00:15:36
Memory Utilized: 14.54 GB
Memory Efficiency: 0.00% of 0.00 MB
