07/09/2022 13:44:26 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
07/09/2022 13:44:26 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=True,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.EPOCH,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=True,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=True,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/runs/Jul09_13-44-26_uc2n508.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.STEPS,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=f1,
mp_parameters=,
no_cuda=False,
num_train_epochs=50.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=32,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
[INFO|tokenization_auto.py:344] 2022-07-09 13:44:27,594 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:28,072 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:28,072 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:30,638 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:30,638 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:30,638 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:30,638 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:30,638 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-09 13:44:31,062 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:31,063 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-09 13:44:31,104 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-09 13:44:31,104 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-09 13:44:31,104 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:31,561 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:31,561 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-09 13:44:31,810 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-09 13:44:32,508 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|file_utils.py:2215] 2022-07-09 13:44:32,931 >> https://huggingface.co/distilroberta-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmp1z2w8vij
Downloading:   0%|          | 0.00/480 [00:00<?, ?B/s]Downloading: 100%|██████████| 480/480 [00:00<00:00, 601kB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:33,390 >> storing https://huggingface.co/distilroberta-base/resolve/main/config.json in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|file_utils.py:2227] 2022-07-09 13:44:33,391 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:648] 2022-07-09 13:44:33,393 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-09 13:44:33,403 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|file_utils.py:2215] 2022-07-09 13:44:34,282 >> https://huggingface.co/distilroberta-base/resolve/main/vocab.json not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmpa4l33fbq
Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]Downloading:   2%|▏         | 17.0k/878k [00:00<00:05, 174kB/s]Downloading:  11%|█         | 96.0k/878k [00:00<00:02, 348kB/s]Downloading:  28%|██▊       | 250k/878k [00:00<00:00, 775kB/s] Downloading:  72%|███████▏  | 630k/878k [00:00<00:00, 1.81MB/s]Downloading: 100%|██████████| 878k/878k [00:00<00:00, 1.51MB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:35,331 >> storing https://huggingface.co/distilroberta-base/resolve/main/vocab.json in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|file_utils.py:2227] 2022-07-09 13:44:35,332 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|file_utils.py:2215] 2022-07-09 13:44:35,769 >> https://huggingface.co/distilroberta-base/resolve/main/merges.txt not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmp410deguc
Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]Downloading:  22%|██▏       | 96.0k/446k [00:00<00:00, 496kB/s]Downloading:  66%|██████▌   | 295k/446k [00:00<00:00, 1.14MB/s]Downloading: 100%|██████████| 446k/446k [00:00<00:00, 1.15MB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:36,704 >> storing https://huggingface.co/distilroberta-base/resolve/main/merges.txt in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|file_utils.py:2227] 2022-07-09 13:44:36,706 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|file_utils.py:2215] 2022-07-09 13:44:37,141 >> https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmpyapl8nii
Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]Downloading:   0%|          | 4.00k/1.29M [00:00<00:33, 40.2kB/s]Downloading:   3%|▎         | 41.0k/1.29M [00:00<00:05, 236kB/s] Downloading:   7%|▋         | 92.0k/1.29M [00:00<00:03, 364kB/s]Downloading:  15%|█▌        | 200k/1.29M [00:00<00:01, 652kB/s] Downloading:  32%|███▏      | 424k/1.29M [00:00<00:00, 1.23MB/s]Downloading:  66%|██████▌   | 872k/1.29M [00:00<00:00, 2.36MB/s]Downloading: 100%|██████████| 1.29M/1.29M [00:00<00:00, 2.22MB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:38,204 >> storing https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|file_utils.py:2227] 2022-07-09 13:44:38,205 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:39,484 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-09 13:44:39,916 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-09 13:44:39,917 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:648] 2022-07-09 13:44:40,640 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-09 13:44:40,641 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|file_utils.py:2215] 2022-07-09 13:44:41,456 >> https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmpc9teu1vo
Downloading:   0%|          | 0.00/316M [00:00<?, ?B/s]Downloading:   2%|▏         | 5.48M/316M [00:00<00:05, 57.5MB/s]Downloading:   5%|▌         | 17.0M/316M [00:00<00:03, 94.6MB/s]Downloading:   9%|▉         | 28.6M/316M [00:00<00:02, 107MB/s] Downloading:  12%|█▏        | 39.0M/316M [00:00<00:02, 108MB/s]Downloading:  16%|█▌        | 50.3M/316M [00:00<00:02, 112MB/s]Downloading:  19%|█▉        | 61.0M/316M [00:00<00:02, 111MB/s]Downloading:  23%|██▎       | 72.1M/316M [00:00<00:02, 113MB/s]Downloading:  26%|██▌       | 82.9M/316M [00:00<00:02, 113MB/s]Downloading:  30%|██▉       | 94.1M/316M [00:00<00:02, 114MB/s]Downloading:  33%|███▎      | 105M/316M [00:01<00:01, 113MB/s] Downloading:  37%|███▋      | 116M/316M [00:01<00:01, 106MB/s]Downloading:  40%|████      | 126M/316M [00:01<00:02, 85.7MB/s]Downloading:  43%|████▎     | 137M/316M [00:01<00:02, 93.1MB/s]Downloading:  47%|████▋     | 148M/316M [00:01<00:01, 98.7MB/s]Downloading:  50%|█████     | 159M/316M [00:01<00:01, 103MB/s] Downloading:  54%|█████▎    | 169M/316M [00:01<00:01, 104MB/s]Downloading:  57%|█████▋    | 181M/316M [00:01<00:01, 107MB/s]Downloading:  61%|██████    | 192M/316M [00:01<00:01, 110MB/s]Downloading:  64%|██████▍   | 202M/316M [00:02<00:01, 107MB/s]Downloading:  67%|██████▋   | 213M/316M [00:02<00:00, 108MB/s]Downloading:  71%|███████   | 224M/316M [00:02<00:00, 112MB/s]Downloading:  74%|███████▍  | 235M/316M [00:02<00:00, 110MB/s]Downloading:  78%|███████▊  | 246M/316M [00:02<00:00, 113MB/s]Downloading:  81%|████████▏ | 257M/316M [00:02<00:00, 112MB/s]Downloading:  85%|████████▍ | 268M/316M [00:02<00:00, 112MB/s]Downloading:  88%|████████▊ | 278M/316M [00:02<00:00, 109MB/s]Downloading:  92%|█████████▏| 290M/316M [00:02<00:00, 114MB/s]Downloading:  95%|█████████▌| 301M/316M [00:02<00:00, 113MB/s]Downloading:  99%|█████████▉| 312M/316M [00:03<00:00, 114MB/s]Downloading: 100%|██████████| 316M/316M [00:03<00:00, 107MB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:44,638 >> storing https://huggingface.co/distilroberta-base/resolve/main/pytorch_model.bin in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1
[INFO|file_utils.py:2227] 2022-07-09 13:44:44,640 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/7a0115a4c463f49bc7ab011872fc4a4b81be681a0434075955d29ac3388e225b.a6127d76576e81475313180aceb31a8688f7a649b80e380d26b5d30302dc83c1
[INFO|tokenization_auto.py:344] 2022-07-09 13:44:46,893 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:47,309 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:47,309 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:49,858 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:49,858 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:49,858 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:49,858 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:49,858 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-09 13:44:50,277 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:50,278 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-09 13:44:50,375 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-09 13:44:50,375 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-09 13:44:50,375 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:50,803 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:50,803 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-09 13:44:50,815 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-09 13:44:51,378 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:51,795 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:51,796 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:54,372 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:54,372 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:54,372 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:54,372 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-09 13:44:54,372 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-09 13:44:54,799 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:54,800 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-09 13:44:54,820 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-09 13:44:54,820 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-09 13:44:54,820 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:55,244 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:55,244 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-09 13:44:55,255 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-09 13:44:55,796 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:44:55,797 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|file_utils.py:2215] 2022-07-09 13:44:56,382 >> https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/tmp4kwjh65t
Downloading:   0%|          | 0.00/59.8M [00:00<?, ?B/s]Downloading:  11%|█▏        | 6.82M/59.8M [00:00<00:00, 71.5MB/s]Downloading:  31%|███       | 18.5M/59.8M [00:00<00:00, 101MB/s] Downloading:  49%|████▊     | 29.1M/59.8M [00:00<00:00, 106MB/s]Downloading:  67%|██████▋   | 40.0M/59.8M [00:00<00:00, 109MB/s]Downloading:  86%|████████▋ | 51.7M/59.8M [00:00<00:00, 114MB/s]Downloading: 100%|██████████| 59.8M/59.8M [00:00<00:00, 109MB/s]
[INFO|file_utils.py:2219] 2022-07-09 13:44:57,011 >> storing https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin in cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[INFO|file_utils.py:2227] 2022-07-09 13:44:57,013 >> creating metadata file for /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[INFO|modeling_utils.py:1431] 2022-07-09 13:44:57,014 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-09 13:44:57,136 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.2.weight', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'fit_denses.1.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-09 13:44:57,137 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-09 13:45:07,153 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-09 13:45:07,174 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-09 13:45:07,174 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-09 13:45:07,174 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-09 13:45:07,174 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-09 13:45:07,174 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-09 13:45:07,174 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-09 13:45:07,174 >>   Total optimization steps = 9000
[INFO|trainer.py:2389] 2022-07-09 13:45:29,626 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:45:29,627 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:45:29,627 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:45:31,412 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-180
[INFO|configuration_utils.py:439] 2022-07-09 13:45:31,415 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-180/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:45:31,592 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-180/pytorch_model.bin
{'eval_loss': 0.25237026810646057, 'eval_accuracy': 0.8862212943632568, 'eval_f1': 0.5992647058823529, 'eval_precision': 0.4822485207100592, 'eval_recall': 0.7912621359223301, 'eval_runtime': 1.7811, 'eval_samples_per_second': 1075.73, 'eval_steps_per_second': 134.747, 'epoch': 1.0}
[INFO|trainer.py:2389] 2022-07-09 13:45:38,126 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:45:38,126 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:45:38,127 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:45:39,731 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-360
[INFO|configuration_utils.py:439] 2022-07-09 13:45:39,733 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-360/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:45:39,860 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-360/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:45:40,113 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.15340352058410645, 'eval_accuracy': 0.9446764091858038, 'eval_f1': 0.756880733944954, 'eval_precision': 0.717391304347826, 'eval_recall': 0.8009708737864077, 'eval_runtime': 1.5998, 'eval_samples_per_second': 1197.66, 'eval_steps_per_second': 150.02, 'epoch': 2.0}
[INFO|trainer.py:2389] 2022-07-09 13:45:46,303 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:45:46,303 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:45:46,303 >>   Batch size = 8
{'loss': 0.2862, 'learning_rate': 9.942690058479533e-05, 'epoch': 2.78}
[INFO|trainer.py:2139] 2022-07-09 13:45:47,939 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-540
[INFO|configuration_utils.py:439] 2022-07-09 13:45:47,941 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-540/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:45:48,064 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-540/pytorch_model.bin
{'eval_loss': 0.19474181532859802, 'eval_accuracy': 0.9342379958246346, 'eval_f1': 0.676923076923077, 'eval_precision': 0.717391304347826, 'eval_recall': 0.6407766990291263, 'eval_runtime': 1.6325, 'eval_samples_per_second': 1173.665, 'eval_steps_per_second': 147.014, 'epoch': 3.0}
[INFO|trainer.py:2389] 2022-07-09 13:45:54,723 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:45:54,723 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:45:54,723 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:45:56,555 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-720
[INFO|configuration_utils.py:439] 2022-07-09 13:45:56,557 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-720/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:45:56,680 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-720/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:45:56,962 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-540] due to args.save_total_limit
{'eval_loss': 0.15519481897354126, 'eval_accuracy': 0.9405010438413361, 'eval_f1': 0.738532110091743, 'eval_precision': 0.7, 'eval_recall': 0.7815533980582524, 'eval_runtime': 1.8259, 'eval_samples_per_second': 1049.323, 'eval_steps_per_second': 131.439, 'epoch': 4.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:03,189 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:03,189 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:03,189 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:46:04,937 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-900
[INFO|configuration_utils.py:439] 2022-07-09 13:46:04,939 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-900/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:05,068 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-900/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:05,329 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.15725819766521454, 'eval_accuracy': 0.9368475991649269, 'eval_f1': 0.7430997876857749, 'eval_precision': 0.660377358490566, 'eval_recall': 0.8495145631067961, 'eval_runtime': 1.7429, 'eval_samples_per_second': 1099.34, 'eval_steps_per_second': 137.704, 'epoch': 5.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:11,535 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:11,535 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:11,535 >>   Batch size = 8
{'loss': 0.154, 'learning_rate': 9.357894736842106e-05, 'epoch': 5.56}
[INFO|trainer.py:2139] 2022-07-09 13:46:13,228 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1080
[INFO|configuration_utils.py:439] 2022-07-09 13:46:13,230 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1080/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:13,370 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1080/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:13,632 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-360] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:46:13,679 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.17150644958019257, 'eval_accuracy': 0.9451983298538622, 'eval_f1': 0.7586206896551724, 'eval_precision': 0.7205240174672489, 'eval_recall': 0.8009708737864077, 'eval_runtime': 1.6886, 'eval_samples_per_second': 1134.687, 'eval_steps_per_second': 142.132, 'epoch': 6.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:20,067 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:20,067 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:20,067 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:46:21,747 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1260
[INFO|configuration_utils.py:439] 2022-07-09 13:46:21,750 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1260/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:21,878 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1260/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:22,151 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.1228400468826294, 'eval_accuracy': 0.9577244258872651, 'eval_f1': 0.7885117493472584, 'eval_precision': 0.8531073446327684, 'eval_recall': 0.7330097087378641, 'eval_runtime': 1.6755, 'eval_samples_per_second': 1143.514, 'eval_steps_per_second': 143.238, 'epoch': 7.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:28,404 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:28,405 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:28,405 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:46:30,320 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1440
[INFO|configuration_utils.py:439] 2022-07-09 13:46:30,323 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1440/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:30,478 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1440/pytorch_model.bin
{'eval_loss': 0.13228043913841248, 'eval_accuracy': 0.954070981210856, 'eval_f1': 0.7843137254901961, 'eval_precision': 0.7920792079207921, 'eval_recall': 0.7766990291262136, 'eval_runtime': 1.9105, 'eval_samples_per_second': 1002.883, 'eval_steps_per_second': 125.622, 'epoch': 8.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:37,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:37,060 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:37,060 >>   Batch size = 8
{'loss': 0.1175, 'learning_rate': 8.773099415204679e-05, 'epoch': 8.33}
[INFO|trainer.py:2139] 2022-07-09 13:46:38,780 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1620
[INFO|configuration_utils.py:439] 2022-07-09 13:46:38,782 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1620/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:38,904 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1620/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:39,224 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.17090219259262085, 'eval_accuracy': 0.9399791231732777, 'eval_f1': 0.7547974413646055, 'eval_precision': 0.6730038022813688, 'eval_recall': 0.8592233009708737, 'eval_runtime': 1.714, 'eval_samples_per_second': 1117.863, 'eval_steps_per_second': 140.025, 'epoch': 9.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:45,521 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:45,521 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:45,521 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:46:47,291 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1800
[INFO|configuration_utils.py:439] 2022-07-09 13:46:47,303 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1800/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:47,492 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1800/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:47,834 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1620] due to args.save_total_limit
{'eval_loss': 0.27495038509368896, 'eval_accuracy': 0.9144050104384134, 'eval_f1': 0.6951672862453532, 'eval_precision': 0.5632530120481928, 'eval_recall': 0.9077669902912622, 'eval_runtime': 1.7653, 'eval_samples_per_second': 1085.356, 'eval_steps_per_second': 135.953, 'epoch': 10.0}
[INFO|trainer.py:2389] 2022-07-09 13:46:53,943 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:46:53,943 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:46:53,944 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:46:55,508 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1980
[INFO|configuration_utils.py:439] 2022-07-09 13:46:55,519 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1980/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:46:55,640 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1980/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:46:56,000 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1260] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:46:56,032 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.15741664171218872, 'eval_accuracy': 0.9608559498956158, 'eval_f1': 0.8120300751879699, 'eval_precision': 0.8393782383419689, 'eval_recall': 0.7864077669902912, 'eval_runtime': 1.5591, 'eval_samples_per_second': 1228.914, 'eval_steps_per_second': 153.935, 'epoch': 11.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:02,199 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:02,199 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:02,199 >>   Batch size = 8
{'loss': 0.096, 'learning_rate': 8.188304093567251e-05, 'epoch': 11.11}
[INFO|trainer.py:2139] 2022-07-09 13:47:03,838 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2160
[INFO|configuration_utils.py:439] 2022-07-09 13:47:03,840 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2160/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:03,963 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2160/pytorch_model.bin
{'eval_loss': 0.12341689318418503, 'eval_accuracy': 0.9551148225469729, 'eval_f1': 0.7990654205607476, 'eval_precision': 0.7702702702702703, 'eval_recall': 0.8300970873786407, 'eval_runtime': 1.6335, 'eval_samples_per_second': 1172.959, 'eval_steps_per_second': 146.926, 'epoch': 12.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:10,452 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:10,452 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:10,452 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:47:12,227 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2340
[INFO|configuration_utils.py:439] 2022-07-09 13:47:12,229 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2340/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:12,392 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2340/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:12,726 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.17579956352710724, 'eval_accuracy': 0.9545929018789144, 'eval_f1': 0.7990762124711317, 'eval_precision': 0.762114537444934, 'eval_recall': 0.8398058252427184, 'eval_runtime': 1.7701, 'eval_samples_per_second': 1082.398, 'eval_steps_per_second': 135.582, 'epoch': 13.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:19,185 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:19,185 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:19,185 >>   Batch size = 8
{'loss': 0.083, 'learning_rate': 7.605847953216376e-05, 'epoch': 13.89}
[INFO|trainer.py:2139] 2022-07-09 13:47:21,346 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2520
[INFO|configuration_utils.py:439] 2022-07-09 13:47:21,348 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2520/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:21,507 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2520/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:21,779 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2340] due to args.save_total_limit
{'eval_loss': 0.16412866115570068, 'eval_accuracy': 0.9525052192066806, 'eval_f1': 0.7888631090487238, 'eval_precision': 0.7555555555555555, 'eval_recall': 0.8252427184466019, 'eval_runtime': 2.1574, 'eval_samples_per_second': 888.096, 'eval_steps_per_second': 111.244, 'epoch': 14.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:28,014 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:28,015 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:28,015 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:47:29,749 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2700
[INFO|configuration_utils.py:439] 2022-07-09 13:47:29,752 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2700/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:29,878 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2700/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:30,159 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.16313880681991577, 'eval_accuracy': 0.958768267223382, 'eval_f1': 0.7926509186351706, 'eval_precision': 0.8628571428571429, 'eval_recall': 0.7330097087378641, 'eval_runtime': 1.7294, 'eval_samples_per_second': 1107.918, 'eval_steps_per_second': 138.779, 'epoch': 15.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:36,349 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:36,350 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:36,350 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:47:38,014 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2880
[INFO|configuration_utils.py:439] 2022-07-09 13:47:38,017 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2880/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:38,158 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2880/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:38,406 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2700] due to args.save_total_limit
{'eval_loss': 0.18772238492965698, 'eval_accuracy': 0.9556367432150313, 'eval_f1': 0.7999999999999999, 'eval_precision': 0.776255707762557, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.6601, 'eval_samples_per_second': 1154.129, 'eval_steps_per_second': 144.567, 'epoch': 16.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:44,583 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:44,583 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:44,583 >>   Batch size = 8
{'loss': 0.0751, 'learning_rate': 7.021052631578949e-05, 'epoch': 16.67}
[INFO|trainer.py:2139] 2022-07-09 13:47:46,344 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3060
[INFO|configuration_utils.py:439] 2022-07-09 13:47:46,346 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3060/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:46,538 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3060/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:46,841 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.1988450139760971, 'eval_accuracy': 0.9478079331941545, 'eval_f1': 0.7767857142857142, 'eval_precision': 0.71900826446281, 'eval_recall': 0.8446601941747572, 'eval_runtime': 1.7551, 'eval_samples_per_second': 1091.65, 'eval_steps_per_second': 136.741, 'epoch': 17.0}
[INFO|trainer.py:2389] 2022-07-09 13:47:53,480 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:47:53,480 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:47:53,480 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:47:55,344 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3240
[INFO|configuration_utils.py:439] 2022-07-09 13:47:55,347 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3240/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:47:55,503 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3240/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:47:55,811 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.1777455359697342, 'eval_accuracy': 0.9504175365344467, 'eval_f1': 0.7865168539325842, 'eval_precision': 0.7322175732217573, 'eval_recall': 0.8495145631067961, 'eval_runtime': 1.8592, 'eval_samples_per_second': 1030.577, 'eval_steps_per_second': 129.091, 'epoch': 18.0}
[INFO|trainer.py:2389] 2022-07-09 13:48:02,039 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:02,039 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:02,039 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:48:03,770 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3420
[INFO|configuration_utils.py:439] 2022-07-09 13:48:03,773 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3420/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:03,902 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3420/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:48:04,355 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.16776062548160553, 'eval_accuracy': 0.9551148225469729, 'eval_f1': 0.7860696517412936, 'eval_precision': 0.8061224489795918, 'eval_recall': 0.7669902912621359, 'eval_runtime': 1.7249, 'eval_samples_per_second': 1110.802, 'eval_steps_per_second': 139.14, 'epoch': 19.0}
[INFO|trainer.py:2389] 2022-07-09 13:48:10,752 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:10,753 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:10,753 >>   Batch size = 8
{'loss': 0.0656, 'learning_rate': 6.436257309941522e-05, 'epoch': 19.44}
[INFO|trainer.py:2139] 2022-07-09 13:48:12,452 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3600
[INFO|configuration_utils.py:439] 2022-07-09 13:48:12,461 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3600/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:12,582 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3600/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:48:12,916 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.21875588595867157, 'eval_accuracy': 0.9478079331941545, 'eval_f1': 0.7727272727272726, 'eval_precision': 0.7264957264957265, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.6952, 'eval_samples_per_second': 1130.281, 'eval_steps_per_second': 141.58, 'epoch': 20.0}
[INFO|trainer.py:2389] 2022-07-09 13:48:19,147 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:19,147 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:19,147 >>   Batch size = 8
[INFO|trainer.py:2139] 2022-07-09 13:48:20,940 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3780
[INFO|configuration_utils.py:439] 2022-07-09 13:48:20,942 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3780/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:21,069 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3780/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:48:21,333 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-3600] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-09 13:48:21,380 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-09 13:48:21,380 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/checkpoint-1980 (score: 0.8120300751879699).
[INFO|trainer.py:2139] 2022-07-09 13:48:21,447 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0
[INFO|configuration_utils.py:439] 2022-07-09 13:48:21,449 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:21,599 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/0/pytorch_model.bin
{'eval_loss': 0.19413365423679352, 'eval_accuracy': 0.9467640918580376, 'eval_f1': 0.7660550458715597, 'eval_precision': 0.7260869565217392, 'eval_recall': 0.8106796116504854, 'eval_runtime': 1.7886, 'eval_samples_per_second': 1071.203, 'eval_steps_per_second': 134.18, 'epoch': 21.0}
{'train_runtime': 194.2665, 'train_samples_per_second': 1478.124, 'train_steps_per_second': 46.328, 'train_loss': 0.12057959137770234, 'epoch': 21.0}
***** train metrics *****
  epoch                    =       21.0
  train_loss               =     0.1206
  train_runtime            = 0:03:14.26
  train_samples            =       5743
  train_samples_per_second =   1478.124
  train_steps_per_second   =     46.328
07/09/2022 13:48:21 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-09 13:48:21,609 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:21,609 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:21,609 >>   Batch size = 8
{'eval_loss': 0.15741664171218872, 'eval_accuracy': 0.9608559498956158, 'eval_f1': 0.8120300751879699, 'eval_precision': 0.8393782383419689, 'eval_recall': 0.7864077669902912, 'eval_runtime': 1.8019, 'eval_samples_per_second': 1063.296, 'eval_steps_per_second': 133.19, 'epoch': 21.0}
***** eval metrics *****
  epoch                   =       21.0
  eval_accuracy           =     0.9609
  eval_f1                 =      0.812
  eval_loss               =     0.1574
  eval_precision          =     0.8394
  eval_recall             =     0.7864
  eval_runtime            = 0:00:01.80
  eval_samples            =       1916
  eval_samples_per_second =   1063.296
  eval_steps_per_second   =     133.19
07/09/2022 13:48:23 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-09 13:48:23,524 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-09 13:48:23,524 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:23,524 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-09 13:48:26,145 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:48:26,145 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-09 13:48:26,568 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-09 13:48:26,720 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.2.weight', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'fit_denses.1.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-09 13:48:26,721 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-09 13:48:26,857 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-09 13:48:26,863 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-09 13:48:26,863 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-09 13:48:26,863 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-09 13:48:26,864 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-09 13:48:26,864 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-09 13:48:26,864 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-09 13:48:26,864 >>   Total optimization steps = 9000
***** predict metrics *****
  predict_accuracy           =     0.9619
  predict_f1                 =     0.8215
  predict_loss               =      0.148
  predict_precision          =     0.8276
  predict_recall             =     0.8155
  predict_runtime            = 0:00:02.19
  predict_samples            =       1916
  predict_samples_per_second =    871.947
  predict_steps_per_second   =    109.221
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:48:33,238 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:33,239 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:33,239 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:48:34,788 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-180
[INFO|configuration_utils.py:439] 2022-07-09 13:48:34,790 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-180/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:34,917 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-180/pytorch_model.bin
{'eval_loss': 0.30402320623397827, 'eval_accuracy': 0.8512526096033403, 'eval_f1': 0.544, 'eval_precision': 0.40572792362768495, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.5436, 'eval_samples_per_second': 1241.241, 'eval_steps_per_second': 155.479, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:48:41,585 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:41,585 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:41,586 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:48:43,434 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-360
[INFO|configuration_utils.py:439] 2022-07-09 13:48:43,437 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-360/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:43,597 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-360/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:48:43,852 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.19641096889972687, 'eval_accuracy': 0.9117954070981211, 'eval_f1': 0.6829268292682926, 'eval_precision': 0.5565749235474006, 'eval_recall': 0.883495145631068, 'eval_runtime': 1.8423, 'eval_samples_per_second': 1040.023, 'eval_steps_per_second': 130.274, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:48:50,224 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:50,224 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:50,224 >>   Batch size = 8
{'loss': 0.2896, 'learning_rate': 9.942690058479533e-05, 'epoch': 2.78}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:48:52,013 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-540
[INFO|configuration_utils.py:439] 2022-07-09 13:48:52,015 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-540/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:48:52,144 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-540/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:48:52,424 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-360] due to args.save_total_limit
{'eval_loss': 0.14169320464134216, 'eval_accuracy': 0.9535490605427975, 'eval_f1': 0.7700258397932818, 'eval_precision': 0.8232044198895028, 'eval_recall': 0.7233009708737864, 'eval_runtime': 1.7835, 'eval_samples_per_second': 1074.268, 'eval_steps_per_second': 134.564, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:48:58,519 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:48:58,519 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:48:58,519 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:00,248 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-720
[INFO|configuration_utils.py:439] 2022-07-09 13:49:00,250 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-720/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:00,377 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-720/pytorch_model.bin
{'eval_loss': 0.23098227381706238, 'eval_accuracy': 0.9044885177453027, 'eval_f1': 0.6690777576853526, 'eval_precision': 0.5331412103746398, 'eval_recall': 0.8980582524271845, 'eval_runtime': 1.7239, 'eval_samples_per_second': 1111.458, 'eval_steps_per_second': 139.222, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:06,805 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:06,805 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:06,805 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:08,472 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-900
[INFO|configuration_utils.py:439] 2022-07-09 13:49:08,475 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-900/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:08,595 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-900/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:08,986 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.15577846765518188, 'eval_accuracy': 0.9462421711899791, 'eval_f1': 0.765375854214123, 'eval_precision': 0.721030042918455, 'eval_recall': 0.8155339805825242, 'eval_runtime': 1.6613, 'eval_samples_per_second': 1153.288, 'eval_steps_per_second': 144.462, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:15,472 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:15,473 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:15,473 >>   Batch size = 8
{'loss': 0.1605, 'learning_rate': 9.357894736842106e-05, 'epoch': 5.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:17,215 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1080
[INFO|configuration_utils.py:439] 2022-07-09 13:49:17,217 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1080/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:17,339 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1080/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:17,662 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.15644147992134094, 'eval_accuracy': 0.9405010438413361, 'eval_f1': 0.7488986784140969, 'eval_precision': 0.6854838709677419, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.7367, 'eval_samples_per_second': 1103.215, 'eval_steps_per_second': 138.19, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:23,722 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:23,722 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:23,722 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:25,755 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1260
[INFO|configuration_utils.py:439] 2022-07-09 13:49:25,757 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1260/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:25,886 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1260/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:26,153 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.14533397555351257, 'eval_accuracy': 0.9530271398747391, 'eval_f1': 0.7499999999999999, 'eval_precision': 0.8766233766233766, 'eval_recall': 0.6553398058252428, 'eval_runtime': 2.0266, 'eval_samples_per_second': 945.407, 'eval_steps_per_second': 118.423, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:32,398 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:32,398 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:32,398 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:34,062 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1440
[INFO|configuration_utils.py:439] 2022-07-09 13:49:34,065 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1440/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:34,214 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1440/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:34,457 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1260] due to args.save_total_limit
{'eval_loss': 0.18771280348300934, 'eval_accuracy': 0.9530271398747391, 'eval_f1': 0.761904761904762, 'eval_precision': 0.8372093023255814, 'eval_recall': 0.6990291262135923, 'eval_runtime': 1.6574, 'eval_samples_per_second': 1156.02, 'eval_steps_per_second': 144.804, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:40,646 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:40,647 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:40,647 >>   Batch size = 8
{'loss': 0.1126, 'learning_rate': 8.773099415204679e-05, 'epoch': 8.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:42,249 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1620
[INFO|configuration_utils.py:439] 2022-07-09 13:49:42,260 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1620/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:42,390 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1620/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:42,671 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.21583710610866547, 'eval_accuracy': 0.9243215031315241, 'eval_f1': 0.716242661448141, 'eval_precision': 0.6, 'eval_recall': 0.8883495145631068, 'eval_runtime': 1.5959, 'eval_samples_per_second': 1200.59, 'eval_steps_per_second': 150.387, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:49,020 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:49,020 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:49,020 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:50,811 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1800
[INFO|configuration_utils.py:439] 2022-07-09 13:49:50,813 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1800/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:50,953 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1800/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:51,208 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-540] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:49:51,255 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1620] due to args.save_total_limit
{'eval_loss': 0.18507878482341766, 'eval_accuracy': 0.9493736951983298, 'eval_f1': 0.7749419953596287, 'eval_precision': 0.7422222222222222, 'eval_recall': 0.8106796116504854, 'eval_runtime': 1.7841, 'eval_samples_per_second': 1073.919, 'eval_steps_per_second': 134.52, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:49:57,451 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:49:57,451 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:49:57,451 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:49:59,183 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1980
[INFO|configuration_utils.py:439] 2022-07-09 13:49:59,185 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1980/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:49:59,312 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1980/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:49:59,616 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.15373629331588745, 'eval_accuracy': 0.9582463465553236, 'eval_f1': 0.7969543147208121, 'eval_precision': 0.8351063829787234, 'eval_recall': 0.7621359223300971, 'eval_runtime': 1.7266, 'eval_samples_per_second': 1109.704, 'eval_steps_per_second': 139.003, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:06,257 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:06,257 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:06,257 >>   Batch size = 8
{'loss': 0.0928, 'learning_rate': 8.189473684210527e-05, 'epoch': 11.11}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:08,110 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2160
[INFO|configuration_utils.py:439] 2022-07-09 13:50:08,112 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2160/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:08,260 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2160/pytorch_model.bin
{'eval_loss': 0.18394725024700165, 'eval_accuracy': 0.9493736951983298, 'eval_f1': 0.7810383747178328, 'eval_precision': 0.729957805907173, 'eval_recall': 0.8398058252427184, 'eval_runtime': 1.8482, 'eval_samples_per_second': 1036.7, 'eval_steps_per_second': 129.858, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:14,576 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:14,577 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:14,577 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:16,124 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2340
[INFO|configuration_utils.py:439] 2022-07-09 13:50:16,127 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2340/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:16,249 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2340/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:50:16,494 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.18178245425224304, 'eval_accuracy': 0.9566805845511482, 'eval_f1': 0.7919799498746868, 'eval_precision': 0.8186528497409327, 'eval_recall': 0.7669902912621359, 'eval_runtime': 1.5421, 'eval_samples_per_second': 1242.451, 'eval_steps_per_second': 155.631, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:22,798 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:22,798 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:22,798 >>   Batch size = 8
{'loss': 0.0799, 'learning_rate': 7.6046783625731e-05, 'epoch': 13.89}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:24,536 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2520
[INFO|configuration_utils.py:439] 2022-07-09 13:50:24,539 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2520/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:24,671 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2520/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:50:24,931 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2340] due to args.save_total_limit
{'eval_loss': 0.19736608862876892, 'eval_accuracy': 0.9467640918580376, 'eval_f1': 0.7627906976744185, 'eval_precision': 0.7321428571428571, 'eval_recall': 0.7961165048543689, 'eval_runtime': 1.7334, 'eval_samples_per_second': 1105.343, 'eval_steps_per_second': 138.456, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:31,105 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:31,105 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:31,105 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:32,855 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2700
[INFO|configuration_utils.py:439] 2022-07-09 13:50:32,858 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2700/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:32,984 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2700/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:50:33,383 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-1980] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:50:33,419 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.1854454129934311, 'eval_accuracy': 0.9592901878914405, 'eval_f1': 0.8040201005025125, 'eval_precision': 0.8333333333333334, 'eval_recall': 0.7766990291262136, 'eval_runtime': 1.744, 'eval_samples_per_second': 1098.6, 'eval_steps_per_second': 137.612, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:39,764 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:39,764 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:39,764 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:41,526 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2880
[INFO|configuration_utils.py:439] 2022-07-09 13:50:41,528 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2880/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:41,656 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2880/pytorch_model.bin
{'eval_loss': 0.16043546795845032, 'eval_accuracy': 0.9535490605427975, 'eval_f1': 0.7855421686746988, 'eval_precision': 0.7799043062200957, 'eval_recall': 0.7912621359223301, 'eval_runtime': 1.7559, 'eval_samples_per_second': 1091.162, 'eval_steps_per_second': 136.68, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:48,107 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:48,107 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:48,108 >>   Batch size = 8
{'loss': 0.0732, 'learning_rate': 7.019883040935674e-05, 'epoch': 16.67}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:49,798 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3060
[INFO|configuration_utils.py:439] 2022-07-09 13:50:49,801 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3060/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:49,923 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3060/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:50:50,167 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.18956749141216278, 'eval_accuracy': 0.9556367432150313, 'eval_f1': 0.7848101265822784, 'eval_precision': 0.8201058201058201, 'eval_recall': 0.7524271844660194, 'eval_runtime': 1.686, 'eval_samples_per_second': 1136.425, 'eval_steps_per_second': 142.35, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:50:56,354 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:50:56,354 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:50:56,354 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:50:58,150 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3240
[INFO|configuration_utils.py:439] 2022-07-09 13:50:58,153 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3240/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:50:58,282 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3240/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:50:58,635 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.17992670834064484, 'eval_accuracy': 0.958768267223382, 'eval_f1': 0.803970223325062, 'eval_precision': 0.8223350253807107, 'eval_recall': 0.7864077669902912, 'eval_runtime': 1.7905, 'eval_samples_per_second': 1070.108, 'eval_steps_per_second': 134.043, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:04,726 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:04,726 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:04,726 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:06,509 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3420
[INFO|configuration_utils.py:439] 2022-07-09 13:51:06,511 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3420/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:06,637 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3420/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:06,882 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.20530040562152863, 'eval_accuracy': 0.9478079331941545, 'eval_f1': 0.7777777777777778, 'eval_precision': 0.7172131147540983, 'eval_recall': 0.8495145631067961, 'eval_runtime': 1.7759, 'eval_samples_per_second': 1078.882, 'eval_steps_per_second': 135.142, 'epoch': 19.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:13,060 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:13,060 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:13,060 >>   Batch size = 8
{'loss': 0.0726, 'learning_rate': 6.435087719298246e-05, 'epoch': 19.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:14,784 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3600
[INFO|configuration_utils.py:439] 2022-07-09 13:51:14,787 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3600/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:14,916 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3600/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:15,204 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.1999015212059021, 'eval_accuracy': 0.9561586638830898, 'eval_f1': 0.79, 'eval_precision': 0.8144329896907216, 'eval_recall': 0.7669902912621359, 'eval_runtime': 1.718, 'eval_samples_per_second': 1115.281, 'eval_steps_per_second': 139.701, 'epoch': 20.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:21,494 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:21,494 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:21,495 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:23,267 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3780
[INFO|configuration_utils.py:439] 2022-07-09 13:51:23,269 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3780/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:23,391 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3780/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:23,645 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3600] due to args.save_total_limit
{'eval_loss': 0.20535147190093994, 'eval_accuracy': 0.9592901878914405, 'eval_f1': 0.8020304568527918, 'eval_precision': 0.8404255319148937, 'eval_recall': 0.7669902912621359, 'eval_runtime': 1.7658, 'eval_samples_per_second': 1085.069, 'eval_steps_per_second': 135.917, 'epoch': 21.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:29,822 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:29,822 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:29,822 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:31,614 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3960
[INFO|configuration_utils.py:439] 2022-07-09 13:51:31,621 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3960/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:31,747 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3960/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:32,010 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3780] due to args.save_total_limit
{'eval_loss': 0.17786912620067596, 'eval_accuracy': 0.9551148225469729, 'eval_f1': 0.7971698113207547, 'eval_precision': 0.7752293577981652, 'eval_recall': 0.8203883495145631, 'eval_runtime': 1.7867, 'eval_samples_per_second': 1072.35, 'eval_steps_per_second': 134.324, 'epoch': 22.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:38,572 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:38,572 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:38,572 >>   Batch size = 8
{'loss': 0.0621, 'learning_rate': 5.851461988304093e-05, 'epoch': 22.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:40,503 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4140
[INFO|configuration_utils.py:439] 2022-07-09 13:51:40,505 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4140/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:40,629 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4140/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:40,901 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-3960] due to args.save_total_limit
{'eval_loss': 0.18535713851451874, 'eval_accuracy': 0.954070981210856, 'eval_f1': 0.7894736842105263, 'eval_precision': 0.7783018867924528, 'eval_recall': 0.8009708737864077, 'eval_runtime': 1.9248, 'eval_samples_per_second': 995.454, 'eval_steps_per_second': 124.691, 'epoch': 23.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:47,238 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:47,238 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:47,238 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:48,991 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4320
[INFO|configuration_utils.py:439] 2022-07-09 13:51:48,994 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4320/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:49,138 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4320/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:49,376 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4140] due to args.save_total_limit
{'eval_loss': 0.1946946084499359, 'eval_accuracy': 0.9556367432150313, 'eval_f1': 0.8018648018648018, 'eval_precision': 0.7713004484304933, 'eval_recall': 0.8349514563106796, 'eval_runtime': 1.7474, 'eval_samples_per_second': 1096.459, 'eval_steps_per_second': 137.343, 'epoch': 24.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:51:55,832 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:55,832 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:55,832 >>   Batch size = 8
{'loss': 0.0543, 'learning_rate': 5.266666666666666e-05, 'epoch': 25.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:51:57,599 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4500
[INFO|configuration_utils.py:439] 2022-07-09 13:51:57,602 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4500/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:57,736 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4500/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:51:57,994 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-4320] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-09 13:51:58,031 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-09 13:51:58,031 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/checkpoint-2700 (score: 0.8040201005025125).
[INFO|trainer.py:2139] 2022-07-09 13:51:58,099 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1
[INFO|configuration_utils.py:439] 2022-07-09 13:51:58,101 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:51:58,250 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/1/pytorch_model.bin
{'eval_loss': 0.1661515235900879, 'eval_accuracy': 0.9608559498956158, 'eval_f1': 0.8031496062992126, 'eval_precision': 0.8742857142857143, 'eval_recall': 0.7427184466019418, 'eval_runtime': 1.7604, 'eval_samples_per_second': 1088.362, 'eval_steps_per_second': 136.329, 'epoch': 25.0}
{'train_runtime': 211.2284, 'train_samples_per_second': 1359.429, 'train_steps_per_second': 42.608, 'train_loss': 0.11084310870700412, 'epoch': 25.0}
***** train metrics *****
  epoch                    =       25.0
  train_loss               =     0.1108
  train_runtime            = 0:03:31.22
  train_samples            =       5743
  train_samples_per_second =   1359.429
  train_steps_per_second   =     42.608
07/09/2022 13:51:58 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-09 13:51:58,258 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:51:58,258 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:58,258 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.1854454129934311, 'eval_accuracy': 0.9592901878914405, 'eval_f1': 0.8040201005025125, 'eval_precision': 0.8333333333333334, 'eval_recall': 0.7766990291262136, 'eval_runtime': 1.5609, 'eval_samples_per_second': 1227.523, 'eval_steps_per_second': 153.761, 'epoch': 25.0}
***** eval metrics *****
  epoch                   =       25.0
  eval_accuracy           =     0.9593
  eval_f1                 =      0.804
  eval_loss               =     0.1854
  eval_precision          =     0.8333
  eval_recall             =     0.7767
  eval_runtime            = 0:00:01.56
  eval_samples            =       1916
  eval_samples_per_second =   1227.523
  eval_steps_per_second   =    153.761
07/09/2022 13:51:59 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-09 13:51:59,873 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-09 13:51:59,873 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:51:59,873 >>   Batch size = 8
[INFO|configuration_utils.py:648] 2022-07-09 13:52:02,594 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-09 13:52:02,595 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-09 13:52:03,022 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-09 13:52:03,182 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertForSequenceClassification: ['fit_denses.2.weight', 'fit_denses.3.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'fit_denses.2.bias', 'fit_denses.3.weight', 'cls.predictions.bias', 'fit_denses.0.weight', 'fit_denses.1.weight', 'fit_denses.4.weight', 'fit_denses.4.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'fit_denses.1.bias']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:1704] 2022-07-09 13:52:03,182 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|trainer.py:457] 2022-07-09 13:52:03,315 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-09 13:52:03,322 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-09 13:52:03,322 >>   Num examples = 5743
[INFO|trainer.py:1281] 2022-07-09 13:52:03,322 >>   Num Epochs = 50
[INFO|trainer.py:1282] 2022-07-09 13:52:03,322 >>   Instantaneous batch size per device = 32
[INFO|trainer.py:1283] 2022-07-09 13:52:03,322 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:1284] 2022-07-09 13:52:03,322 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-09 13:52:03,322 >>   Total optimization steps = 9000
***** predict metrics *****
  predict_accuracy           =     0.9582
  predict_f1                 =     0.8077
  predict_loss               =     0.1792
  predict_precision          =        0.8
  predict_recall             =     0.8155
  predict_runtime            = 0:00:02.22
  predict_samples            =       1916
  predict_samples_per_second =    862.732
  predict_steps_per_second   =    108.067
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:09,874 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:09,874 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:09,874 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:11,481 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-180
[INFO|configuration_utils.py:439] 2022-07-09 13:52:11,484 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-180/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:11,629 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-180/pytorch_model.bin
{'eval_loss': 0.24171209335327148, 'eval_accuracy': 0.9138830897703549, 'eval_f1': 0.5822784810126582, 'eval_precision': 0.6084656084656085, 'eval_recall': 0.558252427184466, 'eval_runtime': 1.6021, 'eval_samples_per_second': 1195.903, 'eval_steps_per_second': 149.8, 'epoch': 1.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:18,256 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:18,256 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:18,256 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:19,971 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-360
[INFO|configuration_utils.py:439] 2022-07-09 13:52:19,973 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-360/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:20,109 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-360/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:52:20,376 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-180] due to args.save_total_limit
{'eval_loss': 0.16695252060890198, 'eval_accuracy': 0.9431106471816284, 'eval_f1': 0.7046070460704608, 'eval_precision': 0.7975460122699386, 'eval_recall': 0.6310679611650486, 'eval_runtime': 1.7099, 'eval_samples_per_second': 1120.542, 'eval_steps_per_second': 140.36, 'epoch': 2.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:26,886 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:26,886 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:26,886 >>   Batch size = 8
{'loss': 0.2968, 'learning_rate': 9.942690058479533e-05, 'epoch': 2.78}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:28,690 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-540
[INFO|configuration_utils.py:439] 2022-07-09 13:52:28,694 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-540/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:28,834 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-540/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:52:29,121 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-360] due to args.save_total_limit
{'eval_loss': 0.16911908984184265, 'eval_accuracy': 0.9389352818371608, 'eval_f1': 0.7439824945295405, 'eval_precision': 0.6772908366533864, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.7983, 'eval_samples_per_second': 1065.437, 'eval_steps_per_second': 133.458, 'epoch': 3.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:35,328 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:35,328 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:35,328 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:36,977 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-720
[INFO|configuration_utils.py:439] 2022-07-09 13:52:36,979 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-720/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:37,101 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-720/pytorch_model.bin
{'eval_loss': 0.18814821541309357, 'eval_accuracy': 0.9321503131524008, 'eval_f1': 0.6919431279620853, 'eval_precision': 0.6759259259259259, 'eval_recall': 0.7087378640776699, 'eval_runtime': 1.6441, 'eval_samples_per_second': 1165.405, 'eval_steps_per_second': 145.98, 'epoch': 4.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:43,641 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:43,641 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:43,641 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:45,437 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-900
[INFO|configuration_utils.py:439] 2022-07-09 13:52:45,439 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-900/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:45,566 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-900/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:52:45,855 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-720] due to args.save_total_limit
{'eval_loss': 0.3038184940814972, 'eval_accuracy': 0.8517745302713987, 'eval_f1': 0.5748502994011976, 'eval_precision': 0.4155844155844156, 'eval_recall': 0.9320388349514563, 'eval_runtime': 1.7907, 'eval_samples_per_second': 1069.995, 'eval_steps_per_second': 134.029, 'epoch': 5.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:52:52,253 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:52:52,253 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:52:52,253 >>   Batch size = 8
{'loss': 0.1624, 'learning_rate': 9.357894736842106e-05, 'epoch': 5.56}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:52:53,952 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1080
[INFO|configuration_utils.py:439] 2022-07-09 13:52:53,962 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1080/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:52:54,082 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1080/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:52:54,356 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-900] due to args.save_total_limit
{'eval_loss': 0.1785772144794464, 'eval_accuracy': 0.9368475991649269, 'eval_f1': 0.7420042643923241, 'eval_precision': 0.6615969581749049, 'eval_recall': 0.8446601941747572, 'eval_runtime': 1.694, 'eval_samples_per_second': 1131.04, 'eval_steps_per_second': 141.675, 'epoch': 6.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:00,531 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:00,531 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:00,531 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:02,241 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1260
[INFO|configuration_utils.py:439] 2022-07-09 13:53:02,243 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1260/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:02,382 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1260/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:02,679 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-540] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:53:02,725 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1080] due to args.save_total_limit
{'eval_loss': 0.16341449320316315, 'eval_accuracy': 0.9545929018789144, 'eval_f1': 0.7872860635696822, 'eval_precision': 0.7931034482758621, 'eval_recall': 0.7815533980582524, 'eval_runtime': 1.7055, 'eval_samples_per_second': 1123.413, 'eval_steps_per_second': 140.72, 'epoch': 7.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:08,969 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:08,970 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:08,970 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:10,775 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1440
[INFO|configuration_utils.py:439] 2022-07-09 13:53:10,777 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1440/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:10,956 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1440/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:11,220 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1260] due to args.save_total_limit
{'eval_loss': 0.12870857119560242, 'eval_accuracy': 0.9577244258872651, 'eval_f1': 0.7896103896103895, 'eval_precision': 0.8491620111731844, 'eval_recall': 0.7378640776699029, 'eval_runtime': 1.8011, 'eval_samples_per_second': 1063.813, 'eval_steps_per_second': 133.254, 'epoch': 8.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:17,494 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:17,494 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:17,494 >>   Batch size = 8
{'loss': 0.123, 'learning_rate': 8.773099415204679e-05, 'epoch': 8.33}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:19,132 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1620
[INFO|configuration_utils.py:439] 2022-07-09 13:53:19,135 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1620/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:19,304 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1620/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:19,547 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1440] due to args.save_total_limit
{'eval_loss': 0.14993171393871307, 'eval_accuracy': 0.9551148225469729, 'eval_f1': 0.8045454545454546, 'eval_precision': 0.7564102564102564, 'eval_recall': 0.8592233009708737, 'eval_runtime': 1.6335, 'eval_samples_per_second': 1172.972, 'eval_steps_per_second': 146.928, 'epoch': 9.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:25,803 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:25,804 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:25,804 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:27,432 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1800
[INFO|configuration_utils.py:439] 2022-07-09 13:53:27,434 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1800/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:27,574 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1800/pytorch_model.bin
{'eval_loss': 0.1424463987350464, 'eval_accuracy': 0.9577244258872651, 'eval_f1': 0.800982800982801, 'eval_precision': 0.8109452736318408, 'eval_recall': 0.7912621359223301, 'eval_runtime': 1.6239, 'eval_samples_per_second': 1179.867, 'eval_steps_per_second': 147.791, 'epoch': 10.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:34,413 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:34,413 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:34,413 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:36,105 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1980
[INFO|configuration_utils.py:439] 2022-07-09 13:53:36,108 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1980/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:36,235 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1980/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:36,575 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1800] due to args.save_total_limit
{'eval_loss': 0.24182023108005524, 'eval_accuracy': 0.9305845511482255, 'eval_f1': 0.7302231237322515, 'eval_precision': 0.627177700348432, 'eval_recall': 0.8737864077669902, 'eval_runtime': 1.6866, 'eval_samples_per_second': 1136.034, 'eval_steps_per_second': 142.301, 'epoch': 11.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:42,927 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:42,927 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:42,927 >>   Batch size = 8
{'loss': 0.0974, 'learning_rate': 8.188304093567251e-05, 'epoch': 11.11}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:44,763 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2160
[INFO|configuration_utils.py:439] 2022-07-09 13:53:44,767 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2160/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:44,896 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2160/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:45,163 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1980] due to args.save_total_limit
{'eval_loss': 0.12352541089057922, 'eval_accuracy': 0.9598121085594989, 'eval_f1': 0.8020565552699228, 'eval_precision': 0.8524590163934426, 'eval_recall': 0.7572815533980582, 'eval_runtime': 1.8299, 'eval_samples_per_second': 1047.08, 'eval_steps_per_second': 131.158, 'epoch': 12.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:53:52,031 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:53:52,031 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:53:52,031 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:53:53,893 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2340
[INFO|configuration_utils.py:439] 2022-07-09 13:53:53,896 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2340/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:53:54,017 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2340/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:53:54,390 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-1620] due to args.save_total_limit
[INFO|trainer.py:2217] 2022-07-09 13:53:54,423 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2160] due to args.save_total_limit
{'eval_loss': 0.13140428066253662, 'eval_accuracy': 0.9634655532359081, 'eval_f1': 0.8364485981308412, 'eval_precision': 0.8063063063063063, 'eval_recall': 0.8689320388349514, 'eval_runtime': 1.8575, 'eval_samples_per_second': 1031.514, 'eval_steps_per_second': 129.208, 'epoch': 13.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:00,595 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:00,595 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:00,595 >>   Batch size = 8
{'loss': 0.0897, 'learning_rate': 7.6046783625731e-05, 'epoch': 13.89}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:02,224 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2520
[INFO|configuration_utils.py:439] 2022-07-09 13:54:02,227 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2520/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:02,349 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2520/pytorch_model.bin
{'eval_loss': 0.16033770143985748, 'eval_accuracy': 0.9613778705636743, 'eval_f1': 0.827906976744186, 'eval_precision': 0.7946428571428571, 'eval_recall': 0.8640776699029126, 'eval_runtime': 1.6253, 'eval_samples_per_second': 1178.868, 'eval_steps_per_second': 147.666, 'epoch': 14.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:08,798 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:08,799 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:08,799 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:10,416 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2700
[INFO|configuration_utils.py:439] 2022-07-09 13:54:10,418 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2700/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:10,539 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2700/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:10,794 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2520] due to args.save_total_limit
{'eval_loss': 0.16236822307109833, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8246913580246913, 'eval_precision': 0.8391959798994975, 'eval_recall': 0.8106796116504854, 'eval_runtime': 1.6128, 'eval_samples_per_second': 1188.021, 'eval_steps_per_second': 148.813, 'epoch': 15.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:17,077 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:17,077 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:17,077 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:18,839 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2880
[INFO|configuration_utils.py:439] 2022-07-09 13:54:18,842 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2880/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:18,969 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2880/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:19,251 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2700] due to args.save_total_limit
{'eval_loss': 0.15036557614803314, 'eval_accuracy': 0.958768267223382, 'eval_f1': 0.7893333333333333, 'eval_precision': 0.8757396449704142, 'eval_recall': 0.7184466019417476, 'eval_runtime': 1.7571, 'eval_samples_per_second': 1090.441, 'eval_steps_per_second': 136.59, 'epoch': 16.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:25,753 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:25,754 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:25,754 >>   Batch size = 8
{'loss': 0.0736, 'learning_rate': 7.019883040935674e-05, 'epoch': 16.67}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:27,539 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3060
[INFO|configuration_utils.py:439] 2022-07-09 13:54:27,542 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3060/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:27,685 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3060/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:27,986 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2880] due to args.save_total_limit
{'eval_loss': 0.13621151447296143, 'eval_accuracy': 0.9629436325678496, 'eval_f1': 0.8220551378446115, 'eval_precision': 0.8497409326424871, 'eval_recall': 0.7961165048543689, 'eval_runtime': 1.78, 'eval_samples_per_second': 1076.384, 'eval_steps_per_second': 134.829, 'epoch': 17.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:34,245 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:34,246 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:34,246 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:35,943 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3240
[INFO|configuration_utils.py:439] 2022-07-09 13:54:35,946 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3240/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:36,068 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3240/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:36,354 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3060] due to args.save_total_limit
{'eval_loss': 0.14475217461585999, 'eval_accuracy': 0.9624217118997912, 'eval_f1': 0.8252427184466019, 'eval_precision': 0.8252427184466019, 'eval_recall': 0.8252427184466019, 'eval_runtime': 1.693, 'eval_samples_per_second': 1131.736, 'eval_steps_per_second': 141.762, 'epoch': 18.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:42,588 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:42,588 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:42,588 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:44,228 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3420
[INFO|configuration_utils.py:439] 2022-07-09 13:54:44,230 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3420/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:44,376 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3420/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:44,611 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3240] due to args.save_total_limit
{'eval_loss': 0.16489018499851227, 'eval_accuracy': 0.9561586638830898, 'eval_f1': 0.8108108108108107, 'eval_precision': 0.7563025210084033, 'eval_recall': 0.8737864077669902, 'eval_runtime': 1.636, 'eval_samples_per_second': 1171.126, 'eval_steps_per_second': 146.696, 'epoch': 19.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:50,845 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:50,845 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:50,845 >>   Batch size = 8
{'loss': 0.0668, 'learning_rate': 6.436257309941522e-05, 'epoch': 19.44}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:54:52,481 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3600
[INFO|configuration_utils.py:439] 2022-07-09 13:54:52,484 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3600/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:54:52,606 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3600/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:54:52,859 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3420] due to args.save_total_limit
{'eval_loss': 0.14256973564624786, 'eval_accuracy': 0.9608559498956158, 'eval_f1': 0.8110831234256927, 'eval_precision': 0.8429319371727748, 'eval_recall': 0.7815533980582524, 'eval_runtime': 1.6313, 'eval_samples_per_second': 1174.546, 'eval_steps_per_second': 147.125, 'epoch': 20.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:54:59,196 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:54:59,196 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:54:59,196 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:55:00,853 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3780
[INFO|configuration_utils.py:439] 2022-07-09 13:55:00,856 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3780/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:55:00,976 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3780/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:55:01,227 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3600] due to args.save_total_limit
{'eval_loss': 0.15574604272842407, 'eval_accuracy': 0.9639874739039666, 'eval_f1': 0.8198433420365536, 'eval_precision': 0.8870056497175142, 'eval_recall': 0.7621359223300971, 'eval_runtime': 1.6514, 'eval_samples_per_second': 1160.239, 'eval_steps_per_second': 145.333, 'epoch': 21.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:55:07,506 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:55:07,506 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:55:07,506 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:55:09,220 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3960
[INFO|configuration_utils.py:439] 2022-07-09 13:55:09,222 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3960/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:55:09,349 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3960/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:55:09,623 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3780] due to args.save_total_limit
{'eval_loss': 0.1852041631937027, 'eval_accuracy': 0.954070981210856, 'eval_f1': 0.7999999999999999, 'eval_precision': 0.7521367521367521, 'eval_recall': 0.8543689320388349, 'eval_runtime': 1.7086, 'eval_samples_per_second': 1121.416, 'eval_steps_per_second': 140.47, 'epoch': 22.0}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2389] 2022-07-09 13:55:16,283 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:55:16,283 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:55:16,283 >>   Batch size = 8
{'loss': 0.0602, 'learning_rate': 5.851461988304093e-05, 'epoch': 22.22}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[INFO|trainer.py:2139] 2022-07-09 13:55:17,925 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-4140
[INFO|configuration_utils.py:439] 2022-07-09 13:55:17,927 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-4140/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:55:18,055 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-4140/pytorch_model.bin
[INFO|trainer.py:2217] 2022-07-09 13:55:18,310 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-3960] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-09 13:55:18,349 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:1516] 2022-07-09 13:55:18,349 >> Loading best model from /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/checkpoint-2340 (score: 0.8364485981308412).
[INFO|trainer.py:2139] 2022-07-09 13:55:18,409 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2
[INFO|configuration_utils.py:439] 2022-07-09 13:55:18,410 >> Configuration saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/config.json
[INFO|modeling_utils.py:1084] 2022-07-09 13:55:18,555 >> Model weights saved in /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/baseline/abtbuy-COMBO-False-PROB-0.10-AUG-typo-32-1e-04-TinyBERT_General_4L_312D/2/pytorch_model.bin
{'eval_loss': 0.18271468579769135, 'eval_accuracy': 0.9561586638830898, 'eval_f1': 0.8090909090909091, 'eval_precision': 0.7606837606837606, 'eval_recall': 0.8640776699029126, 'eval_runtime': 1.6373, 'eval_samples_per_second': 1170.216, 'eval_steps_per_second': 146.582, 'epoch': 23.0}
{'train_runtime': 195.0808, 'train_samples_per_second': 1471.954, 'train_steps_per_second': 46.135, 'train_loss': 0.1188736145047174, 'epoch': 23.0}
***** train metrics *****
  epoch                    =       23.0
  train_loss               =     0.1189
  train_runtime            = 0:03:15.08
  train_samples            =       5743
  train_samples_per_second =   1471.954
  train_steps_per_second   =     46.135
07/09/2022 13:55:18 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:2389] 2022-07-09 13:55:18,564 >> ***** Running Evaluation *****
[INFO|trainer.py:2391] 2022-07-09 13:55:18,564 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:55:18,564 >>   Batch size = 8
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.13140428066253662, 'eval_accuracy': 0.9634655532359081, 'eval_f1': 0.8364485981308412, 'eval_precision': 0.8063063063063063, 'eval_recall': 0.8689320388349514, 'eval_runtime': 1.6804, 'eval_samples_per_second': 1140.224, 'eval_steps_per_second': 142.826, 'epoch': 23.0}
***** eval metrics *****
  epoch                   =       23.0
  eval_accuracy           =     0.9635
  eval_f1                 =     0.8364
  eval_loss               =     0.1314
  eval_precision          =     0.8063
  eval_recall             =     0.8689
  eval_runtime            = 0:00:01.68
  eval_samples            =       1916
  eval_samples_per_second =   1140.224
  eval_steps_per_second   =    142.826
07/09/2022 13:55:20 - INFO - __main__ - *** Predict ***
[INFO|trainer.py:2389] 2022-07-09 13:55:20,330 >> ***** Running Prediction *****
[INFO|trainer.py:2391] 2022-07-09 13:55:20,330 >>   Num examples = 1916
[INFO|trainer.py:2394] 2022-07-09 13:55:20,330 >>   Batch size = 8
***** predict metrics *****
  predict_accuracy           =     0.9588
  predict_f1                 =      0.815
  predict_loss               =     0.1536
  predict_precision          =     0.7873
  predict_recall             =     0.8447
  predict_runtime            = 0:00:02.15
  predict_samples            =       1916
  predict_samples_per_second =    887.823
  predict_steps_per_second   =     111.21

============================= JOB FEEDBACK =============================

NodeName=uc2n508
Job ID: 20884377
Cluster: uc2
User/Group: ma_dmittal/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:20:33
CPU Efficiency: 16.16% of 02:07:10 core-walltime
Job Wall-clock time: 00:12:43
Memory Utilized: 13.98 GB
Memory Efficiency: 0.00% of 0.00 MB
