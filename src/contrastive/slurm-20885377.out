07/10/2022 01:17:46 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: True
07/10/2022 01:17:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
bf16=False,
bf16_full_eval=False,
dataloader_drop_last=False,
dataloader_num_workers=4,
dataloader_pin_memory=True,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
debug=[],
deepspeed=None,
disable_tqdm=True,
do_eval=False,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_steps=None,
evaluation_strategy=IntervalStrategy.NO,
fp16=True,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_model_id=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=0.0001,
length_column_name=length,
load_best_model_at_end=False,
local_rank=-1,
log_level=-1,
log_level_replica=-1,
log_on_each_node=True,
logging_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/runs/Jul10_01-17-46_uc2n508.localdomain,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
no_cuda=False,
num_train_epochs=200.0,
optim=OptimizerNames.ADAMW_HF,
output_dir=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=8,
per_device_train_batch_size=1024,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
remove_unused_columns=True,
report_to=['tensorboard'],
resume_from_checkpoint=None,
run_name=/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/,
save_on_each_node=False,
save_steps=500,
save_strategy=IntervalStrategy.EPOCH,
save_total_limit=None,
seed=42,
sharded_ddp=[],
skip_memory_metrics=True,
tf32=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_legacy_prediction_loop=False,
warmup_ratio=0.05,
warmup_steps=0,
weight_decay=0.01,
xpu_backend=None,
)
[INFO|tokenization_auto.py:344] 2022-07-10 01:17:46,576 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:17:46,997 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:17:46,997 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:49,553 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/vocab.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4ec675a1f3cd38f2ddbe71010ce58471a710dd0188687381cf6f06fa7860c86a.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:49,554 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:49,554 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:49,554 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:49,554 >> loading file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:17:49,980 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:17:49,980 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|tokenization_utils.py:426] 2022-07-10 01:17:50,004 >> Adding [COL] to the vocabulary
[INFO|tokenization_utils.py:426] 2022-07-10 01:17:50,004 >> Adding [VAL] to the vocabulary
[WARNING|logging.py:279] 2022-07-10 01:17:50,004 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|configuration_utils.py:648] 2022-07-10 01:17:50,424 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:17:50,425 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[WARNING|logging.py:279] 2022-07-10 01:17:50,437 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[INFO|tokenization_auto.py:344] 2022-07-10 01:17:50,980 >> Could not locate the tokenizer configuration file, will try to use the model config instead.
[INFO|configuration_utils.py:648] 2022-07-10 01:17:51,404 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:17:51,405 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,404 >> loading file https://huggingface.co/distilroberta-base/resolve/main/vocab.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/23e0f7484fc8a320856b168861166b48c2976bb4e0861602422e1b0c3fe5bf61.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,405 >> loading file https://huggingface.co/distilroberta-base/resolve/main/merges.txt from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/c7e8020011da613ff5a9175ddad64cd47238a9525db975eb50ecb965e9f7302f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,405 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/b6a9ca6504e67903474c3fdf82ba249882406e61c2176a9d4dc9c3691c663767.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,405 >> loading file https://huggingface.co/distilroberta-base/resolve/main/added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,405 >> loading file https://huggingface.co/distilroberta-base/resolve/main/special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:1786] 2022-07-10 01:17:54,405 >> loading file https://huggingface.co/distilroberta-base/resolve/main/tokenizer_config.json from cache at None
[INFO|configuration_utils.py:648] 2022-07-10 01:17:54,829 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:17:54,829 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:648] 2022-07-10 01:17:55,320 >> loading configuration file https://huggingface.co/distilroberta-base/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/42d6b7c87cbac84fcdf35aa69504a5ccfca878fcee2a1a9b9ff7a3d1297f9094.aa95727ac70adfa1aaf5c88bea30a4f5e50869c68e68bce96ef1ec41b5facf46
[INFO|configuration_utils.py:684] 2022-07-10 01:17:55,321 >> Model config RobertaConfig {
  "_name_or_path": "distilroberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 6,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.17.0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:648] 2022-07-10 01:17:57,547 >> loading configuration file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/config.json from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/740f3ffeb47194ea97e78563af6f51d8fce8d8346d36d41889c75780869cf609.71ad48e6ad371e0c3ffa4982a5cc1c97e41023678e120b71ebe50addd42ec567
[INFO|configuration_utils.py:684] 2022-07-10 01:17:57,548 >> Model config BertConfig {
  "_name_or_path": "huawei-noah/TinyBERT_General_4L_312D",
  "attention_probs_dropout_prob": 0.1,
  "cell": {},
  "classifier_dropout": null,
  "emb_size": 312,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 312,
  "initializer_range": 0.02,
  "intermediate_size": 1200,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 4,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "pre_trained": "",
  "structure": [],
  "transformers_version": "4.17.0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

[INFO|modeling_utils.py:1431] 2022-07-10 01:17:58,020 >> loading weights file https://huggingface.co/huawei-noah/TinyBERT_General_4L_312D/resolve/main/pytorch_model.bin from cache at /home/ma/ma_ma/ma_dmittal/.cache/huggingface/transformers/4a4dca34df2df30c98747c12bff19ea7ee5380f4f180d8cfbbeff703c02793da.252b81fc76dfdd6968ed3f27881bcf37416e6c4ec326288155a94380bb85ed17
[WARNING|modeling_utils.py:1693] 2022-07-10 01:17:58,153 >> Some weights of the model checkpoint at huawei-noah/TinyBERT_General_4L_312D were not used when initializing BertModel: ['fit_denses.3.weight', 'fit_denses.2.weight', 'fit_denses.2.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'fit_denses.3.bias', 'cls.predictions.decoder.weight', 'fit_denses.4.bias', 'cls.seq_relationship.bias', 'fit_denses.1.bias', 'fit_denses.4.weight', 'cls.predictions.transform.dense.weight', 'fit_denses.0.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'fit_denses.0.weight', 'cls.predictions.bias', 'fit_denses.1.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[INFO|modeling_utils.py:1710] 2022-07-10 01:17:58,153 >> All the weights of BertModel were initialized from the model checkpoint at huawei-noah/TinyBERT_General_4L_312D.
If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.
[INFO|trainer.py:457] 2022-07-10 01:18:00,956 >> Using amp half precision backend
/home/ma/ma_ma/ma_dmittal/anaconda3/envs/deeper-0322/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
[INFO|trainer.py:1279] 2022-07-10 01:18:00,961 >> ***** Running training *****
[INFO|trainer.py:1280] 2022-07-10 01:18:00,961 >>   Num examples = 2817
[INFO|trainer.py:1281] 2022-07-10 01:18:00,961 >>   Num Epochs = 200
[INFO|trainer.py:1282] 2022-07-10 01:18:00,961 >>   Instantaneous batch size per device = 1024
[INFO|trainer.py:1283] 2022-07-10 01:18:00,961 >>   Total train batch size (w. parallel, distributed & accumulation) = 1024
[INFO|trainer.py:1284] 2022-07-10 01:18:00,961 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:1285] 2022-07-10 01:18:00,961 >>   Total optimization steps = 600
[INFO|trainer.py:2139] 2022-07-10 01:18:05,604 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-3
[INFO|trainer.py:2148] 2022-07-10 01:18:05,604 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'loss': 7.0955, 'learning_rate': 1e-05, 'epoch': 1.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:09,579 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-6
[INFO|trainer.py:2148] 2022-07-10 01:18:09,579 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:09,948 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-3] due to args.save_total_limit
{'loss': 6.9284, 'learning_rate': 2e-05, 'epoch': 2.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:13,531 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-9
[INFO|trainer.py:2148] 2022-07-10 01:18:13,531 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:13,912 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-6] due to args.save_total_limit
{'loss': 6.4501, 'learning_rate': 3e-05, 'epoch': 3.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:17,443 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-12
[INFO|trainer.py:2148] 2022-07-10 01:18:17,443 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:17,833 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-9] due to args.save_total_limit
{'loss': 5.7886, 'learning_rate': 4e-05, 'epoch': 4.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:21,507 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-15
[INFO|trainer.py:2148] 2022-07-10 01:18:21,507 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:21,875 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-12] due to args.save_total_limit
{'loss': 5.0212, 'learning_rate': 5e-05, 'epoch': 5.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:25,544 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-18
[INFO|trainer.py:2148] 2022-07-10 01:18:25,544 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:25,927 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-15] due to args.save_total_limit
{'loss': 4.4255, 'learning_rate': 6e-05, 'epoch': 6.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:29,518 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-21
[INFO|trainer.py:2148] 2022-07-10 01:18:29,518 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:30,009 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-18] due to args.save_total_limit
{'loss': 3.9997, 'learning_rate': 7e-05, 'epoch': 7.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:33,694 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-24
[INFO|trainer.py:2148] 2022-07-10 01:18:33,694 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:34,090 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-21] due to args.save_total_limit
{'loss': 3.577, 'learning_rate': 8e-05, 'epoch': 8.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:37,731 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-27
[INFO|trainer.py:2148] 2022-07-10 01:18:37,731 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:38,111 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-24] due to args.save_total_limit
{'loss': 2.998, 'learning_rate': 9e-05, 'epoch': 9.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:41,715 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-30
[INFO|trainer.py:2148] 2022-07-10 01:18:41,715 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:42,101 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-27] due to args.save_total_limit
{'loss': 2.8267, 'learning_rate': 0.0001, 'epoch': 10.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:45,606 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-33
[INFO|trainer.py:2148] 2022-07-10 01:18:45,606 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:46,007 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-30] due to args.save_total_limit
{'loss': 2.5981, 'learning_rate': 9.947368421052632e-05, 'epoch': 11.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:49,566 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-36
[INFO|trainer.py:2148] 2022-07-10 01:18:49,566 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:49,967 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-33] due to args.save_total_limit
{'loss': 2.2546, 'learning_rate': 9.894736842105263e-05, 'epoch': 12.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:53,584 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-39
[INFO|trainer.py:2148] 2022-07-10 01:18:53,584 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:53,985 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-36] due to args.save_total_limit
{'loss': 2.2184, 'learning_rate': 9.842105263157894e-05, 'epoch': 13.0}
[INFO|trainer.py:2139] 2022-07-10 01:18:57,727 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-42
[INFO|trainer.py:2148] 2022-07-10 01:18:57,727 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:18:58,123 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-39] due to args.save_total_limit
{'loss': 2.0109, 'learning_rate': 9.789473684210527e-05, 'epoch': 14.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:01,667 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-45
[INFO|trainer.py:2148] 2022-07-10 01:19:01,667 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:02,049 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-42] due to args.save_total_limit
{'loss': 1.8381, 'learning_rate': 9.736842105263158e-05, 'epoch': 15.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:05,583 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-48
[INFO|trainer.py:2148] 2022-07-10 01:19:05,583 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:05,963 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-45] due to args.save_total_limit
{'loss': 1.7273, 'learning_rate': 9.68421052631579e-05, 'epoch': 16.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:09,430 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-51
[INFO|trainer.py:2148] 2022-07-10 01:19:09,430 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:09,830 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-48] due to args.save_total_limit
{'loss': 1.9402, 'learning_rate': 9.631578947368421e-05, 'epoch': 17.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:13,442 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-54
[INFO|trainer.py:2148] 2022-07-10 01:19:13,443 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:13,827 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-51] due to args.save_total_limit
{'loss': 1.7197, 'learning_rate': 9.578947368421052e-05, 'epoch': 18.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:17,370 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-57
[INFO|trainer.py:2148] 2022-07-10 01:19:17,370 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:17,747 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-54] due to args.save_total_limit
{'loss': 1.4987, 'learning_rate': 9.526315789473685e-05, 'epoch': 19.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:21,261 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-60
[INFO|trainer.py:2148] 2022-07-10 01:19:21,261 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:21,649 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-57] due to args.save_total_limit
{'loss': 1.7294, 'learning_rate': 9.473684210526316e-05, 'epoch': 20.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:25,203 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-63
[INFO|trainer.py:2148] 2022-07-10 01:19:25,203 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:25,604 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-60] due to args.save_total_limit
{'loss': 1.515, 'learning_rate': 9.421052631578949e-05, 'epoch': 21.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:29,171 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-66
[INFO|trainer.py:2148] 2022-07-10 01:19:29,172 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:29,554 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-63] due to args.save_total_limit
{'loss': 1.4605, 'learning_rate': 9.36842105263158e-05, 'epoch': 22.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:32,989 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-69
[INFO|trainer.py:2148] 2022-07-10 01:19:32,990 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:33,376 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-66] due to args.save_total_limit
{'loss': 1.4539, 'learning_rate': 9.315789473684211e-05, 'epoch': 23.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:36,985 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-72
[INFO|trainer.py:2148] 2022-07-10 01:19:36,985 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:37,379 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-69] due to args.save_total_limit
{'loss': 1.2583, 'learning_rate': 9.263157894736843e-05, 'epoch': 24.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:40,814 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-75
[INFO|trainer.py:2148] 2022-07-10 01:19:40,815 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:41,219 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-72] due to args.save_total_limit
{'loss': 1.3535, 'learning_rate': 9.210526315789474e-05, 'epoch': 25.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:44,876 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-78
[INFO|trainer.py:2148] 2022-07-10 01:19:44,876 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:45,277 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-75] due to args.save_total_limit
{'loss': 1.3522, 'learning_rate': 9.157894736842105e-05, 'epoch': 26.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:48,944 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-81
[INFO|trainer.py:2148] 2022-07-10 01:19:48,944 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:49,327 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-78] due to args.save_total_limit
{'loss': 1.2064, 'learning_rate': 9.105263157894738e-05, 'epoch': 27.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:52,852 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-84
[INFO|trainer.py:2148] 2022-07-10 01:19:52,852 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:53,231 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-81] due to args.save_total_limit
{'loss': 1.4073, 'learning_rate': 9.052631578947369e-05, 'epoch': 28.0}
[INFO|trainer.py:2139] 2022-07-10 01:19:56,711 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-87
[INFO|trainer.py:2148] 2022-07-10 01:19:56,711 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:19:57,095 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-84] due to args.save_total_limit
{'loss': 1.0099, 'learning_rate': 9e-05, 'epoch': 29.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:00,717 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-90
[INFO|trainer.py:2148] 2022-07-10 01:20:00,717 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:01,100 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-87] due to args.save_total_limit
{'loss': 1.0904, 'learning_rate': 8.947368421052632e-05, 'epoch': 30.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:04,640 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-93
[INFO|trainer.py:2148] 2022-07-10 01:20:04,641 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:05,019 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-90] due to args.save_total_limit
{'loss': 1.2947, 'learning_rate': 8.894736842105263e-05, 'epoch': 31.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:08,627 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-96
[INFO|trainer.py:2148] 2022-07-10 01:20:08,628 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:09,020 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-93] due to args.save_total_limit
{'loss': 1.0734, 'learning_rate': 8.842105263157894e-05, 'epoch': 32.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:12,615 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-99
[INFO|trainer.py:2148] 2022-07-10 01:20:12,615 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:13,008 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-96] due to args.save_total_limit
{'loss': 1.132, 'learning_rate': 8.789473684210526e-05, 'epoch': 33.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:16,542 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-102
[INFO|trainer.py:2148] 2022-07-10 01:20:16,542 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:16,939 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-99] due to args.save_total_limit
{'loss': 1.1389, 'learning_rate': 8.736842105263158e-05, 'epoch': 34.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:20,626 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-105
[INFO|trainer.py:2148] 2022-07-10 01:20:20,627 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:21,017 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-102] due to args.save_total_limit
{'loss': 1.0077, 'learning_rate': 8.68421052631579e-05, 'epoch': 35.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:24,697 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-108
[INFO|trainer.py:2148] 2022-07-10 01:20:24,697 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:25,101 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-105] due to args.save_total_limit
{'loss': 0.8314, 'learning_rate': 8.631578947368421e-05, 'epoch': 36.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:28,473 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-111
[INFO|trainer.py:2148] 2022-07-10 01:20:28,473 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:28,862 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-108] due to args.save_total_limit
{'loss': 1.094, 'learning_rate': 8.578947368421054e-05, 'epoch': 37.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:32,484 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-114
[INFO|trainer.py:2148] 2022-07-10 01:20:32,485 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:32,877 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-111] due to args.save_total_limit
{'loss': 0.9646, 'learning_rate': 8.526315789473685e-05, 'epoch': 38.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:36,351 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-117
[INFO|trainer.py:2148] 2022-07-10 01:20:36,351 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:36,730 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-114] due to args.save_total_limit
{'loss': 0.9518, 'learning_rate': 8.473684210526316e-05, 'epoch': 39.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:40,180 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-120
[INFO|trainer.py:2148] 2022-07-10 01:20:40,180 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:40,605 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-117] due to args.save_total_limit
{'loss': 1.1625, 'learning_rate': 8.421052631578948e-05, 'epoch': 40.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:44,191 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-123
[INFO|trainer.py:2148] 2022-07-10 01:20:44,191 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:44,567 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-120] due to args.save_total_limit
{'loss': 1.0325, 'learning_rate': 8.36842105263158e-05, 'epoch': 41.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:48,094 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-126
[INFO|trainer.py:2148] 2022-07-10 01:20:48,094 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:48,474 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-123] due to args.save_total_limit
{'loss': 0.8733, 'learning_rate': 8.315789473684212e-05, 'epoch': 42.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:52,005 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-129
[INFO|trainer.py:2148] 2022-07-10 01:20:52,005 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:52,384 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-126] due to args.save_total_limit
{'loss': 0.903, 'learning_rate': 8.263157894736843e-05, 'epoch': 43.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:55,832 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-132
[INFO|trainer.py:2148] 2022-07-10 01:20:55,833 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:20:56,218 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-129] due to args.save_total_limit
{'loss': 0.992, 'learning_rate': 8.210526315789474e-05, 'epoch': 44.0}
[INFO|trainer.py:2139] 2022-07-10 01:20:59,750 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-135
[INFO|trainer.py:2148] 2022-07-10 01:20:59,750 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:00,136 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-132] due to args.save_total_limit
{'loss': 0.973, 'learning_rate': 8.157894736842105e-05, 'epoch': 45.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:03,462 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-138
[INFO|trainer.py:2148] 2022-07-10 01:21:03,463 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:03,850 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-135] due to args.save_total_limit
{'loss': 0.9875, 'learning_rate': 8.105263157894737e-05, 'epoch': 46.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:07,388 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-141
[INFO|trainer.py:2148] 2022-07-10 01:21:07,388 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:07,776 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-138] due to args.save_total_limit
{'loss': 0.7224, 'learning_rate': 8.052631578947368e-05, 'epoch': 47.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:11,342 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-144
[INFO|trainer.py:2148] 2022-07-10 01:21:11,342 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:11,732 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-141] due to args.save_total_limit
{'loss': 0.815, 'learning_rate': 8e-05, 'epoch': 48.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:15,346 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-147
[INFO|trainer.py:2148] 2022-07-10 01:21:15,346 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:15,733 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-144] due to args.save_total_limit
{'loss': 0.7992, 'learning_rate': 7.947368421052632e-05, 'epoch': 49.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:19,269 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-150
[INFO|trainer.py:2148] 2022-07-10 01:21:19,270 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:19,666 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-147] due to args.save_total_limit
{'loss': 0.7953, 'learning_rate': 7.894736842105263e-05, 'epoch': 50.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:23,135 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-153
[INFO|trainer.py:2148] 2022-07-10 01:21:23,135 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:23,522 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-150] due to args.save_total_limit
{'loss': 1.0355, 'learning_rate': 7.842105263157895e-05, 'epoch': 51.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:27,127 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-156
[INFO|trainer.py:2148] 2022-07-10 01:21:27,128 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:27,523 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-153] due to args.save_total_limit
{'loss': 1.0153, 'learning_rate': 7.789473684210526e-05, 'epoch': 52.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:31,220 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-159
[INFO|trainer.py:2148] 2022-07-10 01:21:31,220 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:31,622 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-156] due to args.save_total_limit
{'loss': 0.6578, 'learning_rate': 7.736842105263159e-05, 'epoch': 53.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:35,204 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-162
[INFO|trainer.py:2148] 2022-07-10 01:21:35,204 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:35,587 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-159] due to args.save_total_limit
{'loss': 0.7344, 'learning_rate': 7.68421052631579e-05, 'epoch': 54.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:39,197 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-165
[INFO|trainer.py:2148] 2022-07-10 01:21:39,197 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:39,575 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-162] due to args.save_total_limit
{'loss': 0.7646, 'learning_rate': 7.631578947368422e-05, 'epoch': 55.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:43,078 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-168
[INFO|trainer.py:2148] 2022-07-10 01:21:43,079 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:43,468 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-165] due to args.save_total_limit
{'loss': 0.6401, 'learning_rate': 7.578947368421054e-05, 'epoch': 56.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:47,053 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-171
[INFO|trainer.py:2148] 2022-07-10 01:21:47,053 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:47,441 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-168] due to args.save_total_limit
{'loss': 0.84, 'learning_rate': 7.526315789473685e-05, 'epoch': 57.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:51,066 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-174
[INFO|trainer.py:2148] 2022-07-10 01:21:51,066 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:51,459 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-171] due to args.save_total_limit
{'loss': 0.7466, 'learning_rate': 7.473684210526316e-05, 'epoch': 58.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:54,906 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-177
[INFO|trainer.py:2148] 2022-07-10 01:21:54,906 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:55,309 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-174] due to args.save_total_limit
{'loss': 0.7317, 'learning_rate': 7.421052631578948e-05, 'epoch': 59.0}
[INFO|trainer.py:2139] 2022-07-10 01:21:58,825 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-180
[INFO|trainer.py:2148] 2022-07-10 01:21:58,825 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:21:59,211 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-177] due to args.save_total_limit
{'loss': 0.8603, 'learning_rate': 7.368421052631579e-05, 'epoch': 60.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:02,801 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-183
[INFO|trainer.py:2148] 2022-07-10 01:22:02,801 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:03,208 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-180] due to args.save_total_limit
{'loss': 0.7397, 'learning_rate': 7.315789473684212e-05, 'epoch': 61.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:06,711 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-186
[INFO|trainer.py:2148] 2022-07-10 01:22:06,711 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:07,107 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-183] due to args.save_total_limit
{'loss': 0.8682, 'learning_rate': 7.263157894736843e-05, 'epoch': 62.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:10,599 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-189
[INFO|trainer.py:2148] 2022-07-10 01:22:10,599 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:10,992 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-186] due to args.save_total_limit
{'loss': 0.6928, 'learning_rate': 7.210526315789474e-05, 'epoch': 63.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:14,487 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-192
[INFO|trainer.py:2148] 2022-07-10 01:22:14,488 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:14,882 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-189] due to args.save_total_limit
{'loss': 0.9477, 'learning_rate': 7.157894736842105e-05, 'epoch': 64.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:18,485 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-195
[INFO|trainer.py:2148] 2022-07-10 01:22:18,485 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:18,873 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-192] due to args.save_total_limit
{'loss': 0.7139, 'learning_rate': 7.105263157894737e-05, 'epoch': 65.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:22,535 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-198
[INFO|trainer.py:2148] 2022-07-10 01:22:22,535 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:22,925 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-195] due to args.save_total_limit
{'loss': 0.7054, 'learning_rate': 7.052631578947368e-05, 'epoch': 66.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:26,535 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-201
[INFO|trainer.py:2148] 2022-07-10 01:22:26,535 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:26,927 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-198] due to args.save_total_limit
{'loss': 0.8236, 'learning_rate': 7e-05, 'epoch': 67.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:30,399 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-204
[INFO|trainer.py:2148] 2022-07-10 01:22:30,399 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:30,792 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-201] due to args.save_total_limit
{'loss': 0.774, 'learning_rate': 6.947368421052632e-05, 'epoch': 68.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:34,475 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-207
[INFO|trainer.py:2148] 2022-07-10 01:22:34,476 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:34,864 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-204] due to args.save_total_limit
{'loss': 0.6832, 'learning_rate': 6.894736842105263e-05, 'epoch': 69.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:38,522 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-210
[INFO|trainer.py:2148] 2022-07-10 01:22:38,522 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:38,910 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-207] due to args.save_total_limit
{'loss': 0.5417, 'learning_rate': 6.842105263157895e-05, 'epoch': 70.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:42,513 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-213
[INFO|trainer.py:2148] 2022-07-10 01:22:42,513 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:42,904 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-210] due to args.save_total_limit
{'loss': 0.6621, 'learning_rate': 6.789473684210527e-05, 'epoch': 71.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:46,460 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-216
[INFO|trainer.py:2148] 2022-07-10 01:22:46,460 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:46,839 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-213] due to args.save_total_limit
{'loss': 0.644, 'learning_rate': 6.736842105263159e-05, 'epoch': 72.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:50,469 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-219
[INFO|trainer.py:2148] 2022-07-10 01:22:50,470 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:50,852 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-216] due to args.save_total_limit
{'loss': 0.5558, 'learning_rate': 6.68421052631579e-05, 'epoch': 73.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:54,437 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-222
[INFO|trainer.py:2148] 2022-07-10 01:22:54,438 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:54,829 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-219] due to args.save_total_limit
{'loss': 0.7649, 'learning_rate': 6.631578947368421e-05, 'epoch': 74.0}
[INFO|trainer.py:2139] 2022-07-10 01:22:58,429 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-225
[INFO|trainer.py:2148] 2022-07-10 01:22:58,429 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:22:58,820 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-222] due to args.save_total_limit
{'loss': 0.6736, 'learning_rate': 6.578947368421054e-05, 'epoch': 75.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:02,295 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-228
[INFO|trainer.py:2148] 2022-07-10 01:23:02,295 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:02,688 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-225] due to args.save_total_limit
{'loss': 0.7398, 'learning_rate': 6.526315789473685e-05, 'epoch': 76.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:06,091 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-231
[INFO|trainer.py:2148] 2022-07-10 01:23:06,091 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:06,470 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-228] due to args.save_total_limit
{'loss': 0.7664, 'learning_rate': 6.473684210526316e-05, 'epoch': 77.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:10,105 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-234
[INFO|trainer.py:2148] 2022-07-10 01:23:10,105 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:10,496 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-231] due to args.save_total_limit
{'loss': 0.5128, 'learning_rate': 6.421052631578948e-05, 'epoch': 78.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:14,031 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-237
[INFO|trainer.py:2148] 2022-07-10 01:23:14,031 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:14,438 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-234] due to args.save_total_limit
{'loss': 0.7332, 'learning_rate': 6.368421052631579e-05, 'epoch': 79.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:17,999 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-240
[INFO|trainer.py:2148] 2022-07-10 01:23:17,999 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:18,379 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-237] due to args.save_total_limit
{'loss': 0.6539, 'learning_rate': 6.31578947368421e-05, 'epoch': 80.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:21,815 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-243
[INFO|trainer.py:2148] 2022-07-10 01:23:21,815 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:22,199 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-240] due to args.save_total_limit
{'loss': 0.6477, 'learning_rate': 6.263157894736842e-05, 'epoch': 81.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:25,822 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-246
[INFO|trainer.py:2148] 2022-07-10 01:23:25,822 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:26,205 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-243] due to args.save_total_limit
{'loss': 0.5192, 'learning_rate': 6.210526315789474e-05, 'epoch': 82.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:29,778 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-249
[INFO|trainer.py:2148] 2022-07-10 01:23:29,779 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:30,169 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-246] due to args.save_total_limit
{'loss': 0.5996, 'learning_rate': 6.157894736842106e-05, 'epoch': 83.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:33,784 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-252
[INFO|trainer.py:2148] 2022-07-10 01:23:33,784 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:34,168 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-249] due to args.save_total_limit
{'loss': 0.5133, 'learning_rate': 6.105263157894737e-05, 'epoch': 84.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:37,729 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-255
[INFO|trainer.py:2148] 2022-07-10 01:23:37,729 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:38,109 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-252] due to args.save_total_limit
{'loss': 0.6131, 'learning_rate': 6.052631578947369e-05, 'epoch': 85.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:41,674 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-258
[INFO|trainer.py:2148] 2022-07-10 01:23:41,674 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:42,084 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-255] due to args.save_total_limit
{'loss': 0.8829, 'learning_rate': 6e-05, 'epoch': 86.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:45,748 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-261
[INFO|trainer.py:2148] 2022-07-10 01:23:45,748 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:46,134 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-258] due to args.save_total_limit
{'loss': 0.5239, 'learning_rate': 5.9473684210526315e-05, 'epoch': 87.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:49,610 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-264
[INFO|trainer.py:2148] 2022-07-10 01:23:49,611 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:50,013 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-261] due to args.save_total_limit
{'loss': 0.7636, 'learning_rate': 5.894736842105263e-05, 'epoch': 88.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:53,712 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-267
[INFO|trainer.py:2148] 2022-07-10 01:23:53,712 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:54,096 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-264] due to args.save_total_limit
{'loss': 0.4822, 'learning_rate': 5.8421052631578954e-05, 'epoch': 89.0}
[INFO|trainer.py:2139] 2022-07-10 01:23:57,642 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-270
[INFO|trainer.py:2148] 2022-07-10 01:23:57,642 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:23:58,030 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-267] due to args.save_total_limit
{'loss': 0.6229, 'learning_rate': 5.789473684210527e-05, 'epoch': 90.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:01,637 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-273
[INFO|trainer.py:2148] 2022-07-10 01:24:01,637 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:02,027 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-270] due to args.save_total_limit
{'loss': 0.4875, 'learning_rate': 5.736842105263158e-05, 'epoch': 91.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:05,687 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-276
[INFO|trainer.py:2148] 2022-07-10 01:24:05,687 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:06,103 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-273] due to args.save_total_limit
{'loss': 0.6831, 'learning_rate': 5.68421052631579e-05, 'epoch': 92.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:09,741 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-279
[INFO|trainer.py:2148] 2022-07-10 01:24:09,741 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:10,154 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-276] due to args.save_total_limit
{'loss': 0.5757, 'learning_rate': 5.631578947368421e-05, 'epoch': 93.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:13,740 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-282
[INFO|trainer.py:2148] 2022-07-10 01:24:13,740 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:14,154 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-279] due to args.save_total_limit
{'loss': 0.5825, 'learning_rate': 5.5789473684210526e-05, 'epoch': 94.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:17,607 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-285
[INFO|trainer.py:2148] 2022-07-10 01:24:17,607 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:17,997 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-282] due to args.save_total_limit
{'loss': 0.722, 'learning_rate': 5.526315789473685e-05, 'epoch': 95.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:21,552 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-288
[INFO|trainer.py:2148] 2022-07-10 01:24:21,552 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:21,941 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-285] due to args.save_total_limit
{'loss': 0.6149, 'learning_rate': 5.4736842105263165e-05, 'epoch': 96.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:25,497 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-291
[INFO|trainer.py:2148] 2022-07-10 01:24:25,497 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:25,899 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-288] due to args.save_total_limit
{'loss': 0.5609, 'learning_rate': 5.421052631578948e-05, 'epoch': 97.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:29,563 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-294
[INFO|trainer.py:2148] 2022-07-10 01:24:29,563 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:29,964 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-291] due to args.save_total_limit
{'loss': 0.477, 'learning_rate': 5.368421052631579e-05, 'epoch': 98.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:33,601 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-297
[INFO|trainer.py:2148] 2022-07-10 01:24:33,601 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:33,986 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-294] due to args.save_total_limit
{'loss': 0.4568, 'learning_rate': 5.3157894736842104e-05, 'epoch': 99.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:37,511 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-300
[INFO|trainer.py:2148] 2022-07-10 01:24:37,512 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:37,914 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-297] due to args.save_total_limit
{'loss': 0.5617, 'learning_rate': 5.2631578947368424e-05, 'epoch': 100.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:41,430 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-303
[INFO|trainer.py:2148] 2022-07-10 01:24:41,430 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:41,821 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-300] due to args.save_total_limit
{'loss': 0.5701, 'learning_rate': 5.210526315789474e-05, 'epoch': 101.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:45,325 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-306
[INFO|trainer.py:2148] 2022-07-10 01:24:45,326 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:45,706 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-303] due to args.save_total_limit
{'loss': 0.6554, 'learning_rate': 5.157894736842106e-05, 'epoch': 102.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:49,207 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-309
[INFO|trainer.py:2148] 2022-07-10 01:24:49,207 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:49,620 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-306] due to args.save_total_limit
{'loss': 0.5857, 'learning_rate': 5.1052631578947376e-05, 'epoch': 103.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:53,211 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-312
[INFO|trainer.py:2148] 2022-07-10 01:24:53,211 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:53,592 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-309] due to args.save_total_limit
{'loss': 0.5996, 'learning_rate': 5.052631578947369e-05, 'epoch': 104.0}
[INFO|trainer.py:2139] 2022-07-10 01:24:57,168 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-315
[INFO|trainer.py:2148] 2022-07-10 01:24:57,168 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:24:57,562 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-312] due to args.save_total_limit
{'loss': 0.5572, 'learning_rate': 5e-05, 'epoch': 105.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:01,231 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-318
[INFO|trainer.py:2148] 2022-07-10 01:25:01,231 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:01,631 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-315] due to args.save_total_limit
{'loss': 0.6871, 'learning_rate': 4.9473684210526315e-05, 'epoch': 106.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:05,204 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-321
[INFO|trainer.py:2148] 2022-07-10 01:25:05,204 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:05,586 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-318] due to args.save_total_limit
{'loss': 0.6255, 'learning_rate': 4.8947368421052635e-05, 'epoch': 107.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:09,071 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-324
[INFO|trainer.py:2148] 2022-07-10 01:25:09,072 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:09,467 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-321] due to args.save_total_limit
{'loss': 0.6663, 'learning_rate': 4.842105263157895e-05, 'epoch': 108.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:13,025 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-327
[INFO|trainer.py:2148] 2022-07-10 01:25:13,025 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:13,418 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-324] due to args.save_total_limit
{'loss': 0.4596, 'learning_rate': 4.789473684210526e-05, 'epoch': 109.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:16,941 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-330
[INFO|trainer.py:2148] 2022-07-10 01:25:16,941 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:17,340 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-327] due to args.save_total_limit
{'loss': 0.6788, 'learning_rate': 4.736842105263158e-05, 'epoch': 110.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:20,811 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-333
[INFO|trainer.py:2148] 2022-07-10 01:25:20,812 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:21,196 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-330] due to args.save_total_limit
{'loss': 0.8111, 'learning_rate': 4.68421052631579e-05, 'epoch': 111.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:24,681 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-336
[INFO|trainer.py:2148] 2022-07-10 01:25:24,681 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:25,068 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-333] due to args.save_total_limit
{'loss': 0.5594, 'learning_rate': 4.6315789473684214e-05, 'epoch': 112.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:28,637 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-339
[INFO|trainer.py:2148] 2022-07-10 01:25:28,638 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:29,025 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-336] due to args.save_total_limit
{'loss': 0.5732, 'learning_rate': 4.5789473684210527e-05, 'epoch': 113.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:32,512 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-342
[INFO|trainer.py:2148] 2022-07-10 01:25:32,512 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:32,909 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-339] due to args.save_total_limit
{'loss': 0.6758, 'learning_rate': 4.5263157894736846e-05, 'epoch': 114.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:36,487 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-345
[INFO|trainer.py:2148] 2022-07-10 01:25:36,487 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:36,867 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-342] due to args.save_total_limit
{'loss': 0.6733, 'learning_rate': 4.473684210526316e-05, 'epoch': 115.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:40,376 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-348
[INFO|trainer.py:2148] 2022-07-10 01:25:40,377 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:40,764 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-345] due to args.save_total_limit
{'loss': 0.6839, 'learning_rate': 4.421052631578947e-05, 'epoch': 116.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:44,274 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-351
[INFO|trainer.py:2148] 2022-07-10 01:25:44,274 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:44,653 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-348] due to args.save_total_limit
{'loss': 0.6542, 'learning_rate': 4.368421052631579e-05, 'epoch': 117.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:48,089 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-354
[INFO|trainer.py:2148] 2022-07-10 01:25:48,089 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:48,482 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-351] due to args.save_total_limit
{'loss': 0.6351, 'learning_rate': 4.3157894736842105e-05, 'epoch': 118.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:52,087 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-357
[INFO|trainer.py:2148] 2022-07-10 01:25:52,087 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:52,476 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-354] due to args.save_total_limit
{'loss': 0.5247, 'learning_rate': 4.2631578947368425e-05, 'epoch': 119.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:55,910 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-360
[INFO|trainer.py:2148] 2022-07-10 01:25:55,910 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:25:56,313 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-357] due to args.save_total_limit
{'loss': 0.6521, 'learning_rate': 4.210526315789474e-05, 'epoch': 120.0}
[INFO|trainer.py:2139] 2022-07-10 01:25:59,799 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-363
[INFO|trainer.py:2148] 2022-07-10 01:25:59,799 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:00,204 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-360] due to args.save_total_limit
{'loss': 0.6335, 'learning_rate': 4.157894736842106e-05, 'epoch': 121.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:03,887 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-366
[INFO|trainer.py:2148] 2022-07-10 01:26:03,887 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:04,292 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-363] due to args.save_total_limit
{'loss': 0.5448, 'learning_rate': 4.105263157894737e-05, 'epoch': 122.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:07,812 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-369
[INFO|trainer.py:2148] 2022-07-10 01:26:07,812 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:08,200 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-366] due to args.save_total_limit
{'loss': 0.6569, 'learning_rate': 4.0526315789473684e-05, 'epoch': 123.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:11,678 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-372
[INFO|trainer.py:2148] 2022-07-10 01:26:11,678 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:12,075 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-369] due to args.save_total_limit
{'loss': 0.6735, 'learning_rate': 4e-05, 'epoch': 124.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:15,608 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-375
[INFO|trainer.py:2148] 2022-07-10 01:26:15,608 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:16,005 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-372] due to args.save_total_limit
{'loss': 0.6474, 'learning_rate': 3.9473684210526316e-05, 'epoch': 125.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:19,531 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-378
[INFO|trainer.py:2148] 2022-07-10 01:26:19,531 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:20,055 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-375] due to args.save_total_limit
{'loss': 0.7448, 'learning_rate': 3.894736842105263e-05, 'epoch': 126.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:23,605 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-381
[INFO|trainer.py:2148] 2022-07-10 01:26:23,605 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:23,995 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-378] due to args.save_total_limit
{'loss': 0.5223, 'learning_rate': 3.842105263157895e-05, 'epoch': 127.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:27,502 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-384
[INFO|trainer.py:2148] 2022-07-10 01:26:27,502 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:27,889 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-381] due to args.save_total_limit
{'loss': 0.5225, 'learning_rate': 3.789473684210527e-05, 'epoch': 128.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:31,514 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-387
[INFO|trainer.py:2148] 2022-07-10 01:26:31,515 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:31,904 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-384] due to args.save_total_limit
{'loss': 0.7576, 'learning_rate': 3.736842105263158e-05, 'epoch': 129.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:35,502 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-390
[INFO|trainer.py:2148] 2022-07-10 01:26:35,502 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:35,889 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-387] due to args.save_total_limit
{'loss': 0.6454, 'learning_rate': 3.6842105263157895e-05, 'epoch': 130.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:39,490 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-393
[INFO|trainer.py:2148] 2022-07-10 01:26:39,490 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:39,872 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-390] due to args.save_total_limit
{'loss': 0.6279, 'learning_rate': 3.6315789473684214e-05, 'epoch': 131.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:43,410 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-396
[INFO|trainer.py:2148] 2022-07-10 01:26:43,410 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:43,805 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-393] due to args.save_total_limit
{'loss': 0.6552, 'learning_rate': 3.578947368421053e-05, 'epoch': 132.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:47,381 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-399
[INFO|trainer.py:2148] 2022-07-10 01:26:47,381 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:47,773 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-396] due to args.save_total_limit
{'loss': 0.6236, 'learning_rate': 3.526315789473684e-05, 'epoch': 133.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:51,398 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-402
[INFO|trainer.py:2148] 2022-07-10 01:26:51,398 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:51,789 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-399] due to args.save_total_limit
{'loss': 0.731, 'learning_rate': 3.473684210526316e-05, 'epoch': 134.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:55,316 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-405
[INFO|trainer.py:2148] 2022-07-10 01:26:55,316 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:55,705 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-402] due to args.save_total_limit
{'loss': 0.6566, 'learning_rate': 3.421052631578947e-05, 'epoch': 135.0}
[INFO|trainer.py:2139] 2022-07-10 01:26:59,366 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-408
[INFO|trainer.py:2148] 2022-07-10 01:26:59,367 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:26:59,759 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-405] due to args.save_total_limit
{'loss': 0.5048, 'learning_rate': 3.368421052631579e-05, 'epoch': 136.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:03,350 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-411
[INFO|trainer.py:2148] 2022-07-10 01:27:03,350 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:03,738 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-408] due to args.save_total_limit
{'loss': 0.4123, 'learning_rate': 3.3157894736842106e-05, 'epoch': 137.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:07,295 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-414
[INFO|trainer.py:2148] 2022-07-10 01:27:07,296 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:07,686 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-411] due to args.save_total_limit
{'loss': 0.7518, 'learning_rate': 3.2631578947368426e-05, 'epoch': 138.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:11,264 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-417
[INFO|trainer.py:2148] 2022-07-10 01:27:11,264 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:11,664 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-414] due to args.save_total_limit
{'loss': 0.7173, 'learning_rate': 3.210526315789474e-05, 'epoch': 139.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:15,381 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-420
[INFO|trainer.py:2148] 2022-07-10 01:27:15,381 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:15,783 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-417] due to args.save_total_limit
{'loss': 0.5589, 'learning_rate': 3.157894736842105e-05, 'epoch': 140.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:19,398 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-423
[INFO|trainer.py:2148] 2022-07-10 01:27:19,398 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:19,790 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-420] due to args.save_total_limit
{'loss': 0.49, 'learning_rate': 3.105263157894737e-05, 'epoch': 141.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:23,405 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-426
[INFO|trainer.py:2148] 2022-07-10 01:27:23,405 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:23,799 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-423] due to args.save_total_limit
{'loss': 0.61, 'learning_rate': 3.0526315789473684e-05, 'epoch': 142.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:27,320 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-429
[INFO|trainer.py:2148] 2022-07-10 01:27:27,320 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:27,706 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-426] due to args.save_total_limit
{'loss': 0.6649, 'learning_rate': 3e-05, 'epoch': 143.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:31,307 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-432
[INFO|trainer.py:2148] 2022-07-10 01:27:31,307 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:31,690 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-429] due to args.save_total_limit
{'loss': 0.4037, 'learning_rate': 2.9473684210526314e-05, 'epoch': 144.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:35,233 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-435
[INFO|trainer.py:2148] 2022-07-10 01:27:35,233 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:35,628 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-432] due to args.save_total_limit
{'loss': 0.5286, 'learning_rate': 2.8947368421052634e-05, 'epoch': 145.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:39,152 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-438
[INFO|trainer.py:2148] 2022-07-10 01:27:39,153 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:39,547 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-435] due to args.save_total_limit
{'loss': 0.6186, 'learning_rate': 2.842105263157895e-05, 'epoch': 146.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:43,010 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-441
[INFO|trainer.py:2148] 2022-07-10 01:27:43,010 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:43,394 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-438] due to args.save_total_limit
{'loss': 0.5143, 'learning_rate': 2.7894736842105263e-05, 'epoch': 147.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:46,938 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-444
[INFO|trainer.py:2148] 2022-07-10 01:27:46,939 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:47,330 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-441] due to args.save_total_limit
{'loss': 0.4162, 'learning_rate': 2.7368421052631583e-05, 'epoch': 148.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:50,741 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-447
[INFO|trainer.py:2148] 2022-07-10 01:27:50,741 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:51,127 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-444] due to args.save_total_limit
{'loss': 0.4106, 'learning_rate': 2.6842105263157896e-05, 'epoch': 149.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:54,655 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-450
[INFO|trainer.py:2148] 2022-07-10 01:27:54,656 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:55,039 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-447] due to args.save_total_limit
{'loss': 0.5194, 'learning_rate': 2.6315789473684212e-05, 'epoch': 150.0}
[INFO|trainer.py:2139] 2022-07-10 01:27:58,494 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-453
[INFO|trainer.py:2148] 2022-07-10 01:27:58,494 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:27:58,874 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-450] due to args.save_total_limit
{'loss': 0.6118, 'learning_rate': 2.578947368421053e-05, 'epoch': 151.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:02,413 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-456
[INFO|trainer.py:2148] 2022-07-10 01:28:02,414 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:02,798 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-453] due to args.save_total_limit
{'loss': 0.3967, 'learning_rate': 2.5263157894736845e-05, 'epoch': 152.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:06,386 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-459
[INFO|trainer.py:2148] 2022-07-10 01:28:06,386 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:06,780 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-456] due to args.save_total_limit
{'loss': 0.5426, 'learning_rate': 2.4736842105263158e-05, 'epoch': 153.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:10,254 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-462
[INFO|trainer.py:2148] 2022-07-10 01:28:10,254 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:10,641 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-459] due to args.save_total_limit
{'loss': 0.5195, 'learning_rate': 2.4210526315789474e-05, 'epoch': 154.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:14,213 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-465
[INFO|trainer.py:2148] 2022-07-10 01:28:14,213 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:14,620 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-462] due to args.save_total_limit
{'loss': 0.6089, 'learning_rate': 2.368421052631579e-05, 'epoch': 155.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:18,177 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-468
[INFO|trainer.py:2148] 2022-07-10 01:28:18,177 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:18,566 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-465] due to args.save_total_limit
{'loss': 0.5156, 'learning_rate': 2.3157894736842107e-05, 'epoch': 156.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:22,219 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-471
[INFO|trainer.py:2148] 2022-07-10 01:28:22,219 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:22,617 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-468] due to args.save_total_limit
{'loss': 0.6103, 'learning_rate': 2.2631578947368423e-05, 'epoch': 157.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:26,175 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-474
[INFO|trainer.py:2148] 2022-07-10 01:28:26,175 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:26,570 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-471] due to args.save_total_limit
{'loss': 0.522, 'learning_rate': 2.2105263157894736e-05, 'epoch': 158.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:30,240 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-477
[INFO|trainer.py:2148] 2022-07-10 01:28:30,240 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:30,646 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-474] due to args.save_total_limit
{'loss': 0.5883, 'learning_rate': 2.1578947368421053e-05, 'epoch': 159.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:34,276 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-480
[INFO|trainer.py:2148] 2022-07-10 01:28:34,276 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:34,684 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-477] due to args.save_total_limit
{'loss': 0.4085, 'learning_rate': 2.105263157894737e-05, 'epoch': 160.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:38,262 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-483
[INFO|trainer.py:2148] 2022-07-10 01:28:38,262 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:38,653 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-480] due to args.save_total_limit
{'loss': 0.5271, 'learning_rate': 2.0526315789473685e-05, 'epoch': 161.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:42,208 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-486
[INFO|trainer.py:2148] 2022-07-10 01:28:42,209 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:42,603 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-483] due to args.save_total_limit
{'loss': 0.5078, 'learning_rate': 2e-05, 'epoch': 162.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:46,138 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-489
[INFO|trainer.py:2148] 2022-07-10 01:28:46,139 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:46,528 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-486] due to args.save_total_limit
{'loss': 0.4908, 'learning_rate': 1.9473684210526315e-05, 'epoch': 163.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:49,967 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-492
[INFO|trainer.py:2148] 2022-07-10 01:28:49,967 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:50,349 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-489] due to args.save_total_limit
{'loss': 0.6058, 'learning_rate': 1.8947368421052634e-05, 'epoch': 164.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:53,791 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-495
[INFO|trainer.py:2148] 2022-07-10 01:28:53,791 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:54,189 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-492] due to args.save_total_limit
{'loss': 0.6029, 'learning_rate': 1.8421052631578947e-05, 'epoch': 165.0}
[INFO|trainer.py:2139] 2022-07-10 01:28:57,796 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-498
[INFO|trainer.py:2148] 2022-07-10 01:28:57,796 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:28:58,180 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-495] due to args.save_total_limit
{'loss': 0.5998, 'learning_rate': 1.7894736842105264e-05, 'epoch': 166.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:01,766 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-501
[INFO|trainer.py:2148] 2022-07-10 01:29:01,766 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:02,161 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-498] due to args.save_total_limit
{'loss': 0.4803, 'learning_rate': 1.736842105263158e-05, 'epoch': 167.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:05,648 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-504
[INFO|trainer.py:2148] 2022-07-10 01:29:05,648 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:06,025 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-501] due to args.save_total_limit
{'loss': 0.7109, 'learning_rate': 1.6842105263157896e-05, 'epoch': 168.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:09,628 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-507
[INFO|trainer.py:2148] 2022-07-10 01:29:09,628 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:10,013 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-504] due to args.save_total_limit
{'loss': 0.4929, 'learning_rate': 1.6315789473684213e-05, 'epoch': 169.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:13,557 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-510
[INFO|trainer.py:2148] 2022-07-10 01:29:13,557 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:13,943 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-507] due to args.save_total_limit
{'loss': 0.699, 'learning_rate': 1.5789473684210526e-05, 'epoch': 170.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:17,475 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-513
[INFO|trainer.py:2148] 2022-07-10 01:29:17,476 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:17,857 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-510] due to args.save_total_limit
{'loss': 0.6123, 'learning_rate': 1.5263157894736842e-05, 'epoch': 171.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:21,399 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-516
[INFO|trainer.py:2148] 2022-07-10 01:29:21,399 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:21,783 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-513] due to args.save_total_limit
{'loss': 0.5098, 'learning_rate': 1.4736842105263157e-05, 'epoch': 172.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:25,329 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-519
[INFO|trainer.py:2148] 2022-07-10 01:29:25,329 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:25,712 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-516] due to args.save_total_limit
{'loss': 0.6099, 'learning_rate': 1.4210526315789475e-05, 'epoch': 173.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:29,215 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-522
[INFO|trainer.py:2148] 2022-07-10 01:29:29,215 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:29,618 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-519] due to args.save_total_limit
{'loss': 0.5924, 'learning_rate': 1.3684210526315791e-05, 'epoch': 174.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:33,092 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-525
[INFO|trainer.py:2148] 2022-07-10 01:29:33,092 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:33,481 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-522] due to args.save_total_limit
{'loss': 0.7151, 'learning_rate': 1.3157894736842106e-05, 'epoch': 175.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:37,044 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-528
[INFO|trainer.py:2148] 2022-07-10 01:29:37,044 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:37,430 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-525] due to args.save_total_limit
{'loss': 0.5256, 'learning_rate': 1.2631578947368422e-05, 'epoch': 176.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:40,901 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-531
[INFO|trainer.py:2148] 2022-07-10 01:29:40,901 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:41,281 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-528] due to args.save_total_limit
{'loss': 0.6248, 'learning_rate': 1.2105263157894737e-05, 'epoch': 177.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:44,820 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-534
[INFO|trainer.py:2148] 2022-07-10 01:29:44,820 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:45,206 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-531] due to args.save_total_limit
{'loss': 0.7241, 'learning_rate': 1.1578947368421053e-05, 'epoch': 178.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:48,701 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-537
[INFO|trainer.py:2148] 2022-07-10 01:29:48,702 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:49,090 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-534] due to args.save_total_limit
{'loss': 0.5157, 'learning_rate': 1.1052631578947368e-05, 'epoch': 179.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:52,621 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-540
[INFO|trainer.py:2148] 2022-07-10 01:29:52,622 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:53,011 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-537] due to args.save_total_limit
{'loss': 0.3767, 'learning_rate': 1.0526315789473684e-05, 'epoch': 180.0}
[INFO|trainer.py:2139] 2022-07-10 01:29:56,536 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-543
[INFO|trainer.py:2148] 2022-07-10 01:29:56,536 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:29:56,918 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-540] due to args.save_total_limit
{'loss': 0.5058, 'learning_rate': 1e-05, 'epoch': 181.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:00,464 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-546
[INFO|trainer.py:2148] 2022-07-10 01:30:00,465 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:00,846 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-543] due to args.save_total_limit
{'loss': 0.4623, 'learning_rate': 9.473684210526317e-06, 'epoch': 182.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:04,370 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-549
[INFO|trainer.py:2148] 2022-07-10 01:30:04,370 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:04,749 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-546] due to args.save_total_limit
{'loss': 0.5891, 'learning_rate': 8.947368421052632e-06, 'epoch': 183.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:08,288 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-552
[INFO|trainer.py:2148] 2022-07-10 01:30:08,288 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:08,682 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-549] due to args.save_total_limit
{'loss': 0.6109, 'learning_rate': 8.421052631578948e-06, 'epoch': 184.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:12,200 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-555
[INFO|trainer.py:2148] 2022-07-10 01:30:12,201 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:12,593 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-552] due to args.save_total_limit
{'loss': 0.6176, 'learning_rate': 7.894736842105263e-06, 'epoch': 185.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:16,146 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-558
[INFO|trainer.py:2148] 2022-07-10 01:30:16,147 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:16,535 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-555] due to args.save_total_limit
{'loss': 0.6044, 'learning_rate': 7.3684210526315784e-06, 'epoch': 186.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:20,103 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-561
[INFO|trainer.py:2148] 2022-07-10 01:30:20,104 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:20,491 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-558] due to args.save_total_limit
{'loss': 0.3811, 'learning_rate': 6.842105263157896e-06, 'epoch': 187.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:24,034 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-564
[INFO|trainer.py:2148] 2022-07-10 01:30:24,034 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:24,411 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-561] due to args.save_total_limit
{'loss': 0.6034, 'learning_rate': 6.315789473684211e-06, 'epoch': 188.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:27,911 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-567
[INFO|trainer.py:2148] 2022-07-10 01:30:27,911 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:28,289 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-564] due to args.save_total_limit
{'loss': 0.4924, 'learning_rate': 5.789473684210527e-06, 'epoch': 189.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:31,830 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-570
[INFO|trainer.py:2148] 2022-07-10 01:30:31,831 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:32,209 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-567] due to args.save_total_limit
{'loss': 0.39, 'learning_rate': 5.263157894736842e-06, 'epoch': 190.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:35,783 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-573
[INFO|trainer.py:2148] 2022-07-10 01:30:35,783 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:36,165 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-570] due to args.save_total_limit
{'loss': 0.4812, 'learning_rate': 4.736842105263159e-06, 'epoch': 191.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:39,722 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-576
[INFO|trainer.py:2148] 2022-07-10 01:30:39,722 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:40,106 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-573] due to args.save_total_limit
{'loss': 0.6159, 'learning_rate': 4.210526315789474e-06, 'epoch': 192.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:43,653 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-579
[INFO|trainer.py:2148] 2022-07-10 01:30:43,653 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:44,041 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-576] due to args.save_total_limit
{'loss': 0.5183, 'learning_rate': 3.6842105263157892e-06, 'epoch': 193.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:47,665 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-582
[INFO|trainer.py:2148] 2022-07-10 01:30:47,665 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:48,053 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-579] due to args.save_total_limit
{'loss': 0.4852, 'learning_rate': 3.1578947368421056e-06, 'epoch': 194.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:51,523 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-585
[INFO|trainer.py:2148] 2022-07-10 01:30:51,523 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:51,920 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-582] due to args.save_total_limit
{'loss': 0.7022, 'learning_rate': 2.631578947368421e-06, 'epoch': 195.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:55,500 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-588
[INFO|trainer.py:2148] 2022-07-10 01:30:55,500 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:55,897 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-585] due to args.save_total_limit
{'loss': 0.5019, 'learning_rate': 2.105263157894737e-06, 'epoch': 196.0}
[INFO|trainer.py:2139] 2022-07-10 01:30:59,443 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-591
[INFO|trainer.py:2148] 2022-07-10 01:30:59,443 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:30:59,830 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-588] due to args.save_total_limit
{'loss': 0.6257, 'learning_rate': 1.5789473684210528e-06, 'epoch': 197.0}
[INFO|trainer.py:2139] 2022-07-10 01:31:03,483 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-594
[INFO|trainer.py:2148] 2022-07-10 01:31:03,483 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:31:03,871 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-591] due to args.save_total_limit
{'loss': 0.5716, 'learning_rate': 1.0526315789473685e-06, 'epoch': 198.0}
[INFO|trainer.py:2139] 2022-07-10 01:31:07,398 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-597
[INFO|trainer.py:2148] 2022-07-10 01:31:07,398 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:31:07,794 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-594] due to args.save_total_limit
{'loss': 0.5848, 'learning_rate': 5.263157894736843e-07, 'epoch': 199.0}
[INFO|trainer.py:2139] 2022-07-10 01:31:11,369 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-600
[INFO|trainer.py:2148] 2022-07-10 01:31:11,369 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
[INFO|trainer.py:2217] 2022-07-10 01:31:11,762 >> Deleting older checkpoint [/pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/checkpoint-597] due to args.save_total_limit
[INFO|trainer.py:1508] 2022-07-10 01:31:11,799 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


[INFO|trainer.py:2139] 2022-07-10 01:31:11,803 >> Saving model checkpoint to /pfs/work7/workspace/scratch/ma_dmittal-dmittal/di-student/reports/contrastive/amazongoogle-clean-COMBO-False-PROB-0.10-AUG-typo1024-1e-04-0.07-TinyBERT_General_4L_312D/
[INFO|trainer.py:2148] 2022-07-10 01:31:11,803 >> Trainer.model is not a `PreTrainedModel`, only saving its state dict.
{'loss': 0.7144, 'learning_rate': 0.0, 'epoch': 200.0}
{'train_runtime': 790.838, 'train_samples_per_second': 712.409, 'train_steps_per_second': 0.759, 'train_loss': 0.9680497127771378, 'epoch': 200.0}
***** train metrics *****
  epoch                    =      200.0
  train_loss               =      0.968
  train_runtime            = 0:13:10.83
  train_samples            =       2817
  train_samples_per_second =    712.409
  train_steps_per_second   =      0.759

============================= JOB FEEDBACK =============================

NodeName=uc2n508
Job ID: 20885377
Cluster: uc2
User/Group: ma_dmittal/ma_ma
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 10
CPU Utilized: 00:29:12
CPU Efficiency: 20.93% of 02:19:30 core-walltime
Job Wall-clock time: 00:13:57
Memory Utilized: 14.28 GB
Memory Efficiency: 0.00% of 0.00 MB
